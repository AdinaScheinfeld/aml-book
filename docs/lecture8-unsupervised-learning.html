
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 8: Unsupervised Learning &#8212; Applied Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 9: Density Estimation" href="lecture9-density-estimation.html" />
    <link rel="prev" title="Lecture 6: Generative Models and Naive Bayes" href="lecture6-naive-bayes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-supervised-learning.html">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-linear-regression.html">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6-naive-bayes.html">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9-density-estimation.html">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10-clustering.html">
   Lecture 10: Clustering
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/lecture8-unsupervised-learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture8-unsupervised-learning.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/lecture8-unsupervised-learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-unsupervised-learning">
   8.1. Introduction to Unsupervised Learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning-example">
     8.1.1. Unsupervised Learning - Example
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning-example-dataset">
       8.1.1.1. Unsupervised Learning - Example Dataset
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning-example-algorithm">
       8.1.1.2. Unsupervised Learning - Example Algorithm
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applications-of-unsupervised-learning">
     8.1.2. Applications of Unsupervised Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-language-of-unsupervised-learning">
   8.2. The Language of Unsupervised Learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning-setting">
     8.2.1. Unsupervised Learning - Setting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-notation">
       8.2.1.1. Model: Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-class-notation">
       8.2.1.2. Model Class: Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#objective-notation">
       8.2.1.3. Objective: Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#optimizer-notation">
       8.2.1.4. Optimizer: Notation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-example-k-means">
     8.2.2. An Example:
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -Means
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-k-means-model">
       8.2.2.1. The
       <span class="math notranslate nohighlight">
        \(K\)
       </span>
       -Means Model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#k-means-at-a-high-level">
       8.2.2.2. K-Means at a High Level
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-k-means-objective">
       8.2.2.3. The
       <span class="math notranslate nohighlight">
        \(K\)
       </span>
       -Means Objective
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-k-means-optimizer">
       8.2.2.4. The
       <span class="math notranslate nohighlight">
        \(K\)
       </span>
       -Means Optimizer
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-k-means">
       8.2.2.5. Algorithm: K-Means
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unsupervised-learning-in-practice">
   8.3. Unsupervised Learning in Practice
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-generalization">
     8.3.1. Review: Generalization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-unsupervised-learning-dataset">
     8.3.2. An Unsupervised Learning Dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting-in-unsupervised-learning">
     8.3.3. Underfitting in Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting-in-unsupervised-learning">
     8.3.4. Overfitting in Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalization-in-unsupervised-learning">
     8.3.5. Generalization in Unsupervised Learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#detecting-overfitting-and-underfitting">
       8.3.5.1. Detecting Overfitting and Underfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-elbow-method">
       8.3.5.2. The Elbow Method
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#reducing-overfitting">
       8.3.5.3. Reducing Overfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generalization-in-unsupervised-learning-summary">
       8.3.5.4. Generalization in Unsupervised Learning: Summary
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 8: Unsupervised Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-unsupervised-learning">
   8.1. Introduction to Unsupervised Learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning-example">
     8.1.1. Unsupervised Learning - Example
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning-example-dataset">
       8.1.1.1. Unsupervised Learning - Example Dataset
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning-example-algorithm">
       8.1.1.2. Unsupervised Learning - Example Algorithm
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applications-of-unsupervised-learning">
     8.1.2. Applications of Unsupervised Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-language-of-unsupervised-learning">
   8.2. The Language of Unsupervised Learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning-setting">
     8.2.1. Unsupervised Learning - Setting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-notation">
       8.2.1.1. Model: Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-class-notation">
       8.2.1.2. Model Class: Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#objective-notation">
       8.2.1.3. Objective: Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#optimizer-notation">
       8.2.1.4. Optimizer: Notation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-example-k-means">
     8.2.2. An Example:
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -Means
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-k-means-model">
       8.2.2.1. The
       <span class="math notranslate nohighlight">
        \(K\)
       </span>
       -Means Model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#k-means-at-a-high-level">
       8.2.2.2. K-Means at a High Level
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-k-means-objective">
       8.2.2.3. The
       <span class="math notranslate nohighlight">
        \(K\)
       </span>
       -Means Objective
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-k-means-optimizer">
       8.2.2.4. The
       <span class="math notranslate nohighlight">
        \(K\)
       </span>
       -Means Optimizer
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-k-means">
       8.2.2.5. Algorithm: K-Means
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unsupervised-learning-in-practice">
   8.3. Unsupervised Learning in Practice
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-generalization">
     8.3.1. Review: Generalization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-unsupervised-learning-dataset">
     8.3.2. An Unsupervised Learning Dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting-in-unsupervised-learning">
     8.3.3. Underfitting in Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting-in-unsupervised-learning">
     8.3.4. Overfitting in Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalization-in-unsupervised-learning">
     8.3.5. Generalization in Unsupervised Learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#detecting-overfitting-and-underfitting">
       8.3.5.1. Detecting Overfitting and Underfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-elbow-method">
       8.3.5.2. The Elbow Method
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#reducing-overfitting">
       8.3.5.3. Reducing Overfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generalization-in-unsupervised-learning-summary">
       8.3.5.4. Generalization in Unsupervised Learning: Summary
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-8-unsupervised-learning">
<h1>Lecture 8: Unsupervised Learning<a class="headerlink" href="#lecture-8-unsupervised-learning" title="Permalink to this headline">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction-to-unsupervised-learning">
<h1>8.1. Introduction to Unsupervised Learning<a class="headerlink" href="#introduction-to-unsupervised-learning" title="Permalink to this headline">#</a></h1>
<p>As mentioned before, the three main types of machine learning methods are supervised learning, unsupervised learning, and Reinforcement learning. We were focusing on supervised learning for the last few lectures, but now we will shift our attention to another primary learning paradigm, unsupervised learning.</p>
<p>Let’s start by understanding what is unsupervised learning at a high level, starting with a dataset and an algorithm.</p>
<p>Unlike supervised Learning, in unsupervised Learning, we have a dataset <em>without</em> labels. Even though our data points are not labeled, we can still learn interesting things about the structure of the data including:</p>
<ul class="simple">
<li><p>Clusters hidden in the dataset.</p></li>
<li><p>Outliers: particularly unusual and/or interesting datapoints.</p></li>
<li><p>Useful signal hidden in noise, e.g. human speech over a noisy phone.</p></li>
</ul>
<p>In unsupervised learning, we also define a dataset and a learning algorithm.</p>
<div class="math notranslate nohighlight">
\[ \text{Dataset} + \text{Learning Algorithm} \to \text{Unsupervised Model} \]</div>
<p>The unsupervised model describes interesting structures in the data. For instance, it can identify interesting hidden clusters.</p>
<section id="unsupervised-learning-example">
<h2>8.1.1. Unsupervised Learning - Example<a class="headerlink" href="#unsupervised-learning-example" title="Permalink to this headline">#</a></h2>
<p>To concretize the ideas of unsupervised learning, let’s look at an example of an unsupervised learning model.</p>
<section id="unsupervised-learning-example-dataset">
<h3>8.1.1.1. Unsupervised Learning - Example Dataset<a class="headerlink" href="#unsupervised-learning-example-dataset" title="Permalink to this headline">#</a></h3>
<p>As an example of an unsupervised learning dataset, we will use our Iris flower dataset example, but we will discard the labels.</p>
<p>We start by loading this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import standard machine learning libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Print out the description of the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher&#39;s paper. Note that it&#39;s the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher&#39;s paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. &quot;The use of multiple measurements in taxonomic problems&quot;
     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to
     Mathematical Statistics&quot; (John Wiley, NY, 1950).
   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
</pre></div>
</div>
</div>
</div>
<p>For simplicity, we will discard petal length and width and use only sepal length and width as the features of each datapoint. Importantly, we also discard the class attribute, which is the label of this dataset, since we will run an unsupervised learning algorithm on this dataset.</p>
<p>With each datapoint containing two attributes, we can easily visualize this dataset in 2D using the code below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Visualize the Iris flower dataset</span>
<span class="c1"># iris.data[:,0] is the sepal length, while iris.data[:,1] is the sepal width </span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sepal width (cm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sepal length (cm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Dataset of Iris flowers&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fontconfig warning: ignoring UTF-8: not a valid region tag
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Dataset of Iris flowers&#39;)
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_9_2.png" src="_images/lecture8-unsupervised-learning_9_2.png" />
</div>
</div>
</section>
<section id="unsupervised-learning-example-algorithm">
<h3>8.1.1.2. Unsupervised Learning - Example Algorithm<a class="headerlink" href="#unsupervised-learning-example-algorithm" title="Permalink to this headline">#</a></h3>
<p>We can use this dataset, with only sepal length and width as features and without labels, as input to a popular unsupervised learning algorithm, <span class="math notranslate nohighlight">\(K\)</span>-means.</p>
<ul class="simple">
<li><p>The algorithm seeks to find <span class="math notranslate nohighlight">\(K\)</span> hidden clusters in the data.</p></li>
<li><p>Each cluster is characterized by its centroid (its mean).</p></li>
<li><p>The clusters reveal interesting structures in the data.</p></li>
</ul>
<p>The code below runs <span class="math notranslate nohighlight">\(K\)</span>-means, which is available in Scikit-Learn libary, on the data using <span class="math notranslate nohighlight">\(K = 3\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit K-Means with K=3</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KMeans(n_clusters=3)
</pre></div>
</div>
</div>
</div>
<p>We can now visualize the 3 clusters we have learned from <span class="math notranslate nohighlight">\(K\)</span>-means:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize the datapoints</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># visualize the learned clusters with red markers</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sepal width (cm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sepal length (cm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Dataset of Iris flowers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Datapoints&#39;</span><span class="p">,</span> <span class="s1">&#39;Probability peaks&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7fa46a892130&gt;
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_13_1.png" src="_images/lecture8-unsupervised-learning_13_1.png" />
</div>
</div>
<p>Looking at the visualization produced below, where we give different colors to datapoints belonging to different classes, we find that these learned clusters correspond nicely to the three types (classes) of flowers found in the dataset, which are the labels that the learning model did not have access to.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize datapoints belonging to different classes with colors</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Paired&#39;</span><span class="p">)</span>
<span class="c1"># visualize the learned cluster with red markers</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sepal width (cm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sepal length (cm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Dataset of Iris flowers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">p1</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Iris Setosa&#39;</span><span class="p">,</span> <span class="s1">&#39;Iris Versicolour&#39;</span><span class="p">,</span> <span class="s1">&#39;Iris Virginica&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7fa4990c40d0&gt;
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_15_1.png" src="_images/lecture8-unsupervised-learning_15_1.png" />
</div>
</div>
<p>Hopefully, this example helps convince you that there is usually interesting structural information you can learn using unsupervised learning algorithms from totally unlabeled data.</p>
</section>
</section>
<section id="applications-of-unsupervised-learning">
<h2>8.1.2. Applications of Unsupervised Learning<a class="headerlink" href="#applications-of-unsupervised-learning" title="Permalink to this headline">#</a></h2>
<p>Now, you might ask in what applications do we not have labels and want to use unsupervised learning?</p>
<p>Here are some applications we use it for:</p>
<ul class="simple">
<li><p>Visualization: identifying and making accessible useful hidden structures in the data.</p></li>
<li><p>Anomaly detection: identifying factory components that are likely to break soon.</p></li>
<li><p>Signal denoising: extracting human speech from a noisy recording.</p></li>
</ul>
<p>Concrete applications include:</p>
<ul class="simple">
<li><p>Discovering Structure in Digits: Unsupervised learning can discover structure in digits without any labels. This task has a similar flavor to the Iris dataset we were looking at earlier.</p></li>
<li><p>DNA Analysis: Unsupervised learning can be used to perform Dimensionality Reduction, which can be applied to DNA to reveal the geography of each country.</p></li>
<li><p>Facial Feature Learning: Modern unsupervised algorithms based on deep learning can uncover structure in human face datasets.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-language-of-unsupervised-learning">
<h1>8.2. The Language of Unsupervised Learning<a class="headerlink" href="#the-language-of-unsupervised-learning" title="Permalink to this headline">#</a></h1>
<p>Next, let’s look at how to define an unsupervised learning problem more formally.</p>
<section id="unsupervised-learning-setting">
<h2>8.2.1. Unsupervised Learning - Setting<a class="headerlink" href="#unsupervised-learning-setting" title="Permalink to this headline">#</a></h2>
<p>Once again, at a high level, an unsupervised machine learning problem has the following structure:</p>
<div class="math notranslate nohighlight">
\[ \underbrace{\text{Dataset}}_\text{Attributes, Features} + \underbrace{\text{Learning Algorithm}}_\text{Model Class + Objective + Optimizer } \to \text{Unsupervised Model} \]</div>
<p>We define a dataset of size <span class="math notranslate nohighlight">\(n\)</span> for unsupervised learning as
$<span class="math notranslate nohighlight">\(\mathcal{D} = \{x^{(i)} \mid i = 1,2,...,n\}\)</span>$</p>
<p>Each <span class="math notranslate nohighlight">\(x^{(i)} \in \mathbb{R}^d\)</span> denotes an input, a vector of <span class="math notranslate nohighlight">\(d\)</span> attributes or features.</p>
<p>We can think of an unsupervised learning algorithm as consisting of three components:</p>
<ul class="simple">
<li><p>A <strong>model class</strong>: the set of possible unsupervised models we consider.</p></li>
<li><p>An <strong>objective</strong> function, which defines how good a model is.</p></li>
<li><p>An <strong>optimizer</strong>, which finds the best predictive model in the model class according to the objective function</p></li>
</ul>
<section id="model-notation">
<h3>8.2.1.1. Model: Notation<a class="headerlink" href="#model-notation" title="Permalink to this headline">#</a></h3>
<p>We’ll say that a model is a function
$<span class="math notranslate nohighlight">\( f_\theta : \mathcal{X} \to \mathcal{S} \)</span><span class="math notranslate nohighlight">\(
that maps inputs \)</span>x \in \mathcal{X}<span class="math notranslate nohighlight">\( to some notion of structure \)</span>s \in \mathcal{S}<span class="math notranslate nohighlight">\(. Models may have *parameters* \)</span>\theta \in \Theta<span class="math notranslate nohighlight">\( living in a set \)</span>\Theta$</p>
<p>Structures can take many forms (clusters, low-dimensional representations, etc.), and we will see many examples.</p>
</section>
<section id="model-class-notation">
<h3>8.2.1.2. Model Class: Notation<a class="headerlink" href="#model-class-notation" title="Permalink to this headline">#</a></h3>
<p>Formally, the model class is a set
$<span class="math notranslate nohighlight">\(\mathcal{M} = \{f_\theta \mid f_\theta : \mathcal{X} \to \mathcal{S}; \; \theta \in \Theta \}.\)</span>$
of possible models (models with different parameters) that map input features to structural elements.</p>
</section>
<section id="objective-notation">
<h3>8.2.1.3. Objective: Notation<a class="headerlink" href="#objective-notation" title="Permalink to this headline">#</a></h3>
<p>We again define an <em>objective function</em> (also called a <em>loss function</em>)
$<span class="math notranslate nohighlight">\(J(\theta) : \Theta \to [0, \infty), \)</span><span class="math notranslate nohighlight">\(
which describes the extent to which \)</span>f_\theta<span class="math notranslate nohighlight">\( &quot;fits&quot; the data \)</span>\mathcal{D} = {x^{(i)} \mid i = 1,2,…,n}$.</p>
</section>
<section id="optimizer-notation">
<h3>8.2.1.4. Optimizer: Notation<a class="headerlink" href="#optimizer-notation" title="Permalink to this headline">#</a></h3>
<p>An optimizer finds a model <span class="math notranslate nohighlight">\(f_\theta \in \mathcal{M}\)</span> with the smallest value of the objective <span class="math notranslate nohighlight">\(J\)</span>.
\begin{align*}
\min_{\theta \in \Theta} J(\theta)
\end{align*}</p>
<p>Intuitively, this is the function that bests “fits” the data on the training dataset.</p>
</section>
</section>
<section id="an-example-k-means">
<h2>8.2.2. An Example: <span class="math notranslate nohighlight">\(K\)</span>-Means<a class="headerlink" href="#an-example-k-means" title="Permalink to this headline">#</a></h2>
<p>As an example, let’s mathematically define the <span class="math notranslate nohighlight">\(K\)</span>-Means algorithm that we saw earlier.</p>
<p>Recall that:</p>
<ul class="simple">
<li><p>The algorithm seeks to find <span class="math notranslate nohighlight">\(K\)</span> hidden clusters in the data.</p></li>
<li><p>Each cluster is characterized by its centroid (its mean).</p></li>
</ul>
<section id="the-k-means-model">
<h3>8.2.2.1. The <span class="math notranslate nohighlight">\(K\)</span>-Means Model<a class="headerlink" href="#the-k-means-model" title="Permalink to this headline">#</a></h3>
<p>The parameters <span class="math notranslate nohighlight">\(\theta\)</span> of the model are <span class="math notranslate nohighlight">\(K\)</span> <em>centroids</em> <span class="math notranslate nohighlight">\(c_1, c_2, \ldots c_K \in \mathcal{X}\)</span>. The class of <span class="math notranslate nohighlight">\(x\)</span> is <span class="math notranslate nohighlight">\(k\)</span> if <span class="math notranslate nohighlight">\(c_k\)</span> is the closest centroid to <span class="math notranslate nohighlight">\(x\)</span> (for this lecture, let’s assume the distance metric in use is Euclidean distance, although the algorithm works with any distance metric of your choice).</p>
<p>We can think of the model returned by <span class="math notranslate nohighlight">\(K\)</span>-Means as a function
$<span class="math notranslate nohighlight">\(f_\theta : \mathcal{X} \to \mathcal{S}\)</span><span class="math notranslate nohighlight">\(
that assigns each input \)</span>x<span class="math notranslate nohighlight">\( to a cluster \)</span>s \in \mathcal{S} = {1,2,\ldots,K}$.</p>
</section>
<section id="k-means-at-a-high-level">
<h3>8.2.2.2. K-Means at a High Level<a class="headerlink" href="#k-means-at-a-high-level" title="Permalink to this headline">#</a></h3>
<p>At a high level, <span class="math notranslate nohighlight">\(K\)</span>-means performs the following steps.</p>
<p>Starting from random centroids, we repeat until convergence:</p>
<ol class="simple">
<li><p>Update each cluster: assign each point to its closest centroid.</p></li>
<li><p>Set each centroid to be the center of the its cluster</p></li>
</ol>
<p>This is best illustrated visually - see <a class="reference external" href="https://commons.wikimedia.org/wiki/File:K-means_convergence.gif">Wikipedia</a></p>
</section>
<section id="the-k-means-objective">
<h3>8.2.2.3. The <span class="math notranslate nohighlight">\(K\)</span>-Means Objective<a class="headerlink" href="#the-k-means-objective" title="Permalink to this headline">#</a></h3>
<p>How do we determine whether <span class="math notranslate nohighlight">\(f_\theta\)</span> is a good clustering of the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>?</p>
<p>We seek centroids <span class="math notranslate nohighlight">\(c_k\)</span> such that the distance between the points and their closest centroid is minimized:
$<span class="math notranslate nohighlight">\(J(\theta) = \sum_{i=1}^n || x^{(i)} - \text{centroid}(f_\theta(x^{(i)})) ||,\)</span><span class="math notranslate nohighlight">\(
where \)</span>\text{centroid}(k) = c_k<span class="math notranslate nohighlight">\( denotes the centroid for cluster \)</span>k$.</p>
</section>
<section id="the-k-means-optimizer">
<h3>8.2.2.4. The <span class="math notranslate nohighlight">\(K\)</span>-Means Optimizer<a class="headerlink" href="#the-k-means-optimizer" title="Permalink to this headline">#</a></h3>
<p>We can optimize this in a two-step process, starting with an initial random cluster assignment <span class="math notranslate nohighlight">\(f_\theta(x)\)</span>.</p>
<p>Starting with random centroids <span class="math notranslate nohighlight">\(c_k\)</span>, repeat until convergence:</p>
<ol class="simple">
<li><p>Update <span class="math notranslate nohighlight">\(f(x)\)</span> such that <span class="math notranslate nohighlight">\(f(x^{(i)}) = \arg\min_k ||x^{(i)} - c_k||\)</span> is the cluster of the closest centroid to <span class="math notranslate nohighlight">\(x^{(i)}\)</span>.</p></li>
<li><p>Set each <span class="math notranslate nohighlight">\(c_k\)</span> to be the center of its cluster <span class="math notranslate nohighlight">\(\{x^{(i)} \mid f(x^{(i)}) = k\}\)</span>.</p></li>
</ol>
<p>Though we do not prove it here, this process is guaranteed to converge after a finite number of iterations.</p>
</section>
<section id="algorithm-k-means">
<h3>8.2.2.5. Algorithm: K-Means<a class="headerlink" href="#algorithm-k-means" title="Permalink to this headline">#</a></h3>
<p>We summarize the <span class="math notranslate nohighlight">\(K\)</span>-means algorithm below:</p>
<ul class="simple">
<li><p><strong>Type</strong>: Unsupervised learning (clustering)</p></li>
<li><p><strong>Model family</strong>: <span class="math notranslate nohighlight">\(k\)</span> centroids</p></li>
<li><p><strong>Objective function</strong>: Sum of distances (of your choice) to the closest centroids</p></li>
<li><p><strong>Optimizer</strong>: Iterative optimization procedure.</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="unsupervised-learning-in-practice">
<h1>8.3. Unsupervised Learning in Practice<a class="headerlink" href="#unsupervised-learning-in-practice" title="Permalink to this headline">#</a></h1>
<p>We will now look at some practical considerations to keep in mind when applying unsupervised learning.</p>
<section id="review-generalization">
<h2>8.3.1. Review: Generalization<a class="headerlink" href="#review-generalization" title="Permalink to this headline">#</a></h2>
<p>In machine learning, <strong>generalization</strong> is the property of predictive models to achieve good performance on new, holdout data that is distinct from the training set.</p>
<p>How does generalization apply to unsupervised learning?</p>
</section>
<section id="an-unsupervised-learning-dataset">
<h2>8.3.2. An Unsupervised Learning Dataset<a class="headerlink" href="#an-unsupervised-learning-dataset" title="Permalink to this headline">#</a></h2>
<p>Consider the following dataset, consisting of datapoints generated by a mixture of four Gaussian distributions. A sample from this mixture distribution should form roughly four clusters.</p>
<p>We visualize a sample of 100 datapoints below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Setting the seed makes the random module of numpy deterministic across different runs of the program</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Generate random 2D datapoints using 4 different Gaussians.</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7fa488a7a700&gt;
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_39_1.png" src="_images/lecture8-unsupervised-learning_39_1.png" />
</div>
</div>
<p>We know the true labels of these clusters, i.e., which Gaussian a datapoint is generated by, so we can visualize them with different colors below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7fa46ab33df0&gt;
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_41_1.png" src="_images/lecture8-unsupervised-learning_41_1.png" />
</div>
</div>
</section>
<section id="underfitting-in-unsupervised-learning">
<h2>8.3.3. Underfitting in Unsupervised Learning<a class="headerlink" href="#underfitting-in-unsupervised-learning" title="Permalink to this headline">#</a></h2>
<p>Underfitting happens when we are not able to fully learn the signal hidden in the data. In the context of <span class="math notranslate nohighlight">\(K\)</span>-Means, this means our <span class="math notranslate nohighlight">\(K\)</span> is lower than the actual number of clusters in the data.</p>
<p>Let’s run <span class="math notranslate nohighlight">\(K\)</span>-Means on our toy dataset made in the previous section 8.3.2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit a K-Means</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KMeans(n_clusters=2)
</pre></div>
</div>
</div>
</div>
<p>The centroids find two distinct components (rough clusters) in the data, but they fail to capture the true structure.</p>
<p>You can see below, where we visualize both the datapoints and the learned clusters, that more than one color might belong to a cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;K-Means Objective: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>K-Means Objective: 462.03
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_46_1.png" src="_images/lecture8-unsupervised-learning_46_1.png" />
</div>
</div>
<p>Consider now what happens if we further increase the number of clusters.</p>
<p>In the figures below, we have <span class="math notranslate nohighlight">\(K\)</span> equal to 4, 10, and 20 from left to right respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Will visualize learned clusters of K-means with different Ks.</span>
<span class="n">Ks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="c1"># Fit K-means</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># Visual both the datapoints and the k learned clusters </span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;K-Means Objective: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/lecture8-unsupervised-learning_48_0.png" src="_images/lecture8-unsupervised-learning_48_0.png" />
</div>
</div>
</section>
<section id="overfitting-in-unsupervised-learning">
<h2>8.3.4. Overfitting in Unsupervised Learning<a class="headerlink" href="#overfitting-in-unsupervised-learning" title="Permalink to this headline">#</a></h2>
<p>Overfitting happens when we fit the noise, but not the signal. In our example, this means fitting small, local noise clusters rather than the true global clusters.</p>
<p>We demonstrate it below by setting <span class="math notranslate nohighlight">\(K = 50\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setting K = 50</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="c1"># Fit K-means</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># Visualize both the datapoints and the learned clusters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;K-Means Objective: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>K-Means Objective: 4.67
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_50_1.png" src="_images/lecture8-unsupervised-learning_50_1.png" />
</div>
</div>
<p>We can see the true structure, i.e., 4 clusters, given enough data. Below we generate 10000 samples to see that</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7fa488d18e80&gt;
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_52_1.png" src="_images/lecture8-unsupervised-learning_52_1.png" />
</div>
</div>
</section>
<section id="generalization-in-unsupervised-learning">
<h2>8.3.5. Generalization in Unsupervised Learning<a class="headerlink" href="#generalization-in-unsupervised-learning" title="Permalink to this headline">#</a></h2>
<p>To talk about generalization, we usually assume that the dataset is sampled from a probability distribution <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>, which we will call the <em>data distribution</em>. We will denote this as
$<span class="math notranslate nohighlight">\(x \sim \mathbb{P}.\)</span>$</p>
<p>Moreover, we assume the dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{x^{(i)} \mid i = 1,2,...,n\}\)</span> consists of <em>independent and identicaly distributed</em> (IID) samples from <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>.</p>
<p>What <em>independent and identically distributed</em> (IID) means is:</p>
<ul class="simple">
<li><p>Each training example is from the same distribution.</p></li>
<li><p>This distribution doesn’t depend on previous training examples.</p></li>
</ul>
<p><strong>Example</strong>: Flipping a coin. Each flip has the same probability of heads &amp; tails and doesn’t depend on previous flips.</p>
<p><strong>Counter-Example</strong>: Yearly census data. The population in each year will be close to that of the previous year.</p>
<p>We can think of the data distribution as being the sum of two distinct components <span class="math notranslate nohighlight">\(\mathbb{P} = F + E\)</span></p>
<ol class="simple">
<li><p>A signal component <span class="math notranslate nohighlight">\(F\)</span> (hidden clusters, speech, low-dimensional data space, etc.)</p></li>
<li><p>A random noise component <span class="math notranslate nohighlight">\(E\)</span></p></li>
</ol>
<p>A machine learning model generalizes if it fits the true signal <span class="math notranslate nohighlight">\(F\)</span>; it overfits if it learns the noise <span class="math notranslate nohighlight">\(E\)</span>.</p>
<p>Below, we visualize examples of underfitting, good fit, and overfitting:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ks</span><span class="p">,</span> <span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Underfitting&#39;</span><span class="p">,</span> <span class="s1">&#39;Good fit&#39;</span><span class="p">,</span> <span class="s1">&#39;Overfitting&#39;</span><span class="p">]</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">titles</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/lecture8-unsupervised-learning_58_0.png" src="_images/lecture8-unsupervised-learning_58_0.png" />
</div>
</div>
<p>The red diamond dots represent the learned clusters, and the circle dots represent datapoints in each class.</p>
<p>Underfitting is the case where you have more actual clusters than learned clusters, while overfitting is the case where you have less actual clusters than learned clusters.</p>
<section id="detecting-overfitting-and-underfitting">
<h3>8.3.5.1. Detecting Overfitting and Underfitting<a class="headerlink" href="#detecting-overfitting-and-underfitting" title="Permalink to this headline">#</a></h3>
<p>Though the figures above tell us if the model is overfitting or underfitting, in real-life scenarios when our data is not two-dimensional, visualizing the data and the cluster centers might not help us detect overfitting or underfitting.</p>
<p>Generally, in unsupervised learning, overfitting and underfitting are more difficult to quantify than in supervised learning because:</p>
<ul class="simple">
<li><p>Performance may depend on our intuition and require human evaluation</p></li>
<li><p>If we know the true labels, we can measure the accuracy of the clustering. But we do not have labels for unsupervised learning.</p></li>
</ul>
<p>If our model is probabilistic, one thing we can do to detect overfitting without labels is to compare the log-likelihood between the training set and a holdout set (next lecture!).</p>
</section>
<section id="the-elbow-method">
<h3>8.3.5.2. The Elbow Method<a class="headerlink" href="#the-elbow-method" title="Permalink to this headline">#</a></h3>
<p>Now, in terms of a practical method one can use to tune hyper-parameters in unsupervised learning, e.g., choosing <span class="math notranslate nohighlight">\(K\)</span> for <span class="math notranslate nohighlight">\(K\)</span>-means, there is the elbow method.</p>
<p>The elbow method is as follows:</p>
<ul class="simple">
<li><p>We plot the objective function as a function of the hyper-parameter <span class="math notranslate nohighlight">\(K\)</span>.</p></li>
<li><p>The “elbow” of the curve happens when its rate of decrease substantially slows down.</p></li>
<li><p>The “elbow’ is a good guess for the hyperparameter.</p></li>
</ul>
<p>In our example, the decrease in objective values slows down after <span class="math notranslate nohighlight">\(K=4\)</span>, and after that, the curve becomes just a line.</p>
<p>Below we plot the graph of objective function value (which is the objective from 8.2.4.3.) vs <span class="math notranslate nohighlight">\(K\)</span>. You can see that at <span class="math notranslate nohighlight">\(K = 4\)</span>, our objective value does not improve too much anymore even if we increase <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ks</span><span class="p">,</span> <span class="n">objs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">),</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">Ks</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">objs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">objs</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="n">objs</span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of clusters K&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Objective Function Value&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Objective Function Value&#39;)
</pre></div>
</div>
<img alt="_images/lecture8-unsupervised-learning_63_1.png" src="_images/lecture8-unsupervised-learning_63_1.png" />
</div>
</div>
</section>
<section id="reducing-overfitting">
<h3>8.3.5.3. Reducing Overfitting<a class="headerlink" href="#reducing-overfitting" title="Permalink to this headline">#</a></h3>
<p>Choosing hyper-parameters according to the elbow method is one thing you can do to avoid overfitting.</p>
<p>In general, there are multiple ways to control overfitting including:</p>
<ol class="simple">
<li><p>Reduce model complexity (e.g., reduce <span class="math notranslate nohighlight">\(K\)</span> in <span class="math notranslate nohighlight">\(K\)</span>-Means)</p></li>
<li><p>Penalize complexity in objective (e.g., penalize large <span class="math notranslate nohighlight">\(K\)</span>)</p></li>
<li><p>Use a probabilistic model and regularize it.</p></li>
</ol>
</section>
<section id="generalization-in-unsupervised-learning-summary">
<h3>8.3.5.4. Generalization in Unsupervised Learning: Summary<a class="headerlink" href="#generalization-in-unsupervised-learning-summary" title="Permalink to this headline">#</a></h3>
<p>As you can see, the concept of generalization applies to both supervised and unsupervised learning.</p>
<ul class="simple">
<li><p>In supervised learning, it is easier to quantify via accuracy.</p></li>
<li><p>In unsupervised learning, we may not be able to easily detect overfitting, but it still happens. So we have discussed practical methods to reduce overfitting.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="lecture6-naive-bayes.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 6: Generative Models and Naive Bayes</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture9-density-estimation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 9: Density Estimation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Cornell University<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>