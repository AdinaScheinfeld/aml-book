
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 3: Linear Regression &#8212; Applied Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 4: Classification and Logistic Regression" href="lecture4-classification.html" />
    <link rel="prev" title="Lecture 2: Supervised Machine Learning" href="lecture2-supervised-learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-supervised-learning.html">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6-naive-bayes.html">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8-unsupervised-learning.html">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9-density-estimation.html">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10-clustering.html">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture12-suppor-vector-machines.html">
   Lecture 12: Support Vector Machines
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/contents/lecture3-linear-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontents/lecture3-linear-regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/contents/lecture3-linear-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#calculus-review">
   3.1. Calculus Review
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recall-components-of-a-supervised-machine-learning-problem">
     Recall: Components of a Supervised Machine Learning Problem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization">
     3.1.1. Optimization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculus-review-derivatives-and-gradients">
     3.1.3. Calculus Review: Derivatives and Gradients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#derivatives">
       3.1.3.1. Derivatives
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partial-derivatives">
       3.1.3.2. Partial Derivatives
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-gradient">
       3.1.3.3. The Gradient
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     3.3. Gradient Descent
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intuition">
       3.3.1. Intuition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#notation">
       3.3.2. Notation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-in-linear-models">
   3.2. Gradient Descent in Linear Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-recap">
     Gradient Descent Recap
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-task-predicting-diabetes">
     3.1. A Supervised Learning Task: Predicting Diabetes
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-model-family">
     3.2. Linear Model Family
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-objective-mean-squared-error">
     3.3. An Objective: Mean Squared Error
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mean-squared-error-derivatives-and-gradients">
       3.3.1. Mean Squared Error: Derivatives and Gradients
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-for-linear-regression">
     3.4. Gradient Descent for Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ordinary-least-squares">
   3.3. Ordinary Least Squares
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-the-gradient">
     Review: The Gradient
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     3.3.1. Notation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#design-matrix">
       3.3.1.1. Design Matrix
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-target-vector">
       3.3.1.2. The Target Vector
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#squared-error-in-matrix-form">
       3.3.1.3. Squared Error in Matrix Form
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-normal-equations">
     3.3.2. The Normal Equations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-gradient-of-the-squared-error">
       3.3.2.1. The Gradient of the Squared Error
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-model-parameters-via-the-normal-equations">
       3.3.2.2. Computing Model Parameters via the Normal Equations
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#a-small-experiment">
         A Small Experiment
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     3.3.4. Ordinary Least Squares
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-linear-least-squares">
   3.4. Non-Linear Least Squares
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-polynomial-functions">
     3.4.1. Review: Polynomial Functions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modeling-non-linear-relationships-with-polynomial-regression">
     3.4.2. Modeling Non-Linear Relationships With Polynomial Regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#polynomial-features-for-diabetes-risk-prediction">
       3.4.2.1. Polynomial Features for Diabetes RIsk Prediction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diabetes-risk-prediction-with-polynomial-regression">
       3.4.2.2. Diabetes RIsk Prediction with Polynomial Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-polynomial-regression">
     3.4.3. Multivariate Polynomial Regression
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     3.4.4. Non-Linear Least Squares
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#towards-general-non-linear-features">
       3.4.4.2. Towards General Non-Linear Features
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-non-linear-least-squares">
       3.4.4.2. Algorithm: Non-Linear Least Squares
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 3: Linear Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#calculus-review">
   3.1. Calculus Review
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recall-components-of-a-supervised-machine-learning-problem">
     Recall: Components of a Supervised Machine Learning Problem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization">
     3.1.1. Optimization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculus-review-derivatives-and-gradients">
     3.1.3. Calculus Review: Derivatives and Gradients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#derivatives">
       3.1.3.1. Derivatives
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partial-derivatives">
       3.1.3.2. Partial Derivatives
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-gradient">
       3.1.3.3. The Gradient
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     3.3. Gradient Descent
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intuition">
       3.3.1. Intuition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#notation">
       3.3.2. Notation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-in-linear-models">
   3.2. Gradient Descent in Linear Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-recap">
     Gradient Descent Recap
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-task-predicting-diabetes">
     3.1. A Supervised Learning Task: Predicting Diabetes
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-model-family">
     3.2. Linear Model Family
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-objective-mean-squared-error">
     3.3. An Objective: Mean Squared Error
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mean-squared-error-derivatives-and-gradients">
       3.3.1. Mean Squared Error: Derivatives and Gradients
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-for-linear-regression">
     3.4. Gradient Descent for Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ordinary-least-squares">
   3.3. Ordinary Least Squares
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-the-gradient">
     Review: The Gradient
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     3.3.1. Notation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#design-matrix">
       3.3.1.1. Design Matrix
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-target-vector">
       3.3.1.2. The Target Vector
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#squared-error-in-matrix-form">
       3.3.1.3. Squared Error in Matrix Form
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-normal-equations">
     3.3.2. The Normal Equations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-gradient-of-the-squared-error">
       3.3.2.1. The Gradient of the Squared Error
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-model-parameters-via-the-normal-equations">
       3.3.2.2. Computing Model Parameters via the Normal Equations
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#a-small-experiment">
         A Small Experiment
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     3.3.4. Ordinary Least Squares
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-linear-least-squares">
   3.4. Non-Linear Least Squares
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-polynomial-functions">
     3.4.1. Review: Polynomial Functions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modeling-non-linear-relationships-with-polynomial-regression">
     3.4.2. Modeling Non-Linear Relationships With Polynomial Regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#polynomial-features-for-diabetes-risk-prediction">
       3.4.2.1. Polynomial Features for Diabetes RIsk Prediction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diabetes-risk-prediction-with-polynomial-regression">
       3.4.2.2. Diabetes RIsk Prediction with Polynomial Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-polynomial-regression">
     3.4.3. Multivariate Polynomial Regression
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     3.4.4. Non-Linear Least Squares
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#towards-general-non-linear-features">
       3.4.4.2. Towards General Non-Linear Features
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-non-linear-least-squares">
       3.4.4.2. Algorithm: Non-Linear Least Squares
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-3-linear-regression">
<h1>Lecture 3: Linear Regression<a class="headerlink" href="#lecture-3-linear-regression" title="Permalink to this headline">¶</a></h1>
<p>Our previous lecture defined the task of supervised learing. In this lecture, we will now define our first supervised learning algorithm—ordinary least squares (OLS). The OLS algorithm performs linear regression—it fits a linear model to a regression dataset.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="calculus-review">
<h1>3.1. Calculus Review<a class="headerlink" href="#calculus-review" title="Permalink to this headline">¶</a></h1>
<p>Before we present our first supervised learning algorithm, we will do a quick calculus review.</p>
<section id="recall-components-of-a-supervised-machine-learning-problem">
<h2>Recall: Components of a Supervised Machine Learning Problem<a class="headerlink" href="#recall-components-of-a-supervised-machine-learning-problem" title="Permalink to this headline">¶</a></h2>
<p>To apply supervised learning, we define a dataset and a learning algorithm.</p>
<div class="math notranslate nohighlight">
\[ \underbrace{\text{Dataset}}_\text{Features, Attributes, Targets} + \underbrace{\text{Learning Algorithm}}_\text{Model Class + Objective + Optimizer } \to \text{Predictive Model} \]</div>
<p>The output is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.</p>
</section>
<section id="optimization">
<h2>3.1.1. Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">¶</a></h2>
<p>Recall that a key part of a supervised learning algorithm is the optimizer, which takes</p>
<ul class="simple">
<li><p>an objective <span class="math notranslate nohighlight">\(J\)</span> (also called a loss function) and</p></li>
<li><p>a model class <span class="math notranslate nohighlight">\(\mathcal{M}\)</span></p></li>
</ul>
<p>It outputs a model <span class="math notranslate nohighlight">\(f \in \mathcal{M}\)</span> with the smallest value of the objective <span class="math notranslate nohighlight">\(J\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\min_{f \in \mathcal{M}} J(f)
\]</div>
<p>Intuitively, this is the function that bests “fits” the data on the training dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(i)}, y^{(i)}) \mid i = 1,2,...,n\}\)</span>.</p>
<p>We will need several tools from calculus to define an optimizer.</p>
<p>We will use the a quadratic function as our running example for an objective <span class="math notranslate nohighlight">\(J\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_function</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The cost function, J(theta).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">theta</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>We can visualize it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First construct a grid of theta1 parameter pairs and their corresponding</span>
<span class="c1"># cost function values.</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">f_vals</span> <span class="o">=</span> <span class="n">quadratic_function</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Theta&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Objective value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simple quadratic function&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Simple quadratic function&#39;)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_9_1.png" src="../_images/lecture3-linear-regression_9_1.png" />
</div>
</div>
</section>
<section id="calculus-review-derivatives-and-gradients">
<h2>3.1.3. Calculus Review: Derivatives and Gradients<a class="headerlink" href="#calculus-review-derivatives-and-gradients" title="Permalink to this headline">¶</a></h2>
<p>The key tool from calculus that we will use is the derivative and its extensions</p>
<section id="derivatives">
<h3>3.1.3.1. Derivatives<a class="headerlink" href="#derivatives" title="Permalink to this headline">¶</a></h3>
<p>Recall that the derivative</p>
<div class="math notranslate nohighlight">
\[
\frac{d f(\theta_0)}{d \theta}
\]</div>
<p>of a univariate function <span class="math notranslate nohighlight">\(f : \mathbb{R} \to \mathbb{R}\)</span> is the instantaneous rate of change of the function <span class="math notranslate nohighlight">\(f(\theta)\)</span> with respect to its parameter <span class="math notranslate nohighlight">\(\theta\)</span> at the point <span class="math notranslate nohighlight">\(\theta_0\)</span>.</p>
<p>The code below visualizes several derivatives of the quadratic function. Note that the derivatives are the slopes of the tangents at various points of the curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_derivative</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">theta</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span>

<span class="n">df0</span> <span class="o">=</span> <span class="n">quadratic_derivative</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">]]))</span> <span class="c1"># derivative at zero</span>
<span class="n">f0</span> <span class="o">=</span> <span class="n">quadratic_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">]]))</span>
<span class="n">line_length</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="n">line_length</span><span class="p">,</span> <span class="n">f0</span><span class="o">-</span><span class="n">line_length</span><span class="o">*</span><span class="n">df0</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="o">+</span><span class="n">line_length</span><span class="p">,</span> <span class="n">f0</span><span class="o">+</span><span class="n">line_length</span><span class="o">*</span><span class="n">df0</span><span class="p">),</span>
             <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arrowstyle&#39;</span><span class="p">:</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">},</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Theta&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Objective value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simple quadratic function&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Simple quadratic function&#39;)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_12_1.png" src="../_images/lecture3-linear-regression_12_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">df0s</span> <span class="o">=</span> <span class="n">quadratic_derivative</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span>
<span class="n">f0s</span> <span class="o">=</span> <span class="n">quadratic_function</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f_vals</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">f0</span><span class="p">,</span> <span class="n">df0</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pts</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">f0s</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">df0s</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">pt</span><span class="o">-</span><span class="n">line_length</span><span class="p">,</span> <span class="n">f0</span><span class="o">-</span><span class="n">line_length</span><span class="o">*</span><span class="n">df0</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">pt</span><span class="o">+</span><span class="n">line_length</span><span class="p">,</span> <span class="n">f0</span><span class="o">+</span><span class="n">line_length</span><span class="o">*</span><span class="n">df0</span><span class="p">),</span>
             <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arrowstyle&#39;</span><span class="p">:</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Theta&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Objective value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simple quadratic function&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Simple quadratic function&#39;)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_13_1.png" src="../_images/lecture3-linear-regression_13_1.png" />
</div>
</div>
</section>
<section id="partial-derivatives">
<h3>3.1.3.2. Partial Derivatives<a class="headerlink" href="#partial-derivatives" title="Permalink to this headline">¶</a></h3>
<p>The partial derivative</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f(\theta)}{\partial \theta_j}
\]</div>
<p>of a multivariate function <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> is the derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(\theta_j\)</span> while all the other dimensions <span class="math notranslate nohighlight">\(\theta_k\)</span> for <span class="math notranslate nohighlight">\(k\neq j\)</span> are fixed.</p>
</section>
<section id="the-gradient">
<h3>3.1.3.3. The Gradient<a class="headerlink" href="#the-gradient" title="Permalink to this headline">¶</a></h3>
<p>The gradient <span class="math notranslate nohighlight">\(\nabla f\)</span> is the vector of all the partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \nabla f (\theta) = \begin{bmatrix}
\frac{\partial f(\theta)}{\partial \theta_1} \\
\frac{\partial f(\theta)}{\partial \theta_2} \\
\vdots \\
\frac{\partial f(\theta)}{\partial \theta_d}
\end{bmatrix}.\end{split}\]</div>
<p>The <span class="math notranslate nohighlight">\(j\)</span>-th entry of the vector <span class="math notranslate nohighlight">\(\nabla f (\theta)\)</span> is the partial derivative <span class="math notranslate nohighlight">\(\frac{\partial f(\theta)}{\partial \theta_j}\)</span> of <span class="math notranslate nohighlight">\(f\)</span> with respect to the <span class="math notranslate nohighlight">\(j\)</span>-th component of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>We will use a quadratic function as a running example. We define this function in 2D below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_function2d</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Quadratic objective function, J(theta0, theta1).</span>
<span class="sd">    </span>
<span class="sd">    The inputs theta0, theta1 are 2d arrays and we evaluate</span>
<span class="sd">    the objective at each value theta0[i,j], theta1[i,j].</span>
<span class="sd">    We implement it this way so it&#39;s easier to plot the</span>
<span class="sd">    level curves of the function in 2d.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    theta0 (np.array): 2d array of first parameter theta0</span>
<span class="sd">    theta1 (np.array): 2d array of second parameter theta1</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    fvals (np.array): 2d array of objective function values</span>
<span class="sd">        fvals is the same dimension as theta0 and theta1.</span>
<span class="sd">        fvals[i,j] is the value at theta0[i,j] and theta1[i,j].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">theta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta0</span><span class="p">))</span>
    <span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta1</span><span class="p">))</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">theta1</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">theta0</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize this function. What you see below are the level curves: the value of the function is constant along each curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta0_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">101</span><span class="p">)</span>
<span class="n">theta1_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">101</span><span class="p">)</span>
<span class="n">theta_grid</span> <span class="o">=</span> <span class="n">theta0_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:],</span> <span class="n">theta1_grid</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">J_grid</span> <span class="o">=</span> <span class="n">quadratic_function2d</span><span class="p">(</span><span class="n">theta0_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:],</span> <span class="n">theta1_grid</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">theta0_grid</span><span class="p">,</span> <span class="n">theta1_grid</span><span class="p">)</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">J_grid</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contours</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-4.0, 7.0, -1.0, 4.0)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_19_1.png" src="../_images/lecture3-linear-regression_19_1.png" />
</div>
</div>
<p>Let’s write down the derivative of the quadratic function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_derivative2d</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Derivative of quadratic objective function.</span>
<span class="sd">    </span>
<span class="sd">    The inputs theta0, theta1 are 1d arrays and we evaluate</span>
<span class="sd">    the derivative at each value theta0[i], theta1[i].</span>

<span class="sd">    Parameters:</span>
<span class="sd">    theta0 (np.array): 1d array of first parameter theta0</span>
<span class="sd">    theta1 (np.array): 1d array of second parameter theta1</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    grads (np.array): 2d array of partial derivatives</span>
<span class="sd">        grads is of the same size as theta0 and theta1</span>
<span class="sd">        along first dimension and of size</span>
<span class="sd">        two along the second dimension.</span>
<span class="sd">        grads[i,j] is the j-th partial derivative </span>
<span class="sd">        at input theta0[i], theta1[i].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># this is the gradient of 0.5*((2*theta1-2)**2 + (theta0-3)**2)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">theta0</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">theta1</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">theta0</span><span class="p">),</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
</div>
<p>We can visualize the gradient. In two dimensions, the gradient can be visualized as an arrow. Note that the gradient is always orthogonal to the level curves (it’s something you prove in a Calculus 3 course).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta0_pts</span><span class="p">,</span> <span class="n">theta1_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.35</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">2.75</span><span class="p">])</span>
<span class="n">dfs</span> <span class="o">=</span> <span class="n">quadratic_derivative2d</span><span class="p">(</span><span class="n">theta0_pts</span><span class="p">,</span> <span class="n">theta1_pts</span><span class="p">)</span>
<span class="n">line_length</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">contours</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">J_grid</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">theta0_pt</span><span class="p">,</span> <span class="n">theta1_pt</span><span class="p">,</span> <span class="n">df0</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">theta0_pts</span><span class="p">,</span> <span class="n">theta1_pts</span><span class="p">,</span> <span class="n">dfs</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">theta0_pt</span><span class="p">,</span> <span class="n">theta1_pt</span><span class="p">),</span> 
                     <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">theta0_pt</span><span class="o">-</span><span class="n">line_length</span><span class="o">*</span><span class="n">df0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta1_pt</span><span class="o">-</span><span class="n">line_length</span><span class="o">*</span><span class="n">df0</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                     <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arrowstyle&#39;</span><span class="p">:</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">theta0_pts</span><span class="p">,</span> <span class="n">theta1_pts</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contours</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Theta0&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Theta1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradients of the quadratic function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-4.0, 7.0, -1.0, 4.0)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_23_1.png" src="../_images/lecture3-linear-regression_23_1.png" />
</div>
</div>
</section>
</section>
<section id="gradient-descent">
<h2>3.3. Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Next, we will use gradients to define an important algorithm called <em>gradient descent</em>.</p>
<section id="intuition">
<h3>3.3.1. Intuition<a class="headerlink" href="#intuition" title="Permalink to this headline">¶</a></h3>
<p>Gradient descent is a very common optimization algorithm used in machine learning.</p>
<p>The intuition behind gradient descent is to repeatedly obtain the gradient to determine the direction in which the function decreases most steeply and take a step in that direction.</p>
</section>
<section id="notation">
<h3>3.3.2. Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h3>
<p>More formally, if we want to optimize <span class="math notranslate nohighlight">\(J(\theta)\)</span>, we start with an initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span> for the parameters and repeat the following update until <span class="math notranslate nohighlight">\(\theta\)</span> is no longer changing:</p>
<div class="math notranslate nohighlight">
\[ 
\theta_i := \theta_{i-1} - \alpha \cdot \nabla J(\theta_{i-1}). 
\]</div>
<p>In code, this method may look as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">,</span> <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">random_initialization</span><span class="p">()</span>
<span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta_prev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">convergence_threshold</span><span class="p">:</span>
    <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_prev</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">theta_prev</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above algorithm, we stop when <span class="math notranslate nohighlight">\(||\theta_i - \theta_{i-1}||\)</span> is small.</p>
<p>It’s easy to implement this function in <code class="docutils literal notranslate"><span class="pre">numpy</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">convergence_threshold</span> <span class="o">=</span> <span class="mf">2e-1</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">2e-1</span>
<span class="n">theta</span><span class="p">,</span> <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">opt_pts</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
<span class="n">opt_grads</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta_prev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">convergence_threshold</span><span class="p">:</span>
    <span class="c1"># we repeat this while the value of the function is decreasing</span>
    <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">quadratic_derivative2d</span><span class="p">(</span><span class="o">*</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_prev</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">gradient</span>
    <span class="n">opt_pts</span> <span class="o">+=</span> <span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
    <span class="n">opt_grads</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gradient</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<p>We can now visualize gradient descent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">opt_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">opt_pts</span><span class="p">)</span>
<span class="n">opt_grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">opt_grads</span><span class="p">)</span>

<span class="n">contours</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">J_grid</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contours</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">opt_pts</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">opt_pts</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

<span class="k">for</span> <span class="n">opt_pt</span><span class="p">,</span> <span class="n">opt_grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">opt_pts</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">opt_pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">opt_pt</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> 
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">opt_pt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mf">0.8</span><span class="o">*</span><span class="n">step_size</span><span class="o">*</span><span class="n">opt_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">opt_pt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.8</span><span class="o">*</span><span class="n">step_size</span><span class="o">*</span><span class="n">opt_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                 <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arrowstyle&#39;</span><span class="p">:</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-4.0, 7.0, -1.0, 4.0)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_29_1.png" src="../_images/lecture3-linear-regression_29_1.png" />
</div>
</div>
<p>Each gradient is orthognal to the level curve, and the minues one times the gradient (the “steepest descent direction”) points inwards toward the minimum. By repeatedly following the steepest descent direction, we arrive at the minimum.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="gradient-descent-in-linear-models">
<h1>3.2. Gradient Descent in Linear Models<a class="headerlink" href="#gradient-descent-in-linear-models" title="Permalink to this headline">¶</a></h1>
<p>To better understand gradients and gradient descent, let’s further explore their properties in the context of linear models.</p>
<p>Specifically, we will define a combination of model class, objective, and optimizer—this will give us our first supervised learning algorithm.</p>
<section id="gradient-descent-recap">
<h2>Gradient Descent Recap<a class="headerlink" href="#gradient-descent-recap" title="Permalink to this headline">¶</a></h2>
<p>A key ingredient will be gradient descent. Recall that if we want to optimize <span class="math notranslate nohighlight">\(J(\theta)\)</span>, we start with an initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span> for the parameters and repeat the following update:</p>
<div class="math notranslate nohighlight">
\[ 
\theta_i := \theta_{i-1} - \alpha \cdot \nabla_\theta J(\theta_{i-1}). 
\]</div>
<p>As code, this method may look as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">,</span> <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">random_initialization</span><span class="p">()</span>
<span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta_prev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">convergence_threshold</span><span class="p">:</span>
    <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_prev</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">theta_prev</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="a-supervised-learning-task-predicting-diabetes">
<h2>3.1. A Supervised Learning Task: Predicting Diabetes<a class="headerlink" href="#a-supervised-learning-task-predicting-diabetes" title="Permalink to this headline">¶</a></h2>
<p>In this section, we are going to again use the UCI Diabetes Dataset.</p>
<ul class="simple">
<li><p>For each patient we have a access to their BMI and an estimate of diabetes risk (from 0-400).</p></li>
<li><p>We are interested in understanding how BMI affects an individual’s diabetes risk.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># Load the diabetes dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># add an extra column of onens</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Collect 20 data points and only use bmi dimension</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">]]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span> <span class="o">/</span> <span class="mi">300</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Diabetes Risk&#39;)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_34_1.png" src="../_images/lecture3-linear-regression_34_1.png" />
</div>
</div>
</section>
<section id="linear-model-family">
<h2>3.2. Linear Model Family<a class="headerlink" href="#linear-model-family" title="Permalink to this headline">¶</a></h2>
<p>Recall that a linear model has the form</p>
<!-- \begin{align*}
y & = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + ... + \theta_d \cdot x_d
\end{align*} -->
<div class="math notranslate nohighlight">
\[
y=\theta_0+\theta_1 \cdot x_1+\theta_2 \cdot x_2+\ldots+\theta_d \cdot x_d
\]</div>
<p>where <span class="math notranslate nohighlight">\(x \in \mathbb{R}^d\)</span> is a vector of features and <span class="math notranslate nohighlight">\(y\)</span> is the target. The <span class="math notranslate nohighlight">\(\theta_j\)</span> are the <em>parameters</em> of the model.</p>
<p>By using the notation <span class="math notranslate nohighlight">\(x_0 = 1\)</span>, we can represent the model in a vectorized form</p>
<div class="math notranslate nohighlight">
\[ 
f_\theta(x) = \sum_{j=0}^d \theta_j \cdot x_j = \theta^\top x. 
\]</div>
<p>Let’s define our model in Python.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The linear model we are trying to fit.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    theta (np.array): d-dimensional vector of parameters</span>
<span class="sd">    X (np.array): (n,d)-dimensional data matrix</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    y_pred (np.array): n-dimensional vector of predicted targets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="an-objective-mean-squared-error">
<h2>3.3. An Objective: Mean Squared Error<a class="headerlink" href="#an-objective-mean-squared-error" title="Permalink to this headline">¶</a></h2>
<p>We pick <span class="math notranslate nohighlight">\(\theta\)</span> to minimize the mean squared error (MSE). Slight variants of this objective are also known as the residual sum of squares (RSS) or the sum of squared residuals (SSR).</p>
<div class="math notranslate nohighlight">
\[
J(\theta)= \frac{1}{2n} \sum_{i=1}^n(y^{(i)}-\theta^\top x^{(i)})^2
\]</div>
<p>In other words, we are looking for the best compromise in <span class="math notranslate nohighlight">\(\theta\)</span> over all the data points.</p>
<p>Let’s implement the mean squared error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The cost function, J, describing the goodness of fit.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    theta (np.array): d-dimensional vector of parameters</span>
<span class="sd">    X (np.array): (n,d)-dimensional design matrix</span>
<span class="sd">    y (np.array): n-dimensional vector of targets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="mean-squared-error-derivatives-and-gradients">
<h3>3.3.1. Mean Squared Error: Derivatives and Gradients<a class="headerlink" href="#mean-squared-error-derivatives-and-gradients" title="Permalink to this headline">¶</a></h3>
<p>Let’s work out the derivatives for <span class="math notranslate nohighlight">\(\frac{1}{2} \left( f_\theta(x^{(i)}) - y^{(i)} \right)^2,\)</span> the MSE of a linear model <span class="math notranslate nohighlight">\(f_\theta\)</span> for one training example <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span>, which we denote <span class="math notranslate nohighlight">\(J^{(i)}(\theta)\)</span>.</p>
<!-- \begin{align*}
\frac{\partial}{\partial \theta_j} J^{(i)}(\theta) & = \frac{\partial}{\partial \theta_j} \left(\frac{1}{2} \left( f_\theta(x^{(i)}) - y^{(i)} \right)^2\right) \\
& = \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot \frac{\partial}{\partial \theta_j} \left( f_\theta(x^{(i)}) - y^{(i)} \right) \\
& = \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot \frac{\partial}{\partial \theta_j} \left( \sum_{k=0}^d \theta_k \cdot x^{(i)}_k - y^{(i)} \right) \\
& = \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_j
\end{align*} -->
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \theta_j} J^{(i)}(\theta) &amp; =\frac{\partial}{\partial \theta_j}\left(\frac{1}{2}\left(f_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2\right) \\
&amp; =\left(f_\theta\left(x^{(i)}\right)-y^{(i)}\right) \cdot \frac{\partial}{\partial \theta_j}\left(f_\theta\left(x^{(i)}\right)-y^{(i)}\right) \\
&amp; =\left(f_\theta\left(x^{(i)}\right)-y^{(i)}\right) \cdot \frac{\partial}{\partial \theta_j}\left(\sum_{k=0}^d \theta_k \cdot x_k^{(i)}-y^{(i)}\right) \\
&amp; =\left(f_\theta\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_j^{(i)}
\end{aligned}
\end{split}\]</div>
<p>We can use this derivation to obtain an expression for the gradient of the MSE for a linear model</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\small
{\tiny \nabla_\theta J^{(i)} (\theta)} = \begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \\
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \\
\vdots \\
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}
=
\begin{bmatrix}
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_0 \\
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_1 \\
\vdots \\
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}_d
\end{bmatrix}
=
\left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}
\end{align*}
\end{split}\]</div>
<p>Note that the MSE over the entire dataset is <span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{n}\sum_{i=1}^n J^{(i)}(\theta)\)</span>. Therefore:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_\theta J (\theta) = \begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_0} \\
\frac{\partial J(\theta)}{\partial \theta_1} \\
\vdots \\
\frac{\partial J(\theta)}{\partial \theta_d}
\end{bmatrix}
=
\frac{1}{n}\sum_{i=1}^n
\begin{bmatrix}
\frac{\partial J^{(i)}(\theta)}{\partial \theta_0} \\
\frac{\partial J^{(i)}(\theta)}{\partial \theta_1} \\
\vdots \\
\frac{\partial J^{(i)}(\theta)}{\partial \theta_d}
\end{bmatrix}
=
\frac{1}{n} \sum_{i=1}^n \left( f_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}
\end{align*}
\end{split}\]</div>
<p>Let’s implement this gradient.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse_gradient</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The gradient of the cost function.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    theta (np.array): d-dimensional vector of parameters</span>
<span class="sd">    X (np.array): (n,d)-dimensional design matrix</span>
<span class="sd">    y (np.array): n-dimensional vector of targets</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    grad (np.array): d-dimensional gradient of the MSE</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="gradient-descent-for-linear-regression">
<h2>3.4. Gradient Descent for Linear Regression<a class="headerlink" href="#gradient-descent-for-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Putting this together with the gradient descent algorithm, we obtain a learning method for training linear models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">,</span> <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">random_initialization</span><span class="p">()</span>
<span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">J</span><span class="p">(</span><span class="n">theta_prev</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">conv_threshold</span><span class="p">:</span>
    <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_prev</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p>This update rule is also known as the Least Mean Squares (LMS) or Widrow-Hoff learning rule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">4e-1</span>
<span class="n">theta</span><span class="p">,</span> <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,)</span>
<span class="n">opt_pts</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span>
<span class="n">opt_grads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta_prev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%d</span><span class="s1">. MSE: </span><span class="si">%.6f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
    <span class="n">theta_prev</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">mse_gradient</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_prev</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">gradient</span>
    <span class="n">opt_pts</span> <span class="o">+=</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span>
    <span class="n">opt_grads</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gradient</span><span class="p">]</span>
    <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration 0. MSE: 0.171729
Iteration 100. MSE: 0.014765
Iteration 200. MSE: 0.014349
Iteration 300. MSE: 0.013997
Iteration 400. MSE: 0.013701
</pre></div>
</div>
</div>
</div>
<p>This procedure yields a first concrete example of a supervised learning algorithm. The final weights returned by the ablove algorithm yield a model, which we can visualize below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,)])</span>
<span class="n">y_line</span> <span class="o">=</span> <span class="n">opt_pts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_line</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_line</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Diabetes Risk&#39;)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_49_1.png" src="../_images/lecture3-linear-regression_49_1.png" />
</div>
</div>
<p>Note that the result is similar to what we obtained in the previous lecture by invoking the optimizer in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ordinary-least-squares">
<h1>3.3. Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h1>
<p>In practice, there is a more effective way than gradient descent to find linear model parameters.</p>
<p>This method will produce our first non-toy supervised learning algorithm: Ordinary Least Squares.</p>
<section id="review-the-gradient">
<h2>Review: The Gradient<a class="headerlink" href="#review-the-gradient" title="Permalink to this headline">¶</a></h2>
<p>The gradient <span class="math notranslate nohighlight">\(\nabla_\theta f\)</span> further extends the derivative to multivariate functions <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>, and is defined at a point <span class="math notranslate nohighlight">\(\theta_0\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split} \nabla_\theta f (\theta_0) = \begin{bmatrix}
\frac{\partial f(\theta_0)}{\partial \theta_1} \\
\frac{\partial f(\theta_0)}{\partial \theta_2} \\
\vdots \\
\frac{\partial f(\theta_0)}{\partial \theta_d}
\end{bmatrix}.\end{split}\]</div>
<p>In other words, the <span class="math notranslate nohighlight">\(j\)</span>-th entry of the vector <span class="math notranslate nohighlight">\(\nabla_\theta f (\theta_0)\)</span> is the partial derivative <span class="math notranslate nohighlight">\(\frac{\partial f(\theta_0)}{\partial \theta_j}\)</span> of <span class="math notranslate nohighlight">\(f\)</span> with respect to the <span class="math notranslate nohighlight">\(j\)</span>-th component of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</section>
<section id="id1">
<h2>3.3.1. Notation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>First, we start by introducing some notation.
Suppose that we have a dataset of size <span class="math notranslate nohighlight">\(n\)</span> (e.g., <span class="math notranslate nohighlight">\(n\)</span> patients), indexed by <span class="math notranslate nohighlight">\(i=1,2,...,n\)</span>. Each <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is a vector of <span class="math notranslate nohighlight">\(d\)</span> features.</p>
<section id="design-matrix">
<h3>3.3.1.1. Design Matrix<a class="headerlink" href="#design-matrix" title="Permalink to this headline">¶</a></h3>
<p>Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span>, of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split} X = \begin{bmatrix}
x^{(1)}_1 &amp; x^{(1)}_2 &amp; \ldots &amp; x^{(1)}_d \\
x^{(2)}_1 &amp; x^{(2)}_2 &amp; \ldots &amp; x^{(2)}_d \\
\vdots \\
x^{(n)}_1 &amp; x^{(n)}_2 &amp; \ldots &amp; x^{(n)}_d
\end{bmatrix}
=
\begin{bmatrix}
- &amp; (x^{(1)})^\top &amp; - \\
- &amp; (x^{(2)})^\top &amp; - \\
&amp; \vdots &amp; \\
- &amp; (x^{(n)})^\top &amp; - \\
\end{bmatrix}
.\end{split}\]</div>
<p>We can view the design matrix for the diabetes dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>one</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>422</th>
      <td>-0.078165</td>
      <td>0.050680</td>
      <td>0.077863</td>
      <td>0.052858</td>
      <td>0.078236</td>
      <td>0.064447</td>
      <td>0.026550</td>
      <td>-0.002592</td>
      <td>0.040672</td>
      <td>-0.009362</td>
      <td>1</td>
    </tr>
    <tr>
      <th>423</th>
      <td>0.009016</td>
      <td>0.050680</td>
      <td>-0.039618</td>
      <td>0.028758</td>
      <td>0.038334</td>
      <td>0.073529</td>
      <td>-0.072854</td>
      <td>0.108111</td>
      <td>0.015567</td>
      <td>-0.046641</td>
      <td>1</td>
    </tr>
    <tr>
      <th>424</th>
      <td>0.001751</td>
      <td>0.050680</td>
      <td>0.011039</td>
      <td>-0.019442</td>
      <td>-0.016704</td>
      <td>-0.003819</td>
      <td>-0.047082</td>
      <td>0.034309</td>
      <td>0.024053</td>
      <td>0.023775</td>
      <td>1</td>
    </tr>
    <tr>
      <th>425</th>
      <td>-0.078165</td>
      <td>-0.044642</td>
      <td>-0.040696</td>
      <td>-0.081414</td>
      <td>-0.100638</td>
      <td>-0.112795</td>
      <td>0.022869</td>
      <td>-0.076395</td>
      <td>-0.020289</td>
      <td>-0.050783</td>
      <td>1</td>
    </tr>
    <tr>
      <th>426</th>
      <td>0.030811</td>
      <td>0.050680</td>
      <td>-0.034229</td>
      <td>0.043677</td>
      <td>0.057597</td>
      <td>0.068831</td>
      <td>-0.032356</td>
      <td>0.057557</td>
      <td>0.035462</td>
      <td>0.085907</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="the-target-vector">
<h3>3.3.1.2. The Target Vector<a class="headerlink" href="#the-target-vector" title="Permalink to this headline">¶</a></h3>
<p>Similarly, we can vectorize the target variables into a vector <span class="math notranslate nohighlight">\(y \in \mathbb{R}^n\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 y = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(n)}
\end{bmatrix}.
\end{split}\]</div>
</section>
<section id="squared-error-in-matrix-form">
<h3>3.3.1.3. Squared Error in Matrix Form<a class="headerlink" href="#squared-error-in-matrix-form" title="Permalink to this headline">¶</a></h3>
<p>Recall that we may fit a linear model by choosing <span class="math notranslate nohighlight">\(\theta\)</span> that minimizes the squared error:</p>
<div class="math notranslate nohighlight">
\[
J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^\top x^{(i)})^2
\]</div>
<p>We can write this sum in matrix-vector form as:</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \frac{1}{2} (y-X\theta)^\top(y-X\theta) = \frac{1}{2} \|y-X\theta\|^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(X\)</span> is the design matrix and <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> denotes the Euclidean norm.</p>
</section>
</section>
<section id="the-normal-equations">
<h2>3.3.2. The Normal Equations<a class="headerlink" href="#the-normal-equations" title="Permalink to this headline">¶</a></h2>
<p>Next, we will use this notation to derive a very important formula in supervised learning—the normal equations.</p>
<section id="the-gradient-of-the-squared-error">
<h3>3.3.2.1. The Gradient of the Squared Error<a class="headerlink" href="#the-gradient-of-the-squared-error" title="Permalink to this headline">¶</a></h3>
<p>Let’s begin by observing that we can compute the gradient of the mean squared error as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_\theta J(\theta) 
&amp; = \nabla_\theta \frac{1}{2} (X \theta - y)^\top  (X \theta - y) \\
&amp; = \frac{1}{2} \nabla_\theta \left( (X \theta)^\top  (X \theta) - (X \theta)^\top y - y^\top (X \theta) + y^\top y \right) \\
&amp; = \frac{1}{2} \nabla_\theta \left( \theta^\top  (X^\top X) \theta - 2(X \theta)^\top y \right) \\
&amp; = \frac{1}{2} \left( 2(X^\top X) \theta - 2X^\top y \right) \\
&amp; = (X^\top X) \theta - X^\top y
\end{align*}
\end{split}\]</div>
<p>We used the facts that <span class="math notranslate nohighlight">\(a^\top b = b^\top a\)</span> (line 3), that <span class="math notranslate nohighlight">\(\nabla_x b^\top x = b\)</span> (line 4), and that <span class="math notranslate nohighlight">\(\nabla_x x^\top A x = 2 A x\)</span> for a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> (line 4).</p>
</section>
<section id="computing-model-parameters-via-the-normal-equations">
<h3>3.3.2.2. Computing Model Parameters via the Normal Equations<a class="headerlink" href="#computing-model-parameters-via-the-normal-equations" title="Permalink to this headline">¶</a></h3>
<p>We know from calculus that a function is minimized when its derivative is set to zero. In our case, our objective function is a (multivariate) quadratic; hence it only has one minimum, which is the global minimum.</p>
<p>Setting the above derivative to zero, we obtain the <em>normal equations</em>:</p>
<div class="math notranslate nohighlight">
\[ 
(X^\top X) \theta = X^\top y.
\]</div>
<p>Hence, the value <span class="math notranslate nohighlight">\(\theta^*\)</span> that minimizes this objective is given by:</p>
<div class="math notranslate nohighlight">
\[ 
\theta^* = (X^\top X)^{-1} X^\top y.
\]</div>
<p>Note that we assumed that the matrix <span class="math notranslate nohighlight">\((X^\top X)\)</span> is invertible; we will soon see a simple way of dealing with non-invertible matrices.</p>
<p>We also add a few important remarks:</p>
<ul class="simple">
<li><p>The formula <span class="math notranslate nohighlight">\(\theta^* = (X^\top X)^{-1} X^\top y\)</span> yields an <em>analytical formula</em> for the parameters that minimize our mean squared error objective.</p></li>
<li><p>In contrast, gradient descent is an iterative optimization procedure that is often slower on small datasets.</p></li>
<li><p>The analytical approach is what is most commonly used in practice.</p></li>
</ul>
<section id="a-small-experiment">
<h4>A Small Experiment<a class="headerlink" href="#a-small-experiment" title="Permalink to this headline">¶</a></h4>
<p>Let’s now apply the normal equations to the Diabetes dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">theta_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">theta_best_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">theta_best</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">theta_best_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>one</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-3.888868</td>
      <td>204.648785</td>
      <td>-64.289163</td>
      <td>-262.796691</td>
      <td>14003.726808</td>
      <td>-11798.307781</td>
      <td>-5892.15807</td>
      <td>-1136.947646</td>
      <td>-2736.597108</td>
      <td>-393.879743</td>
      <td>155.698998</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The above dataframe contains the parameters thetea that minimize the mean squared error objective.</p>
<p>We can now use our estimate of theta to compute predictions for 3 new data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect 3 data points for testing</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>

<span class="c1"># generate predictions on the new patients</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta_best</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize these predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Model&#39;</span><span class="p">,</span> <span class="s1">&#39;Prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;Initial patients&#39;</span><span class="p">,</span> <span class="s1">&#39;New patients&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x128d89668&gt;
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_69_1.png" src="../_images/lecture3-linear-regression_69_1.png" />
</div>
</div>
<p>Again, red cross are predictions, red dots are true values. The model gives us very accurate predictions.</p>
<p>Note again that while we are visualizing the data with the BMI on the x-axis, the model actually uses all of the features.</p>
</section>
</section>
</section>
<section id="id2">
<h2>3.3.4. Ordinary Least Squares<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>The above procedure gives us our first non-toy supervised learning algorithm—ordinary least squares (OLS). We can succinctly define OLS in terms of the algorithm components we defined in the last lecture.</p>
<ul class="simple">
<li><p><strong>Type</strong>: Supervised learning (regression)</p></li>
<li><p><strong>Model family</strong>: Linear models</p></li>
<li><p><strong>Objective function</strong>: Mean squared error</p></li>
<li><p><strong>Optimizer</strong>: Normal equations</p></li>
</ul>
<p>We will define many algorithms in this way throughout the course.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="non-linear-least-squares">
<h1>3.4. Non-Linear Least Squares<a class="headerlink" href="#non-linear-least-squares" title="Permalink to this headline">¶</a></h1>
<p>Ordinary Least Squares can only learn linear relationships in the data. Can we also use it to model more complex relationships?</p>
<section id="review-polynomial-functions">
<h2>3.4.1. Review: Polynomial Functions<a class="headerlink" href="#review-polynomial-functions" title="Permalink to this headline">¶</a></h2>
<p>We start with a brief review of polynomials.
Recall that a polynomial of degree <span class="math notranslate nohighlight">\(p\)</span> is a function of the form</p>
<div class="math notranslate nohighlight">
\[
a_p x^p + a_{p-1} x^{p-1} + ... + a_{1} x + a_0.
\]</div>
<p>Below are some examples of polynomial functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">x_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="s1">&#39;131&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Quadratic Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^2$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="s1">&#39;132&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cubic Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^3$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="s1">&#39;133&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Third Degree Polynomial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x_vars</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x_vars</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^3 + 2 x^2 + x + 1$&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x128ed2ac8&gt;
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_74_1.png" src="../_images/lecture3-linear-regression_74_1.png" />
</div>
</div>
</section>
<section id="modeling-non-linear-relationships-with-polynomial-regression">
<h2>3.4.2. Modeling Non-Linear Relationships With Polynomial Regression<a class="headerlink" href="#modeling-non-linear-relationships-with-polynomial-regression" title="Permalink to this headline">¶</a></h2>
<p>Note that the set of <span class="math notranslate nohighlight">\(p\)</span>-th degree polynomials forms a linear model with parameters <span class="math notranslate nohighlight">\(a_p, a_{p-1}, ..., a_0\)</span>.
This means we can use our algorithms for linear models to learn non-linear features!</p>
<p>Specifically, given a one-dimensional continuous variable <span class="math notranslate nohighlight">\(x\)</span>, we can define the <em>polynomial feature</em> function <span class="math notranslate nohighlight">\(\phi : \mathbb{R} \to \mathbb{R}^{p+1}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\phi(x) = \begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}.
\end{split}\]</div>
<p>The class of models of the form</p>
<div class="math notranslate nohighlight">
\[ 
f_\theta(x) := \sum_{j=0}^p \theta_p x^p = \theta^\top \phi(x) 
\]</div>
<p>with parameters <span class="math notranslate nohighlight">\(\theta\)</span> and polynomial features <span class="math notranslate nohighlight">\(\phi\)</span> is the set of <span class="math notranslate nohighlight">\(p\)</span>-degree polynomials.</p>
<ul class="simple">
<li><p>This model is non-linear in the input variable <span class="math notranslate nohighlight">\(x\)</span>, meaning that we can model complex data relationships.</p></li>
</ul>
<ul class="simple">
<li><p>It is a linear model as a function of the parameters <span class="math notranslate nohighlight">\(\theta\)</span>, meaning that we can use our familiar ordinary least squares algorithm to learn these features.</p></li>
</ul>
<section id="polynomial-features-for-diabetes-risk-prediction">
<h3>3.4.2.1. Polynomial Features for Diabetes RIsk Prediction<a class="headerlink" href="#polynomial-features-for-diabetes-risk-prediction" title="Permalink to this headline">¶</a></h3>
<p>In this section, we are going to again use the UCI Diabetes Dataset.</p>
<ul class="simple">
<li><p>For each patient we have a access to a measurement of their body mass index (BMI) and a quantiative diabetes risk score (from 0-300).</p></li>
<li><p>We are interested in understanding how BMI affects an individual’s diabetes risk.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># Load the diabetes dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># add an extra column of onens</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Collect 20 data points</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Diabetes Risk&#39;)
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_80_1.png" src="../_images/lecture3-linear-regression_80_1.png" />
</div>
</div>
<p>Let’s now obtain linear features for this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_bmi</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]]</span>

<span class="n">X_bmi_p3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_bmi</span><span class="p">,</span> <span class="n">X_bmi</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">X_bmi</span><span class="o">**</span><span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_bmi_p3</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">,</span> <span class="s1">&#39;bmi2&#39;</span><span class="p">,</span> <span class="s1">&#39;bmi3&#39;</span><span class="p">]</span>
<span class="n">X_bmi_p3</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">X_bmi_p3</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bmi</th>
      <th>bmi2</th>
      <th>bmi3</th>
      <th>one</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>422</th>
      <td>0.077863</td>
      <td>0.006063</td>
      <td>0.000472</td>
      <td>1</td>
    </tr>
    <tr>
      <th>423</th>
      <td>-0.039618</td>
      <td>0.001570</td>
      <td>-0.000062</td>
      <td>1</td>
    </tr>
    <tr>
      <th>424</th>
      <td>0.011039</td>
      <td>0.000122</td>
      <td>0.000001</td>
      <td>1</td>
    </tr>
    <tr>
      <th>425</th>
      <td>-0.040696</td>
      <td>0.001656</td>
      <td>-0.000067</td>
      <td>1</td>
    </tr>
    <tr>
      <th>426</th>
      <td>-0.034229</td>
      <td>0.001172</td>
      <td>-0.000040</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="diabetes-risk-prediction-with-polynomial-regression">
<h3>3.4.2.2. Diabetes RIsk Prediction with Polynomial Regression<a class="headerlink" href="#diabetes-risk-prediction-with-polynomial-regression" title="Permalink to this headline">¶</a></h3>
<p>By training a linear model on this featurization of the diabetes dataset, we can obtain a polynomial model of diabetest risk as a function of BMI.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a linear regression</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_bmi_p3</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_bmi_p3</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_bmi_p3</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Show the learned polynomial curve</span>
<span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">x_line_p3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_line</span><span class="p">,</span> <span class="n">x_line</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x_line</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">x_line_p3</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_bmi</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1292c99e8&gt;]
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_84_1.png" src="../_images/lecture3-linear-regression_84_1.png" />
</div>
</div>
<p>Notice how the above third-degree polynomial represents a much better fit to the data!</p>
</section>
</section>
<section id="multivariate-polynomial-regression">
<h2>3.4.3. Multivariate Polynomial Regression<a class="headerlink" href="#multivariate-polynomial-regression" title="Permalink to this headline">¶</a></h2>
<p>We can construct non-linear functions of multiple variables by using multivariate polynomials.</p>
<p>For example, a polynomial of degree <span class="math notranslate nohighlight">\(2\)</span> over two variables <span class="math notranslate nohighlight">\(x_1, x_2\)</span> is a function of the form</p>
<!-- $$
a_{20} x_1^2 + a_{10} x_1 + a_{02} x_2^2 + a_{01} x_2 + a_{22} x_1^2 x_2^2 + a_{21} x_1^2 x_2 + a_{12} x_1 x_2^2 + a_11 x_1 x_2 + a_{00}.
$$ -->
<div class="math notranslate nohighlight">
\[
a_{20} x_1^2 + a_{10} x_1 + a_{02} x_2^2 + a_{01} x_2 + a_{11} x_1 x_2 + a_{00}.
\]</div>
<p>In general, a polynomial of degree <span class="math notranslate nohighlight">\(p\)</span> over two variables <span class="math notranslate nohighlight">\(x_1, x_2\)</span> is a function of the form
$<span class="math notranslate nohighlight">\(
f(x_1, x_2) = \sum_{i,j \geq 0 : i+j \leq p} a_{ij} x_1^i x_2^j.
\)</span>$</p>
<p>In our two-dimensional example, this corresponds to a feature function <span class="math notranslate nohighlight">\(\phi : \mathbb{R}^2 \to \mathbb{R}^6\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\phi(x) = \begin{bmatrix}
1 \\
x_1 \\
x_1^2 \\
x_2 \\
x_2^2 \\
x_1 x_2
\end{bmatrix}.
\end{split}\]</div>
<p>The same approach can be used to specify polynomials of any degree and over any number of variables.</p>
</section>
<section id="id3">
<h2>3.4.4. Non-Linear Least Squares<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>We can further extend this approach by going from polynomial to general non-linear features. Combining non-linear features with normal equations gives us an algorithm called non-linear least squares.</p>
<section id="towards-general-non-linear-features">
<h3>3.4.4.2. Towards General Non-Linear Features<a class="headerlink" href="#towards-general-non-linear-features" title="Permalink to this headline">¶</a></h3>
<p>Any non-linear feature map <span class="math notranslate nohighlight">\(\phi(x) : \mathbb{R}^d \to \mathbb{R}^p\)</span> can be used to obtain general models of the form</p>
<div class="math notranslate nohighlight">
\[ 
f_\theta(x) := \theta^\top \phi(x) 
\]</div>
<p>that are highly non-linear in <span class="math notranslate nohighlight">\(x\)</span> but linear in <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>For example, here is a way of modeling complex periodic functions via a sum of sines and cosines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">x_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="s1">&#39;131&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cosine Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_vars</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$cos(x)$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="s1">&#39;132&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sine Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x_vars</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$x^3$&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="s1">&#39;133&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Combination of Sines and Cosines&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vars</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_vars</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x_vars</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x_vars</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;$cos(x) + sin(2x) + cos(4x)$&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x129571160&gt;
</pre></div>
</div>
<img alt="../_images/lecture3-linear-regression_91_1.png" src="../_images/lecture3-linear-regression_91_1.png" />
</div>
</div>
</section>
<section id="algorithm-non-linear-least-squares">
<h3>3.4.4.2. Algorithm: Non-Linear Least Squares<a class="headerlink" href="#algorithm-non-linear-least-squares" title="Permalink to this headline">¶</a></h3>
<p>Because the above model is still linear in <span class="math notranslate nohighlight">\(\theta\)</span>, we can apply the normal equations. This yields the following algorithm, which we again characterize via its components.</p>
<ul class="simple">
<li><p><strong>Type</strong>: Supervised learning (regression)</p></li>
<li><p><strong>Model family</strong>: Linear in the parameters; non-linear with respect to raw inputs.</p></li>
<li><p><strong>Features</strong>: Non-linear functions of the attributes</p></li>
<li><p><strong>Objective function</strong>: Mean squared error</p></li>
<li><p><strong>Optimizer</strong>: Normal equations</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "aml"
        },
        kernelOptions: {
            name: "aml",
            path: "./contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'aml'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="lecture2-supervised-learning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 2: Supervised Machine Learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture4-classification.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 4: Classification and Logistic Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Cornell University<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>