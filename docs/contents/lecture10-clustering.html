
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 10: Clustering &#8212; Applied Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 12: Support Vector Machines" href="lecture12-suppor-vector-machines.html" />
    <link rel="prev" title="Lecture 9: Density Estimation" href="lecture9-density-estimation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-supervised-learning.html">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-linear-regression.html">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6-naive-bayes.html">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture7-gaussian-discriminant-analysis.html">
   Lecture 7: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8-unsupervised-learning.html">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9-density-estimation.html">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture12-suppor-vector-machines.html">
   Lecture 12: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture13-svm-dual.html">
   Lecture 13: Dual Formulation of Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture14-kernels.html">
   Lecture 14: Kernels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture15-decision-trees.html">
   Lecture 15: Tree-Based Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture16-boosting.html">
   Lecture 16: Boosting
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/contents/lecture10-clustering.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontents/lecture10-clustering.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/contents/lecture10-clustering.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture-models-for-clustering">
   10.1. Gaussian Mixture Models for Clustering
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-unsupervised-learning">
     10.1.1. Review: Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-mixture-models-gmm">
     10.1.2. Gaussian Mixture Models (GMM)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clustering-with-known-cluster-assignments">
     10.1.3. Clustering With Known Cluster Assignments
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-labeled-to-unlabeled-clustering">
     10.1.4. From Labeled to Unlabeled Clustering
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-marginal-likelihood-learning">
       10.1.4.1. Maximum Marginal Likelihood Learning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recovering-clusters-from-gmms">
       10.1.4.2. Recovering Clusters from GMMs
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beyond-gaussian-mixtures">
     10.1.5. Beyond Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation-maximization">
   10.2. Expectation Maximization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization-intuition">
     10.2.1. Expectation Maximization: Intuition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization-definition">
     10.2.2. Expectation Maximization: Definition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-and-cons-of-em">
     10.2.3. Pros and Cons of EM
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation-maximization-in-gaussian-mixture-models">
   10.3. Expectation Maximization in Gaussian Mixture Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deriving-the-e-step">
     10.3.1. Deriving the E-Step
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deriving-the-m-step">
     10.3.2. Deriving the M-Step
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-em-in-gaussian-mixture-models">
     10.3.3. Summary: EM in Gaussian Mixture Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalization-in-probabilistic-models">
   10.4. Generalization in Probabilistic Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-data-distribution">
     10.4.1. Review: Data Distribution
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting-and-overfitting">
     10.4.2. Underfitting and Overfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#underfitting-in-unsupervised-learning">
       10.4.2.1. Underfitting in Unsupervised Learning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#overfitting-in-unsupervised-learning">
       10.4.2.2. Overfitting in Unsupervised Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-generalization-using-log-likelihood">
     10.4.3. Measuring Generalization Using Log-Likelihood
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 10: Clustering</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture-models-for-clustering">
   10.1. Gaussian Mixture Models for Clustering
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-unsupervised-learning">
     10.1.1. Review: Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-mixture-models-gmm">
     10.1.2. Gaussian Mixture Models (GMM)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clustering-with-known-cluster-assignments">
     10.1.3. Clustering With Known Cluster Assignments
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-labeled-to-unlabeled-clustering">
     10.1.4. From Labeled to Unlabeled Clustering
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-marginal-likelihood-learning">
       10.1.4.1. Maximum Marginal Likelihood Learning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recovering-clusters-from-gmms">
       10.1.4.2. Recovering Clusters from GMMs
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beyond-gaussian-mixtures">
     10.1.5. Beyond Gaussian Mixtures
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation-maximization">
   10.2. Expectation Maximization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization-intuition">
     10.2.1. Expectation Maximization: Intuition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization-definition">
     10.2.2. Expectation Maximization: Definition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-and-cons-of-em">
     10.2.3. Pros and Cons of EM
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation-maximization-in-gaussian-mixture-models">
   10.3. Expectation Maximization in Gaussian Mixture Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deriving-the-e-step">
     10.3.1. Deriving the E-Step
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deriving-the-m-step">
     10.3.2. Deriving the M-Step
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-em-in-gaussian-mixture-models">
     10.3.3. Summary: EM in Gaussian Mixture Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalization-in-probabilistic-models">
   10.4. Generalization in Probabilistic Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-data-distribution">
     10.4.1. Review: Data Distribution
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting-and-overfitting">
     10.4.2. Underfitting and Overfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#underfitting-in-unsupervised-learning">
       10.4.2.1. Underfitting in Unsupervised Learning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#overfitting-in-unsupervised-learning">
       10.4.2.2. Overfitting in Unsupervised Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-generalization-using-log-likelihood">
     10.4.3. Measuring Generalization Using Log-Likelihood
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-10-clustering">
<h1>Lecture 10: Clustering<a class="headerlink" href="#lecture-10-clustering" title="Permalink to this headline">¶</a></h1>
<p>For this lecture, we are going to dive deeper into another application area of unsupervised learning, clustering.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="gaussian-mixture-models-for-clustering">
<h1>10.1. Gaussian Mixture Models for Clustering<a class="headerlink" href="#gaussian-mixture-models-for-clustering" title="Permalink to this headline">¶</a></h1>
<p>Clustering is a common unsupervised learning problem with numerous applications.</p>
<p>We will start by defining the problem and outlining some models for this problem.</p>
<section id="review-unsupervised-learning">
<h2>10.1.1. Review: Unsupervised Learning<a class="headerlink" href="#review-unsupervised-learning" title="Permalink to this headline">¶</a></h2>
<p>We have a dataset <em>without</em> labels. Our goal is to learn something interesting about the structure of the data:</p>
<ul class="simple">
<li><p>Clusters hidden in the dataset.</p></li>
<li><p>Outliers: particularly unusual and/or interesting datapoints.</p></li>
<li><p>Useful signal hidden in noise, e.g. human speech over a noisy phone.</p></li>
</ul>
<p>Two related problems in the area of unsupervised learning we have looked at are <strong>clustering</strong> and <strong>density estimation</strong></p>
<ul>
<li><p><strong>Clustering</strong> is the problem of identifying distinct components in the data. Usually, we apply clustering when we assume that the data will have a certain structure, specifically:</p>
<ul class="simple">
<li><p>Datapoints in a cluster are more similar to each other than to points in other clusters</p></li>
<li><p>Clusters are usually defined by their centers, and potentially by other shape parameters.</p></li>
<li><p>A simple clustering algorithm we have looked at is <span class="math notranslate nohighlight">\(K\)</span>-Means</p></li>
</ul>
</li>
<li><p><strong>Density Estimation</strong> is the problem of learning <span class="math notranslate nohighlight">\(P_\theta\)</span></p>
<div class="math notranslate nohighlight">
\[
    P_\theta(x) : \mathcal{X} \to [0,1].
    \]</div>
<p>on an unsupervised dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> to approximate the true data distribution <span class="math notranslate nohighlight">\(P_\text{data}\)</span>.</p>
<ul class="simple">
<li><p>If we successfully learn <span class="math notranslate nohighlight">\(P_\theta \approx P_\text{data}\)</span>, then we can use <span class="math notranslate nohighlight">\(P_\theta\)</span> to solve many downstream tasks, including generation, outlier detection, and also <strong>clustering</strong>.</p></li>
</ul>
</li>
</ul>
<p>In this lecture, we will introduce <strong>Gaussian mixture models</strong>, and the <strong>expectation maximization</strong> algorithm to learn the models. The expectation maximization algorithm will involve both density estimation and clustering.</p>
</section>
<section id="gaussian-mixture-models-gmm">
<h2>10.1.2. Gaussian Mixture Models (GMM)<a class="headerlink" href="#gaussian-mixture-models-gmm" title="Permalink to this headline">¶</a></h2>
<p>Gaussian mixtures is a probabilistic model of the form:</p>
<div class="math notranslate nohighlight">
\[
P_\theta (x,z) = P_\theta (x | z) P_\theta (z)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z \in \mathcal{Z} = \{1,2,\ldots,K\}\)</span> is discrete and follows a categorical distribution <span class="math notranslate nohighlight">\(P_\theta(z=k) = \phi_k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> is continuous; conditioned on <span class="math notranslate nohighlight">\(z=k\)</span>, it follows a Normal distribution <span class="math notranslate nohighlight">\(P_\theta(x | z=k) = \mathcal{N}(\mu_k, \Sigma_k)\)</span>.</p></li>
</ul>
<p>The parameters <span class="math notranslate nohighlight">\(\theta\)</span> are the <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k, \phi_k\)</span> for all <span class="math notranslate nohighlight">\(k=1,2,\ldots,K\)</span>.</p>
<p>The model postulates that:</p>
<ul class="simple">
<li><p>Our observed data is comprised of  <span class="math notranslate nohighlight">\(K\)</span> clusters with proportions specified by <span class="math notranslate nohighlight">\(\phi_1,\phi_2, \ldots, \phi_K\)</span></p></li>
<li><p>The points within each cluster follow a Normal distribution <span class="math notranslate nohighlight">\(P_\theta(x | z=k) = \mathcal{N}(\mu_k, \Sigma_k)\)</span>.</p></li>
</ul>
<p>With this structure, <span class="math notranslate nohighlight">\(z^{(i)}\)</span> is the cluster label for data point <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and can be obtained from <span class="math notranslate nohighlight">\(P(z^{(i)}|x^{(i)})\)</span></p>
<p>And to get the marginal distribution <span class="math notranslate nohighlight">\(P_\theta (x)\)</span>, we can sum over the joint distribution as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
P_\theta (x) 
&amp; = \sum_{k=1}^K P_\theta (z=k) P_\theta (x|z=k)\\
&amp; = \phi_1 \mathcal{N}(x; \mu_1, \Sigma_1)  +  \ldots + \phi_K \mathcal{N}(x; \mu_K, \Sigma_K)
\end{align}
\end{split}\]</div>
</section>
<section id="clustering-with-known-cluster-assignments">
<h2>10.1.3. Clustering With Known Cluster Assignments<a class="headerlink" href="#clustering-with-known-cluster-assignments" title="Permalink to this headline">¶</a></h2>
<p>Let’s first think about how we would do clustering if the identity of each cluster is known.</p>
<p>Specifically, let our dataset be <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(1)}, z^{(1)}), (x^{(2)}, z^{(2)}), \ldots, (x^{(n)}, z^{(n)})\}\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x^{(i)} \in \mathbb{R}\)</span> are the datapoints we cluster</p></li>
<li><p><span class="math notranslate nohighlight">\(z^{(i)} \in \{1,2,...,K\}\)</span> are the cluster IDs</p></li>
<li><p>We want to fit a GMM, where <span class="math notranslate nohighlight">\(\mu_k\)</span> represent the cluster centroids.</p></li>
</ul>
<p>How could we do this?</p>
<p>This is the same problem as in Gaussian Discriminant Analysis (GDA). We can fit a GMM to this data as follows:</p>
<ol class="simple">
<li><p>We fit the parameters <span class="math notranslate nohighlight">\(\phi_k\)</span> to be % of each class <span class="math notranslate nohighlight">\(k\)</span> in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p></li>
<li><p>We fit the parameters <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span> to be means and variances of each class <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>We can infer the cluster ID <span class="math notranslate nohighlight">\(z\)</span> of new points using <span class="math notranslate nohighlight">\(P(z|x)\)</span> as in GDA.</p></li>
</ol>
<p>Let’s look at an example on our Iris flower dataset. Below, we load the dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># print part of the dataset</span>
<span class="n">iris_X</span><span class="p">,</span> <span class="n">iris_y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">iris_X</span><span class="p">,</span> <span class="n">iris_y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Recall that we have previously seen how to fit GMMs on a labeled <span class="math notranslate nohighlight">\((x,y)\)</span> dataset using maximum likelihood.</p>
<p>Let’s try this on our Iris dataset again (recall that this is called GDA). Below, we estimate/fit the parameters of GMM:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement these formulas over the Iris dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()[:,:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of features in our toy dataset</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># number of clases</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># size of the dataset</span>

<span class="c1"># these are the shapes of the parameters</span>
<span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">Sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">phis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># we now compute the parameters</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">iris_y</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span>
    <span class="n">mus</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Sigmas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">phis</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The code below generates predictions from the GMM model that we just learned by implementing Bayes’ rule:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement this in numpy</span>
<span class="k">def</span> <span class="nf">gda_predictions</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This returns class assignments and p(y|x) under the GDA model.</span>
<span class="sd">    </span>
<span class="sd">    We compute \arg\max_y p(y|x) as \arg\max_y p(x|y)p(y)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># adjust shapes</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>    
    
    <span class="c1"># compute probabilities</span>
    <span class="n">py</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">phis</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">pxy</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="n">d</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">)))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
        <span class="o">*</span> <span class="o">-</span><span class="mf">.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mus</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">)),</span> <span class="n">x</span><span class="o">-</span><span class="n">mus</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pyx</span> <span class="o">=</span> <span class="n">pxy</span> <span class="o">*</span> <span class="n">py</span>
    <span class="k">return</span> <span class="n">pyx</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">pyx</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>

<span class="n">idx</span><span class="p">,</span> <span class="n">pyx</span> <span class="o">=</span> <span class="n">gda_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can visualize the GMM as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">.02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">.02</span><span class="p">))</span>
<span class="n">Z</span><span class="p">,</span> <span class="n">pyx</span> <span class="o">=</span> <span class="n">gda_predictions</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()],</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
<span class="n">logpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">pyx</span><span class="p">)</span>

<span class="c1"># Put the result into a color plot</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span> <span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">contours</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">logpy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">contours</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Plot also the training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture10-clustering_17_0.png" src="../_images/lecture10-clustering_17_0.png" />
</div>
</div>
<p>In the figure above, the ellipses represent the Gaussians, while the dots with different colors are the datapoints belonging to different classes.</p>
<p>In the above example, the <span class="math notranslate nohighlight">\(P_\theta(z|x)\)</span> from the GMM <span class="math notranslate nohighlight">\(P_\theta\)</span> tells us the cluster ID <span class="math notranslate nohighlight">\(z\)</span> of <span class="math notranslate nohighlight">\(x\)</span>. We straightforwardly trained the GMM <span class="math notranslate nohighlight">\(P_\theta\)</span> by maximizing likelihood since we know the label of each datapoint, i.e., which cluster it belongs to.</p>
<p>But what if we don’t have labels to train <span class="math notranslate nohighlight">\(P_\theta\)</span>?</p>
</section>
<section id="from-labeled-to-unlabeled-clustering">
<h2>10.1.4. From Labeled to Unlabeled Clustering<a class="headerlink" href="#from-labeled-to-unlabeled-clustering" title="Permalink to this headline">¶</a></h2>
<p>We will now talk about how to train a GMM clustering model from unlabeled data.</p>
<p>Our strategy will be to jointly learn cluster parameters and labels:</p>
<ol class="simple">
<li><p>We will define an objective that does not require cluster labels <span class="math notranslate nohighlight">\(z^{(i)}\)</span>.</p></li>
<li><p>We will define an optimizer that jointly infers both labels <span class="math notranslate nohighlight">\(z^{(i)}\)</span> and cluster parameters <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span>.</p></li>
</ol>
<section id="maximum-marginal-likelihood-learning">
<h3>10.1.4.1. Maximum Marginal Likelihood Learning<a class="headerlink" href="#maximum-marginal-likelihood-learning" title="Permalink to this headline">¶</a></h3>
<p>Maximum marginal (log-)likelihood is a way of learning a probabilistic model on an unsupervised dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> by maximizing:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}\sum_{i=1}^n \log P_\theta({x}^{(i)}) = \frac{1}{n}\sum_{i=1}^n \log \left(\sum_{z \in \mathcal{Z}} P_\theta({x}^{(i)}, z)\right).
\]</div>
<ul class="simple">
<li><p>This asks <span class="math notranslate nohighlight">\(P_\theta\)</span> to assign a high probability to the training data in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p></li>
<li><p>However, we need to use <span class="math notranslate nohighlight">\(P(x) = \sum_{z \in \mathcal{Z}} P(x,z)\)</span> to compute this probability because <span class="math notranslate nohighlight">\(z\)</span> is not observed.</p></li>
</ul>
<p>Since our objective is</p>
<div class="math notranslate nohighlight">
\[
\max_\theta \frac{1}{n}\sum_{i=1}^n \log \left(\sum_{z \in \mathcal{Z}} P_\theta({x}^{(i)}, z)\right)
\]</div>
<ul class="simple">
<li><p>Our intuition of assigning high probability to data still holds</p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z\)</span> encodes the cluster id; we ask <span class="math notranslate nohighlight">\(P_\theta\)</span> to be high for <em>at least one <span class="math notranslate nohighlight">\(z\)</span></em></p></li>
</ul>
<ul class="simple">
<li><p>We are  still minimizing KL divergence between <span class="math notranslate nohighlight">\(P_\theta\)</span> and <span class="math notranslate nohighlight">\(P_\text{data}\)</span></p></li>
</ul>
<p>Recall that the Kullback-Leibler (KL) divergence <span class="math notranslate nohighlight">\(D(\cdot\|\cdot)\)</span> between the model distribution and the data distribution is:</p>
<div class="math notranslate nohighlight">
\[
D(P_\text{data} \| P_\theta) = \sum_{{\bf x}} P_\text{data}({\bf x}) \log \frac{P_\text{data}({\bf x})}{P_\theta({\bf x})}.
\]</div>
<p>The KL divergence is always non-negative, and equals zero when <span class="math notranslate nohighlight">\(P_\text{data}\)</span> and <span class="math notranslate nohighlight">\(P_\theta\)</span> are identical. This makes it a natural measure of similarity that’s useful for comparing distributions.</p>
<p>How do we optimize the marginal likelihood objective? Optimizing the likelihood of mixture models is harder than optimizing the likelihood of single gaussian models. For mixture models, there are no closed form solutions, and unlike a single Gaussian, a mixture has many local maxima. We will talk about an algorithm to train mixture models later in this lecture</p>
</section>
<section id="recovering-clusters-from-gmms">
<h3>10.1.4.2. Recovering Clusters from GMMs<a class="headerlink" href="#recovering-clusters-from-gmms" title="Permalink to this headline">¶</a></h3>
<p>Given a trained GMM model <span class="math notranslate nohighlight">\(P_\theta (x,z) = P_\theta (x | z) P_\theta (z)\)</span>, it’s easy to compute the <em>posterior</em> probability</p>
<div class="math notranslate nohighlight">
\[
P_\theta(z = k\mid x) = \frac{P_\theta(z=k, x)}{P_\theta(x)} = \frac{P_\theta(x | z=k) P_\theta(z=k)}{\sum_{l=1}^K P_\theta(x | z=l) P_\theta(z=l)}
\]</div>
<p>of a point <span class="math notranslate nohighlight">\(x\)</span> belonging to class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<ul class="simple">
<li><p>The posterior defines a “soft” assignment of <span class="math notranslate nohighlight">\(x\)</span> to each class.</p></li>
<li><p>This is in contrast to the hard assignments from <span class="math notranslate nohighlight">\(K\)</span>-Means.</p></li>
</ul>
</section>
</section>
<section id="beyond-gaussian-mixtures">
<h2>10.1.5. Beyond Gaussian Mixtures<a class="headerlink" href="#beyond-gaussian-mixtures" title="Permalink to this headline">¶</a></h2>
<p>We will focus on Gaussian mixture models in this lecture, but there exist many other kinds of clustering:</p>
<ul class="simple">
<li><p>Hierarchical clusters</p></li>
<li><p>Points belonging to multiple clusters (e.g. topics)</p></li>
<li><p>Clusters in graphs</p></li>
</ul>
<p>See the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> <a class="reference external" href="https://scikit-learn.org/stable/modules/clustering.html">guide</a> for more!</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="expectation-maximization">
<h1>10.2. Expectation Maximization<a class="headerlink" href="#expectation-maximization" title="Permalink to this headline">¶</a></h1>
<p>Recall the issues we had earlier with trying to perform unlabeled clustering with Gaussian mixture models:</p>
<ul class="simple">
<li><p>Unlike in supervised learning, cluster assignments are latent.</p></li>
<li><p>Hence, there is not a closed form solution for <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
<p>We will now introduce expectation maximization (EM), an algorithm that can be used to fit Gaussian mixture models.</p>
<section id="expectation-maximization-intuition">
<h2>10.2.1. Expectation Maximization: Intuition<a class="headerlink" href="#expectation-maximization-intuition" title="Permalink to this headline">¶</a></h2>
<p>Expectation maximization (EM) is an algorithm for maximizing marginal log-likelihood</p>
<div class="math notranslate nohighlight">
\[
\max_\theta \sum_{x^{(i)}\in \mathcal{D}} \log \left( \sum_{z \in \mathcal{Z}}P_\theta(x^{(i)}, z) \right)
\]</div>
<p>that can also be used to learn Gaussian mixtures.</p>
<p>The idea behind expectation maximization is:</p>
<ul class="simple">
<li><p>If we know the true <span class="math notranslate nohighlight">\(z^{(i)}\)</span> for each <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, we maximize</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\max_\theta \sum_{x^{(i)}, z^{(i)}\in \mathcal{D}} \log \left( P_\theta(x^{(i)}, z^{(i)}) \right).
\]</div>
<p>and it’s easy to find the best <span class="math notranslate nohighlight">\(\theta\)</span> (use solution for supervised learning).</p>
<ul class="simple">
<li><p>If we know <span class="math notranslate nohighlight">\(\theta\)</span>, we can estimate the cluster assignments <span class="math notranslate nohighlight">\(z^{(i)}\)</span> for each <span class="math notranslate nohighlight">\(i\)</span> by computing <span class="math notranslate nohighlight">\(P_\theta(z | x^{(i)})\)</span>.</p></li>
</ul>
<p>Expectation maximization alternates between these two steps.</p>
<ol class="simple">
<li><p>(<strong>E-Step</strong>) Given an estimate <span class="math notranslate nohighlight">\(\theta_t\)</span> of the weights, compute <span class="math notranslate nohighlight">\(P_\theta(z | x^{(i)})\)</span>.
and use it to “hallucinate” expected cluster assignments <span class="math notranslate nohighlight">\(z^{(i)}\)</span>.</p></li>
<li><p>(<strong>M-Step</strong>) Find a new <span class="math notranslate nohighlight">\(\theta_{t+1}\)</span> that maximizes the marginal log-likelihood by optimizing <span class="math notranslate nohighlight">\(P_\theta(x^{(i)}, z^{(i)})\)</span> given the <span class="math notranslate nohighlight">\(z^{(i)}\)</span> from step 1.</p></li>
</ol>
<p>This process increases the marginal likelihood at each step and eventually converges.</p>
</section>
<section id="expectation-maximization-definition">
<h2>10.2.2. Expectation Maximization: Definition<a class="headerlink" href="#expectation-maximization-definition" title="Permalink to this headline">¶</a></h2>
<p>Formally, EM learns the parameters <span class="math notranslate nohighlight">\(\theta\)</span> of a latent-variable model <span class="math notranslate nohighlight">\(P_\theta(x,z)\)</span> over a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{x^{(i)} \mid i = 1,2,...,n\}\)</span> as follows.</p>
<p>For <span class="math notranslate nohighlight">\(t=0,1,2,\ldots\)</span>, repeat until convergence:</p>
<ol class="simple">
<li><p>(<strong>E-Step</strong>) For each <span class="math notranslate nohighlight">\(x^{(i)} \in \mathcal{D}\)</span> compute <span class="math notranslate nohighlight">\(P_{\theta_t}(z|x^{(i)})\)</span></p></li>
<li><p>(<strong>M-Step</strong>) Compute new weights <span class="math notranslate nohighlight">\(\theta_{t+1}\)</span> as</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\theta_{t+1} &amp; = \arg\max_{\theta} \sum_{i=1}^n \mathbb{E}_{z^{(i)} \sim P_{\theta_t}(z|x^{(i)})} \log P_{\theta}(x^{(i)}, z^{(i)}) \\
&amp; = \arg\max_{\theta} \sum_{i=1}^n \sum_{k=1}^K P_{\theta_t}(z=k|x^{(i)}) \log P_{\theta}(x^{(i)}, z=k)
\end{align*}
\end{split}\]</div>
<p>Since assignments <span class="math notranslate nohighlight">\(P_{\theta_t}(z|x^{(i)})\)</span> are “soft”, M-step involves an expectation. For many interesting models, this expectation is tractable.</p>
</section>
<section id="pros-and-cons-of-em">
<h2>10.2.3. Pros and Cons of EM<a class="headerlink" href="#pros-and-cons-of-em" title="Permalink to this headline">¶</a></h2>
<p>EM is a very important optimization algorithm in machine learning.</p>
<ul class="simple">
<li><p>It is easy to implement and is guaranteed to converge.</p></li>
<li><p>It works in a lot of important ML models.</p></li>
</ul>
<p>Its limitations include:</p>
<ul class="simple">
<li><p>It can get stuck in local optima.</p></li>
<li><p>We may not be able to compute <span class="math notranslate nohighlight">\(P_{\theta_t}(z|x^{(i)})\)</span> in every model.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="expectation-maximization-in-gaussian-mixture-models">
<h1>10.3. Expectation Maximization in Gaussian Mixture Models<a class="headerlink" href="#expectation-maximization-in-gaussian-mixture-models" title="Permalink to this headline">¶</a></h1>
<p>Next, let’s work through how Expectation Maximization works in Gaussian Mixture Models.</p>
<section id="deriving-the-e-step">
<h2>10.3.1. Deriving the E-Step<a class="headerlink" href="#deriving-the-e-step" title="Permalink to this headline">¶</a></h2>
<p>In the E-step, we compute the posterior for each data point <span class="math notranslate nohighlight">\(x\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[
 P_\theta(z = k\mid x) = \frac{P_\theta(z=k, x)}{P_\theta(x)} = \frac{P_\theta(x | z=k) P_\theta(z=k)}{\sum_{l=1}^K P_\theta(x | z=l) P_\theta(z=l)}= \frac{\mathcal{N}(x; \mu_k, \Sigma_k) \cdot \phi_k}{\sum_{l=1}^K \mathcal{N}(x; \mu_l, \Sigma_l) \cdot \phi_l}
 \]</div>
<p><span class="math notranslate nohighlight">\(P_\theta(z = k\mid x)\)</span> defines a vector of probabilities that <span class="math notranslate nohighlight">\(x\)</span> originates from component <span class="math notranslate nohighlight">\(k\)</span> given the current set of parameters <span class="math notranslate nohighlight">\(\theta\)</span></p>
</section>
<section id="deriving-the-m-step">
<h2>10.3.2. Deriving the M-Step<a class="headerlink" href="#deriving-the-m-step" title="Permalink to this headline">¶</a></h2>
<p>At the M-step, we optimize the expected log-likelihood of our model.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;\max_\theta \sum_{x \in D} \mathbb{E}_{z \sim P_{\theta_t}(z|x)} \log P_\theta(x,z) = \\
&amp; \max_\theta \left( \sum_{k=1}^K \sum_{x \in D} P_{\theta_t}(z = k|x) \log P_\theta(x|z = k) + \sum_{k=1}^K \sum_{x \in D} P_{\theta_t}(z = k|x) \log P_\theta(z = k) \right)
\end{align*}
\end{split}\]</div>
<p>As in supervised learning, we can optimize the two terms above separately.</p>
<p>We will start with <span class="math notranslate nohighlight">\(P_\theta(x\mid z=k) = \mathcal{N}(x; \mu_k, \Sigma_k)\)</span>. We have to find <span class="math notranslate nohighlight">\(\mu_k^*, \Sigma_k^*\)</span> that optimize</p>
<div class="math notranslate nohighlight">
\[
\max_\theta \sum_{x^{(i)} \in D} P(z=k|x^{(i)}) \log P_\theta(x^{(i)}|z=k)
\]</div>
<p>Note that this corresponds to fitting a Gaussian to a dataset whose elements <span class="math notranslate nohighlight">\(x^{(i)}\)</span> each have a weight <span class="math notranslate nohighlight">\(P(z=k|x^{(i)})\)</span>.</p>
<p>Similar to how we did this in the supervised regime, we compute the derivative, set it to zero, and obtain closed form solutions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mu_k^* &amp; = \frac{\sum_{i=1}^n P(z=k|x^{(i)}) x^{(i)}}{n_k} \\
\Sigma_k^* &amp; = \frac{\sum_{i=1}^n P(z=k|x^{(i)}) (x^{(i)} - \mu_k^*)(x^{(i)} - \mu_k^*)^\top}{n_k} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(n_k = \sum_{i=1}^n P(z=k|x^{(i)})\)</span></p>
<p>Intuitively, the optimal mean and covariance are the <strong>empirical</strong> mean and convaraince of the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> when each element <span class="math notranslate nohighlight">\(x^{(i)}\)</span> has a weight <span class="math notranslate nohighlight">\(P(z=k|x^{(i)})\)</span>.</p>
<p>Similarly, we can show that the optimal class priors <span class="math notranslate nohighlight">\(\phi_k^*\)</span> are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\phi_k^* &amp; = \frac{n_k}{n} \\
\end{align*}
\end{split}\]</div>
<p>Intuitively, the optimal <span class="math notranslate nohighlight">\(\phi_k^*\)</span> is just the proportion of data points with class <span class="math notranslate nohighlight">\(k\)</span></p>
</section>
<section id="summary-em-in-gaussian-mixture-models">
<h2>10.3.3. Summary: EM in Gaussian Mixture Models<a class="headerlink" href="#summary-em-in-gaussian-mixture-models" title="Permalink to this headline">¶</a></h2>
<p>EM learns the parameters <span class="math notranslate nohighlight">\(\theta\)</span> of a Gaussian mixture model <span class="math notranslate nohighlight">\(P_\theta(x,z)\)</span> over a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{x^{(i)} \mid i = 1,2,...,n\}\)</span> as follows.</p>
<p>For <span class="math notranslate nohighlight">\(t=0,1,2,\ldots\)</span>, repeat until convergence:</p>
<ol class="simple">
<li><p>(<strong>E-Step</strong>) For each <span class="math notranslate nohighlight">\(x^{(i)} \in \mathcal{D}\)</span> compute <span class="math notranslate nohighlight">\(P_{\theta_t}(z|x^{(i)})\)</span></p></li>
<li><p>(<strong>M-Step</strong>) Compute optimal parameters <span class="math notranslate nohighlight">\(\mu_k^*, \Sigma_k^*, \phi_k^*\)</span> using the above formulas</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="generalization-in-probabilistic-models">
<h1>10.4. Generalization in Probabilistic Models<a class="headerlink" href="#generalization-in-probabilistic-models" title="Permalink to this headline">¶</a></h1>
<p>Let’s now revisit the concepts of overfitting and underfitting in GMMs.</p>
<section id="review-data-distribution">
<h2>10.4.1. Review: Data Distribution<a class="headerlink" href="#review-data-distribution" title="Permalink to this headline">¶</a></h2>
<p>We will assume that the dataset is sampled from a probability distribution <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>, which we will call the <em>data distribution</em>. We will denote this as</p>
<div class="math notranslate nohighlight">
\[
x \sim \mathbb{P}.
\]</div>
<p>The dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{x^{(i)} \mid i = 1,2,...,n\}\)</span> consists of <em>independent and identicaly distributed</em> (IID) samples from <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>.</p>
</section>
<section id="underfitting-and-overfitting">
<h2>10.4.2. Underfitting and Overfitting<a class="headerlink" href="#underfitting-and-overfitting" title="Permalink to this headline">¶</a></h2>
<p>Consider the following dataset, which is generated from a mixture of four Gaussians. Below, we display 100 samples generated by the mixture distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># generate 150 random points</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_all</span><span class="p">,</span> <span class="n">y_all</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># use the first 100 points as the main dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X_all</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">y_all</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x12b583780&gt;
</pre></div>
</div>
<img alt="../_images/lecture10-clustering_50_1.png" src="../_images/lecture10-clustering_50_1.png" />
</div>
</div>
<p>We know the true labels of these clusters, and we can visualize them with different colors as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x115b872b0&gt;
</pre></div>
</div>
<img alt="../_images/lecture10-clustering_52_1.png" src="../_images/lecture10-clustering_52_1.png" />
</div>
</div>
<p>We generate another 50 datapoints to be a holdout set. We visualize them below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use the last 50 points as a holdout set</span>
<span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_holdout</span> <span class="o">=</span> <span class="n">X_all</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="n">y_all</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_holdout</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x12addcf98&gt;
</pre></div>
</div>
<img alt="../_images/lecture10-clustering_54_1.png" src="../_images/lecture10-clustering_54_1.png" />
</div>
</div>
<section id="underfitting-in-unsupervised-learning">
<h3>10.4.2.1. Underfitting in Unsupervised Learning<a class="headerlink" href="#underfitting-in-unsupervised-learning" title="Permalink to this headline">¶</a></h3>
<p>Underfitting happens when we are not able to fully learn the signal hidden in the data.</p>
<p>In the context of GMMs, this means not capturing all the clusters in the data.</p>
<p>Let’s fit a GMM on our toy dataset. Below, we define our model to be a mixture of two Gaussians and fit it to the training set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit a GMM</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">mixture</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GaussianMixture(n_components=2)
</pre></div>
</div>
</div>
</div>
<p>The model finds two distinct components in the data, but they fail to capture the true structure.</p>
<p>We can also measure the value of our objective (the log-likelihood) on the training and holdout sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">means_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">means_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training Set Log-Likelihood (higher is better): </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Holdout Set Log-Likelihood (higher is better): </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Set Log-Likelihood (higher is better): -4.09
Holdout Set Log-Likelihood (higher is better): -4.22
</pre></div>
</div>
<img alt="../_images/lecture10-clustering_59_1.png" src="../_images/lecture10-clustering_59_1.png" />
</div>
</div>
<p>The above figure shows the learned cluster centroids as red diamonds and the training set as colored dots. You can see that the cluster centroids do not represent each class well, i.e., the model is underfitting.</p>
</section>
<section id="overfitting-in-unsupervised-learning">
<h3>10.4.2.2. Overfitting in Unsupervised Learning<a class="headerlink" href="#overfitting-in-unsupervised-learning" title="Permalink to this headline">¶</a></h3>
<p>Consider now what happens if we further increase the number of clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">means_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">means_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Train LL: </span><span class="si">%.2f</span><span class="s1"> | Holdout LL: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture10-clustering_62_0.png" src="../_images/lecture10-clustering_62_0.png" />
</div>
</div>
<p>In the above figures, we can see that the leftmost figure has the best holdout set’s objective value since we define the model to be a mixture of four Gaussians. As we increase the number of clusters, though the training set objective value gets better, the holdout set objective value gets worse; this means we are overfitting.</p>
</section>
</section>
<section id="measuring-generalization-using-log-likelihood">
<h2>10.4.3. Measuring Generalization Using Log-Likelihood<a class="headerlink" href="#measuring-generalization-using-log-likelihood" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic unsupervised models optimize an objective that can be used to detect overfitting and underfitting by comparing performance between training and holdout sets.</p>
<p>Below, we visualize the performance (measured via negative log-likelihood) on training and holdout sets as <span class="math notranslate nohighlight">\(K\)</span> increases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ks</span><span class="p">,</span> <span class="n">training_objs</span><span class="p">,</span> <span class="n">holdout_objs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">25</span><span class="p">),</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">Ks</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">training_objs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">holdout_objs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">training_objs</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">holdout_objs</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of clusters K&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Negative Log-Likelihood&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training and Holdout Negative Log-Likelihood (Lower is Better)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Training Negative Log-Likelihood&#39;</span><span class="p">,</span> <span class="s1">&#39;Holdout Negative Log-Likelihood&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x12c463320&gt;
</pre></div>
</div>
<img alt="../_images/lecture10-clustering_66_1.png" src="../_images/lecture10-clustering_66_1.png" />
</div>
</div>
<p>The above figure shows that when the number of clusters is below 4, adding more clusters helps reduce both training and holdout sets’ negative log-likelihood. Nevertheless, when the number of clusters is above 4, adding more clusters reduces the training set’s negative log-likelihood but increases the holdset’s negative log-likelihood. The former situation represents underfitting – you can make the model perform better on both training and holdout sets by making the model more expressive. The latter situation represents overfitting – making the model more expressive makes the performance on the holdout set worse.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="lecture9-density-estimation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 9: Density Estimation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture12-suppor-vector-machines.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 12: Support Vector Machines</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Cornell University<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>