
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 7: Gaussian Discriminant Analysis &#8212; Applied Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 8: Unsupervised Learning" href="lecture8-unsupervised-learning.html" />
    <link rel="prev" title="Lecture 6: Generative Models and Naive Bayes" href="lecture6-naive-bayes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-supervised-learning.html">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-linear-regression.html">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6-naive-bayes.html">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 7: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8-unsupervised-learning.html">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9-density-estimation.html">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10-clustering.html">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture12-suppor-vector-machines.html">
   Lecture 12: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture13-svm-dual.html">
   Lecture 13: Dual Formulation of Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture14-kernels.html">
   Lecture 14: Kernels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture15-decision-trees.html">
   Lecture 15: Tree-Based Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture16-boosting.html">
   Lecture 16: Boosting
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/contents/lecture7-gaussian-discriminant-analysis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontents/lecture7-gaussian-discriminant-analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/contents/lecture7-gaussian-discriminant-analysis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 7: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#revisiting-generative-models">
   7.1. Revisiting Generative Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-iris-flowers-dataset">
     7.1.1. The Iris Flowers Dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-discriminative-models">
     7.1.2. Review: Discriminative Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-generative-models">
     7.1.3. Review: Generative Models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probabilistic-interpretations">
       7.1.3.1. Probabilistic Interpretations
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture-models">
   7.2. Gaussian Mixture Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-and-motivation">
     7.2.1. Review and Motivation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-distributions">
       7.2.1.1. Review of Distributions
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#categorical-distribution">
         Categorical Distribution
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#normal-gaussian-distribution">
         Normal (Gaussian) Distribution
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-generative-model-for-iris-flowers">
       7.2.1.2. A Generative Model for Iris Flowers
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-mixture-model">
     7.2.2. Gaussian Mixture Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-mixtures-of-distributions">
       7.2.2.1. Why Mixtures of Distributions?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gmms-are-indeed-mixtures">
       7.2.2.2. GMMs Are Indeed Mixtures
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#predictions-out-of-gaussian-mixture-models">
       7.2.2.3. Predictions Out of Gaussian Mixture Models
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-discriminant-analysis">
   7.3. Gaussian Discriminant Analysis
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-maximum-likelihood-learning">
     7.3.1. Review: Maximum Likelihood Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizing-the-log-likelihood">
     7.3.2. Optimizing the Log Likelihood
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-phi">
       7.3.2.1. Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\phi\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-mu-k-sigma-k">
       7.3.2.2. Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\mu_k, \Sigma_k\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#querying-the-model">
       7.3.2.3. Querying the Model
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-gaussian-discriminant-analysis-gda">
     7.3.3. Algorithm: Gaussian Discriminant Analysis (GDA)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-iris-flower-classification">
       7.3.3.1. Example: Iris Flower Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#special-cases-of-gda">
       7.3.3.2. Special Cases of GDA
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discriminative-vs-generative-algorithms">
   7.4. Discriminative vs. Generative Algorithms
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-discriminant-analysis">
     7.4.1. Linear Discriminant Analysis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-the-lda-model-class">
       7.4.1.1.  What Is the LDA Model Class?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lda-vs-logistic-regression">
       7.4.1.2. LDA vs. Logistic Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-models-vs-discriminative-models">
     7.4.2. Generative Models vs. Discriminative Models
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 7: Gaussian Discriminant Analysis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 7: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#revisiting-generative-models">
   7.1. Revisiting Generative Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-iris-flowers-dataset">
     7.1.1. The Iris Flowers Dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-discriminative-models">
     7.1.2. Review: Discriminative Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-generative-models">
     7.1.3. Review: Generative Models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probabilistic-interpretations">
       7.1.3.1. Probabilistic Interpretations
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture-models">
   7.2. Gaussian Mixture Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-and-motivation">
     7.2.1. Review and Motivation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-distributions">
       7.2.1.1. Review of Distributions
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#categorical-distribution">
         Categorical Distribution
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#normal-gaussian-distribution">
         Normal (Gaussian) Distribution
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-generative-model-for-iris-flowers">
       7.2.1.2. A Generative Model for Iris Flowers
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-mixture-model">
     7.2.2. Gaussian Mixture Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-mixtures-of-distributions">
       7.2.2.1. Why Mixtures of Distributions?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gmms-are-indeed-mixtures">
       7.2.2.2. GMMs Are Indeed Mixtures
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#predictions-out-of-gaussian-mixture-models">
       7.2.2.3. Predictions Out of Gaussian Mixture Models
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-discriminant-analysis">
   7.3. Gaussian Discriminant Analysis
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-maximum-likelihood-learning">
     7.3.1. Review: Maximum Likelihood Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizing-the-log-likelihood">
     7.3.2. Optimizing the Log Likelihood
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-phi">
       7.3.2.1. Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\phi\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-mu-k-sigma-k">
       7.3.2.2. Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\mu_k, \Sigma_k\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#querying-the-model">
       7.3.2.3. Querying the Model
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-gaussian-discriminant-analysis-gda">
     7.3.3. Algorithm: Gaussian Discriminant Analysis (GDA)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-iris-flower-classification">
       7.3.3.1. Example: Iris Flower Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#special-cases-of-gda">
       7.3.3.2. Special Cases of GDA
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discriminative-vs-generative-algorithms">
   7.4. Discriminative vs. Generative Algorithms
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-discriminant-analysis">
     7.4.1. Linear Discriminant Analysis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-the-lda-model-class">
       7.4.1.1.  What Is the LDA Model Class?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lda-vs-logistic-regression">
       7.4.1.2. LDA vs. Logistic Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-models-vs-discriminative-models">
     7.4.2. Generative Models vs. Discriminative Models
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-7-gaussian-discriminant-analysis">
<h1>Lecture 7: Gaussian Discriminant Analysis<a class="headerlink" href="#lecture-7-gaussian-discriminant-analysis" title="Permalink to this headline">¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="revisiting-generative-models">
<h1>7.1. Revisiting Generative Models<a class="headerlink" href="#revisiting-generative-models" title="Permalink to this headline">¶</a></h1>
<p>In the last lecture, we introduced generative modeling and Naive Bayes. In this lecture, we will see more examples of generative models, namely Gaussian Discriminant Analysis.</p>
<p>Let’s first review generative models and their distinction from discriminative models through a simple classification problem.</p>
<section id="the-iris-flowers-dataset">
<h2>7.1.1. The Iris Flowers Dataset<a class="headerlink" href="#the-iris-flowers-dataset" title="Permalink to this headline">¶</a></h2>
<p>As a motivating problem for this lecture, we are going to use the Iris flower dataset (<a class="reference external" href="https://en.wikipedia.org/wiki/Ronald_Fisher">R. A. Fisher, 1936</a>). Recall that our task is to classify subspecies of Iris flowers based on their measurements.</p>
<p>First, let’s load the dataset and print the examples from the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># print part of the dataset</span>
<span class="n">iris_X</span><span class="p">,</span> <span class="n">iris_y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">iris_X</span><span class="p">,</span> <span class="n">iris_y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If we only consider the first two feature columns, we can visualize the dataset in 2D.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># create 2d version of dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()[:,:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span>

<span class="c1"># Plot also the training points</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal Length (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal Width (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">p1</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Setosa&#39;</span><span class="p">,</span> <span class="s1">&#39;Versicolour&#39;</span><span class="p">,</span> <span class="s1">&#39;Virginica&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f7b6107ed00&gt;
</pre></div>
</div>
<img alt="../_images/lecture7-gaussian-discriminant-analysis_5_1.png" src="../_images/lecture7-gaussian-discriminant-analysis_5_1.png" />
</div>
</div>
</section>
<section id="review-discriminative-models">
<h2>7.1.2. Review: Discriminative Models<a class="headerlink" href="#review-discriminative-models" title="Permalink to this headline">¶</a></h2>
<p>Most models we have seen so far have been <em>discriminative</em>:</p>
<ul class="simple">
<li><p>They directly transforms <span class="math notranslate nohighlight">\(x\)</span> into a score for each class <span class="math notranslate nohighlight">\(y\)</span> (e.g., via the formula <span class="math notranslate nohighlight">\(y=\sigma(\theta^\top x)\)</span>)</p></li>
<li><p>They can be interpreted as defining a <em>conditional</em> probability <span class="math notranslate nohighlight">\(P_\theta(y|x)\)</span></p></li>
</ul>
<p>For example, logistic regression is a binary classification algorithm which uses a model</p>
<div class="math notranslate nohighlight">
\[
f_\theta : \mathcal{X} \to [0,1]
\]</div>
<p>of the form</p>
<div class="math notranslate nohighlight">
\[ 
f_\theta(x) = \sigma(\theta^\top x) = \frac{1}{1 + \exp(-\theta^\top x)}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + \exp(-z)}\)</span> is the <em>sigmoid</em> or <em>logistic</em> function.</p>
<p>The logistic model defines (“parameterizes”) a probability distribution <span class="math notranslate nohighlight">\(P_\theta(y|x) : \mathcal{X} \times \mathcal{Y} \to [0,1]\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P_\theta(y=1 | x) &amp; = \sigma(\theta^\top x) \\
P_\theta(y=0 | x) &amp; = 1-\sigma(\theta^\top x).
\end{align*}
\end{split}\]</div>
<p>Logistic regression optimizes the following objective defined over a binary classification dataset  <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(n)}, y^{(n)})\}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\ell(\theta) &amp; = \frac{1}{n}\sum_{i=1}^n \log P_\theta (y^{(i)} \mid x^{(i)}) \\
&amp; = \frac{1}{n}\sum_{i=1}^n {y^{(i)}} \cdot \log \sigma(\theta^\top x^{(i)}) + (1-y^{(i)}) \cdot \log (1-\sigma(\theta^\top x^{(i)})).
\end{align*}
\end{split}\]</div>
<p>This objective is also often called the log-loss, or cross-entropy. This asks the model to ouput a large score <span class="math notranslate nohighlight">\(\sigma(\theta^\top x^{(i)})\)</span> (a score that’s close to one) if <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span>, and a score that’s small (close to zero) if <span class="math notranslate nohighlight">\(y^{(i)}=0\)</span>.</p>
<p>Now, let’s train logistic/softmax regression on this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">)</span>

<span class="c1"># Create an instance of Logistic Regression Classifier and fit the data.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()[:,:</span><span class="mi">2</span><span class="p">]</span>
<span class="c1"># rename class two to class one</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">iris_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LogisticRegression(C=100000.0)
</pre></div>
</div>
</div>
</div>
<p>We visualize the regions predicted to be associated with the blue, brown, and yellow classes and the lines between them are the decision boundaries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">8.2</span><span class="p">,</span> <span class="mf">.02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">.02</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

<span class="c1"># Put the result into a color plot</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>

<span class="c1"># Plot also the training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7-gaussian-discriminant-analysis_13_0.png" src="../_images/lecture7-gaussian-discriminant-analysis_13_0.png" />
</div>
</div>
</section>
<section id="review-generative-models">
<h2>7.1.3. Review: Generative Models<a class="headerlink" href="#review-generative-models" title="Permalink to this headline">¶</a></h2>
<p>Another approach to classification is to use <em>generative</em> models.</p>
<ul class="simple">
<li><p>A generative approach first builds a model of <span class="math notranslate nohighlight">\(x\)</span> for each class:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
P_\theta(x | y=k) \; \text{for each class $k$}.
\]</div>
<p><span class="math notranslate nohighlight">\(P_\theta(x | y=k)\)</span> <em>scores</em> each <span class="math notranslate nohighlight">\(x\)</span> according to how well it matches class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<ul class="simple">
<li><p>A class probability <span class="math notranslate nohighlight">\(P_\theta(y=k)\)</span> encoding our prior beliefs</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
P_\theta(y=k) \; \text{for each class $k$}.
\]</div>
<p>These are often just the % of each class in the data.</p>
<p>In the context of Iris flower classification, we would fit three models on a labeled corpus:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
P_\theta(x|y=\text{0}) &amp;&amp; P_\theta(x|y=\text{1}) &amp;&amp; P_\theta(x|y=\text{2})
\end{align*}
\]</div>
<p>We would also define priors <span class="math notranslate nohighlight">\(P_\theta(y=\text{0}), P_\theta(y=\text{1}),P_\theta(y=\text{2})\)</span>.</p>
<p><span class="math notranslate nohighlight">\(P_\theta(x | y=k)\)</span> <em>scores</em> each <span class="math notranslate nohighlight">\(x\)</span> based on how much it looks like class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<section id="probabilistic-interpretations">
<h3>7.1.3.1. Probabilistic Interpretations<a class="headerlink" href="#probabilistic-interpretations" title="Permalink to this headline">¶</a></h3>
<p>A <em>generative</em> model defines <span class="math notranslate nohighlight">\(P_\theta(x|y)\)</span> and <span class="math notranslate nohighlight">\(P_\theta(y)\)</span>, thus it also defines a distribution of the form <span class="math notranslate nohighlight">\(P_\theta(x,y)\)</span>.</p>
<!-- $$
\begin{align*}
\underbrace{P_\theta(x,y) : \mathcal{X} \times \mathcal{Y} \to [0,1]}_\text{generative model} & \;\; & \underbrace{P_\theta(y|x) : \mathcal{X} \times \mathcal{Y} \to [0,1]}_\text{discriminative model}
\end{align*}
$$ -->
<div class="math notranslate nohighlight">
\[
\underbrace{P_\theta(x, y): \mathcal{X} \times \mathcal{Y} \rightarrow[0,1]}_{\text {generative model }} \quad \underbrace{P_\theta(y \mid x): \mathcal{X} \times \mathcal{Y} \rightarrow[0,1]}_{\text {discriminative model }}
\]</div>
<p>Discriminative models don’t define any probability over the <span class="math notranslate nohighlight">\(x\)</span>’s. Generative models do.</p>
<p>We can learn a generative model <span class="math notranslate nohighlight">\(P_\theta(x, y)\)</span> by maximizing the <em>likelihood</em>:</p>
<div class="math notranslate nohighlight">
\[ 
\max_\theta \frac{1}{n}\sum_{i=1}^n \log P_\theta({x}^{(i)}, y^{(i)}). 
\]</div>
<p>This says that we should choose parameters <span class="math notranslate nohighlight">\(\theta\)</span> such that the model <span class="math notranslate nohighlight">\(P_\theta\)</span> assigns a high probability to each training example <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span> in the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="gaussian-mixture-models">
<h1>7.2. Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Permalink to this headline">¶</a></h1>
<p>Next, we will define another generative model: Gaussian mixtures.</p>
<section id="review-and-motivation">
<h2>7.2.1. Review and Motivation<a class="headerlink" href="#review-and-motivation" title="Permalink to this headline">¶</a></h2>
<section id="review-of-distributions">
<h3>7.2.1.1. Review of Distributions<a class="headerlink" href="#review-of-distributions" title="Permalink to this headline">¶</a></h3>
<section id="categorical-distribution">
<h4>Categorical Distribution<a class="headerlink" href="#categorical-distribution" title="Permalink to this headline">¶</a></h4>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical</a> distribution with parameters <span class="math notranslate nohighlight">\(\theta\)</span> is a probability
over <span class="math notranslate nohighlight">\(K\)</span> discrete outcomes <span class="math notranslate nohighlight">\(x \in \{1,2,...,K\}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P_\theta(x = j) = \theta_j.
\]</div>
<p>When <span class="math notranslate nohighlight">\(K=2\)</span> this is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a>.</p>
</section>
<section id="normal-gaussian-distribution">
<h4>Normal (Gaussian) Distribution<a class="headerlink" href="#normal-gaussian-distribution" title="Permalink to this headline">¶</a></h4>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal</a> distribution <span class="math notranslate nohighlight">\(P_\theta(x) : \mathcal{X} \to [0,1]\)</span> with parameters <span class="math notranslate nohighlight">\(\theta = (\mu, \Sigma)\)</span>
is a probability over a <span class="math notranslate nohighlight">\(d\)</span>-dimensional <span class="math notranslate nohighlight">\(x \in \mathbb{R}^d\)</span></p>
<div class="math notranslate nohighlight">
\[
P_\theta(x; \mu, \Sigma) = \frac{1}{\sqrt{(2 \pi)^d | \Sigma |}} \exp\left(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x-\mu) \right)
\]</div>
<p>In one dimension, this reduces to <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{2 \pi}\sigma} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2} \right)\)</span>.</p>
</section>
</section>
<section id="a-generative-model-for-iris-flowers">
<h3>7.2.1.2. A Generative Model for Iris Flowers<a class="headerlink" href="#a-generative-model-for-iris-flowers" title="Permalink to this headline">¶</a></h3>
<p>To define a generative model for Iris flowers, we need to define three probabilities:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
P_\theta(x|y=\text{0}) &amp;&amp; P_\theta(x|y=\text{1}) &amp;&amp; P_\theta(x|y=\text{2})
\end{align*}
\]</div>
<p>We also define priors <span class="math notranslate nohighlight">\(P_\theta(y=\text{0}), P_\theta(y=\text{1}), P_\theta(y=\text{2})\)</span>.</p>
<p>Each model <span class="math notranslate nohighlight">\(P_\theta(x | y=k)\)</span> <em>scores</em> <span class="math notranslate nohighlight">\(x\)</span> based on how much it looks like class <span class="math notranslate nohighlight">\(k\)</span>.
The inputs <span class="math notranslate nohighlight">\(x\)</span> are vectors of features for the flowers.
How do we choose <span class="math notranslate nohighlight">\(P_\theta(x|y=k)\)</span>?</p>
<p>This what the density of a Normal distribution looks like in 2D:</p>
<center><img width=70% src="img/multivariate_gaussian1.png"></center><p>This is how we can visualize it in a 2D plane:</p>
<center><img width=70% src="img/multivariate_gaussian2.png"></center></section>
</section>
<section id="gaussian-mixture-model">
<h2>7.2.2. Gaussian Mixture Model<a class="headerlink" href="#gaussian-mixture-model" title="Permalink to this headline">¶</a></h2>
<p>A <em>Gaussian mixture</em> model (GMM) <span class="math notranslate nohighlight">\(P_\theta(x,y)\)</span> is defined for <em>real-valued data</em> <span class="math notranslate nohighlight">\(x \in \mathbb{R}^d\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(\theta\)</span> contains prior parameters <span class="math notranslate nohighlight">\(\vec\phi = (\phi_1,...,\phi_K)\)</span> and <span class="math notranslate nohighlight">\(K\)</span> sets of per-class Gaussian parameters <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span>.</p>
<p>The probability of the data <span class="math notranslate nohighlight">\(x\)</span> for each class is a multivariate Gaussian</p>
<div class="math notranslate nohighlight">
\[
P_\theta(x|y=k) = \mathcal{N}(x ; \mu_k, \Sigma_k).
\]</div>
<p>The probability over <span class="math notranslate nohighlight">\(y\)</span> is Categorical:
<span class="math notranslate nohighlight">\(P_\theta(y=k) = \phi_k\)</span>.</p>
<section id="why-mixtures-of-distributions">
<h3>7.2.2.1. Why Mixtures of Distributions?<a class="headerlink" href="#why-mixtures-of-distributions" title="Permalink to this headline">¶</a></h3>
<p>A single distribution (e.g., a Gaussian) can be too simple to fit the data. We can form more complex distributions by <em>mixing</em> <span class="math notranslate nohighlight">\(K\)</span> simple ones:</p>
<div class="math notranslate nohighlight">
\[ P_\theta(x) = \phi_1 P_1(x ;\theta_1) + \phi_2 P_2(x ;\theta_2) + \ldots + \phi_1 P_K(x ;\theta_K) \]</div>
<p>where the <span class="math notranslate nohighlight">\(\phi_k \in [0,1]\)</span> are the weights of each distribution.</p>
<p>A mixture of <span class="math notranslate nohighlight">\(K\)</span> Gaussians is a distribution <span class="math notranslate nohighlight">\(P(x)\)</span> of the form:</p>
<div class="math notranslate nohighlight">
\[
\phi_1 \mathcal{N}(x; \mu_1, \Sigma_1) + \phi_2 \mathcal{N}(x; \mu_2, \Sigma_2) + \ldots + \phi_K \mathcal{N}(x; \mu_K, \Sigma_K).
\]</div>
<p>Mixtures can express distributions that a single mixture component can’t:</p>
<center><img width=65% src="img/mogdensity1d_v2.png"></center>
<p>Here, we have a mixture of 3 Gaussians.</p>
<p>We can also represent a mixture of distributions by introducing <span class="math notranslate nohighlight">\(y \in \{1,2,...,K\}\)</span> and a distribution over <span class="math notranslate nohighlight">\((x,y)\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[ 
P_\theta(x,y) = P_\theta(x|y) P_\theta(y)
\]</div>
<p>that has two components:</p>
<ul class="simple">
<li><p>The distribution <span class="math notranslate nohighlight">\(P_\theta(y=k) = \pi_k\)</span> encodes the mixture weights.</p></li>
<li><p>The distribution <span class="math notranslate nohighlight">\(P_\theta(x|y=k) = P_k(x; \theta_k)\)</span> encodes the <span class="math notranslate nohighlight">\(k\)</span>-th mixed distribution.</p></li>
</ul>
<p>This is a mixture of distributions because:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P_\theta(x) &amp; = \sum_{k=1}^K P_\theta(x|y=k)P_\theta(y=k) \\
&amp; = \pi_1 P_1(x ;\theta_1) + \pi_2 P_2(x ;\theta_2) + \ldots + \pi_1 P_K(x ;\theta_K)
\end{align*}
\end{split}\]</div>
</section>
<section id="gmms-are-indeed-mixtures">
<h3>7.2.2.2. GMMs Are Indeed Mixtures<a class="headerlink" href="#gmms-are-indeed-mixtures" title="Permalink to this headline">¶</a></h3>
<p>The Gaussian Mixture Model is an example of a mixture of <span class="math notranslate nohighlight">\(K\)</span> distributions with mixing weights <span class="math notranslate nohighlight">\(\phi_k = P(y=k)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P_\theta(x) = \sum_{k=1}^K P_\theta(y=k) P_\theta(x|y=k) = \sum_{k=1}^K \phi_k \mathcal{N}(x; \mu_k, \Sigma_k)
\]</div>
<p>Intuitively, this model defines a story for how the data was generated. To obtain a data point,</p>
<ul class="simple">
<li><p>First, we sample a class <span class="math notranslate nohighlight">\(y \sim \text{Categorical}(\phi_1, \phi_2, ..., \phi_K)\)</span> with class proportions given by the <span class="math notranslate nohighlight">\(\phi_k\)</span>.</p></li>
<li><p>Then, we sample an <span class="math notranslate nohighlight">\(x\)</span> from a Gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_k, \Sigma_k)\)</span> specific to that class.</p></li>
</ul>
<p>Such a story can be constructed for most generative algorithms and helps understand them.</p>
<p>Mixtures of Gaussians fit more complex distributions than one Gaussian.</p>
<p>The figures below illustrate how the mixture of two Gaussians can model the distribution composed of two separate clusters, which are poorly represented by a single Gaussian.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Raw data</p></th>
<th class="head"><p>Single Gaussian</p></th>
<th class="head"><p>Mixture of Gaussians</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img width=90% src="img/oldfaithful_v2.png"></p></td>
<td><p><img width=90% src="img/oldfSingle_v2.png"></p></td>
<td><p><img width=90% src="img/oldfMOG_v2.png"></p></td>
</tr>
</tbody>
</table>
</section>
<section id="predictions-out-of-gaussian-mixture-models">
<h3>7.2.2.3. Predictions Out of Gaussian Mixture Models<a class="headerlink" href="#predictions-out-of-gaussian-mixture-models" title="Permalink to this headline">¶</a></h3>
<p>Given a trained model <span class="math notranslate nohighlight">\(P_\theta (x,z) = P_\theta (x | z) P_\theta (z)\)</span>, we can look at the <em>posterior</em> probability</p>
<div class="math notranslate nohighlight">
\[
P_\theta(z = k\mid x) = \frac{P_\theta(z=k, x)}{P_\theta(x)} = \frac{P_\theta(x | z=k) P_\theta(z=k)}{\sum_{l=1}^K P_\theta(x | z=l) P_\theta(z=l)}
\]</div>
<p>of a point <span class="math notranslate nohighlight">\(x\)</span> belonging to class <span class="math notranslate nohighlight">\(k\)</span>.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="gaussian-discriminant-analysis">
<h1>7.3. Gaussian Discriminant Analysis<a class="headerlink" href="#gaussian-discriminant-analysis" title="Permalink to this headline">¶</a></h1>
<p>Next, we will use GMMs as the basis for a new generative classification algorithm, Gaussian Discriminant Analysis (GDA).</p>
<section id="review-maximum-likelihood-learning">
<h2>7.3.1. Review: Maximum Likelihood Learning<a class="headerlink" href="#review-maximum-likelihood-learning" title="Permalink to this headline">¶</a></h2>
<p>We can learn a generative model <span class="math notranslate nohighlight">\(P_\theta(x, y)\)</span> by maximizing the <em>maximum likelihood</em>:</p>
<div class="math notranslate nohighlight">
\[ 
\max_\theta \frac{1}{n}\sum_{i=1}^n \log P_\theta({x}^{(i)}, y^{(i)}). 
\]</div>
<p>This seeks to find parameters <span class="math notranslate nohighlight">\(\theta\)</span> such that the model assigns high probability to the training data.</p>
<p>Let’s use maximum likelihood to fit the Guassian Discriminant model. Note that model parameterss <span class="math notranslate nohighlight">\(\theta\)</span> are the union of the parameters of each sub-model:
$<span class="math notranslate nohighlight">\(\theta = (\mu_1, \Sigma_1, \phi_1, \ldots, \mu_K, \Sigma_K, \phi_K).\)</span>$</p>
<p>Mathematically, the components of the model <span class="math notranslate nohighlight">\(P_\theta(x,y)\)</span> are as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P_\theta(y) &amp; = \frac{\prod_{k=1}^K \phi_k^{\mathbb{I}\{y = y_k\}}}{\sum_{k=1}^k \phi_k} \\
P_\theta(x|y=k) &amp; = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp(-\frac{1}{2}(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k))
\end{align*}
\end{split}\]</div>
</section>
<section id="optimizing-the-log-likelihood">
<h2>7.3.2. Optimizing the Log Likelihood<a class="headerlink" href="#optimizing-the-log-likelihood" title="Permalink to this headline">¶</a></h2>
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(i)}, y^{(i)})\mid i=1,2,\ldots,n\}\)</span>, we want to optimize the log-likelihood <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\ell(\theta) &amp; = \sum_{i=1}^n \log P_\theta(x^{(i)}, y^{(i)}) = \sum_{i=1}^n \log P_\theta(x^{(i)} | y^{(i)}) + \sum_{i=1}^n \log P_\theta(y^{(i)}) \\
&amp; = \sum_{k=1}^K  \underbrace{\sum_{i : y^{(i)} = k} \log P(x^{(i)} | y^{(i)} ; \mu_k, \Sigma_k)}_\text{all the terms that involve $\mu_k, \Sigma_k$} + \underbrace{\sum_{i=1}^n \log P(y^{(i)} ; \vec \phi)}_\text{all the terms that involve $\vec \phi$}.
\end{align*}
\end{split}\]</div>
<p>In equality #2, we use the fact that <span class="math notranslate nohighlight">\(P_\theta(x,y)=P_\theta(y) P_\theta(x|y)\)</span>; in the third one, we change the order of summation.</p>
<p>Each <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span> for <span class="math notranslate nohighlight">\(k=1,2,\ldots,K\)</span> is found in only the following terms:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\max_{\mu_k, \Sigma_k} \sum_{i=1}^n \log P_\theta(x^{(i)}, y^{(i)})
&amp; = \max_{\mu_k, \Sigma_k} \sum_{l=1}^K  \sum_{i : y^{(i)} = l} \log P_\theta(x^{(i)} | y^{(i)} ; \mu_l, \Sigma_l) \\
&amp; = \max_{\mu_k, \Sigma_k} \sum_{i : y^{(i)} = k} \log P_\theta(x^{(i)} | y^{(i)} ; \mu_k, \Sigma_k).
\end{align*}
\end{split}\]</div>
<p>Thus, optimization over <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span> can be carried out independently of all the other parameters by just looking at these terms.</p>
<p>Similarly, optimizing for <span class="math notranslate nohighlight">\(\vec \phi = (\phi_1, \phi_2, \ldots, \phi_K)\)</span> only involves a few terms:</p>
<div class="math notranslate nohighlight">
\[ 
\max_{\vec \phi} \sum_{i=1}^n \log P_\theta(x^{(i)}, y^{(i)} ; \theta) = \max_{\vec\phi} \
\sum_{i=1}^n  \log P_\theta(y^{(i)} ; \vec \phi). 
\]</div>
<section id="learning-the-parameters-phi">
<h3>7.3.2.1. Learning the Parameters <span class="math notranslate nohighlight">\(\phi\)</span><a class="headerlink" href="#learning-the-parameters-phi" title="Permalink to this headline">¶</a></h3>
<p>Let’s first consider the optimization over <span class="math notranslate nohighlight">\(\vec \phi = (\phi_1, \phi_2, \ldots, \phi_K)\)</span>.</p>
<div class="math notranslate nohighlight">
\[ 
\max_{\vec \phi} \sum_{i=1}^n  \log P_\theta(y=y^{(i)} ; \vec \phi). 
\]</div>
<ul class="simple">
<li><p>We have <span class="math notranslate nohighlight">\(n\)</span> datapoints. Each point has a label <span class="math notranslate nohighlight">\(k\in\{1,2,...,K\}\)</span>.</p></li>
<li><p>Our model is a categorical and assigns a probability <span class="math notranslate nohighlight">\(\phi_k\)</span> to each outcome <span class="math notranslate nohighlight">\(k\in\{1,2,...,K\}\)</span>.</p></li>
<li><p>We want to infer <span class="math notranslate nohighlight">\(\phi_k\)</span> assuming our dataset is sampled from the model.</p></li>
</ul>
<p>What are the maximum likelihood <span class="math notranslate nohighlight">\(\phi_k\)</span> that are most likely to have generated our data?
Intuitively, the maximum likelihood class probabilities <span class="math notranslate nohighlight">\(\phi\)</span> should just be the class proportions that we see in the data.</p>
<p>Let’s calculate this formally. Our objective <span class="math notranslate nohighlight">\(J(\vec \phi)\)</span> equals</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
J(\vec\phi) &amp; = \sum_{i=1}^n  \log P_\theta(y^{(i)} ; \vec \phi) \\
&amp; = \sum_{i=1}^n \log \phi_{y^{(i)}} - n \cdot \log \sum_{k=1}^K \phi_k \\ 
&amp; = \sum_{k=1}^K \sum_{i : y^{(i)} = k} \log \phi_k - n \cdot \log \sum_{k=1}^K \phi_k
\end{align*}
\end{split}\]</div>
<p>Taking the partial derivative with respect to <span class="math notranslate nohighlight">\(\phi_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial}{\partial_{\phi_k}} J(\vec\phi) 
&amp; = \frac{\partial}{\partial_{\phi_k}} \left (\sum_{l=1}^K \sum_{i : y^{(i)} = l} \log \phi_l - n \cdot \log \sum_{l=1}^K \phi_l \right) \\
&amp; = \frac{\partial}{\partial_{\phi_k}} \sum_{i : y^{(i)} = k} \log \phi_k - \frac{\partial}{\partial_{\phi_k}} n \cdot \log \sum_{l=1}^K \phi_l \\
&amp; = n_k \cdot \frac{\partial}{\partial_{\phi_k}} \log \phi_k - n \cdot \frac{\partial}{\partial_{\phi_k}}  \log \sum_{l=1}^K \phi_l \\
&amp; =  \frac{n_k}{\phi_k} - \frac{n}{\sum_{l=1}^K \phi_l}
\end{align*}
\end{split}\]</div>
<p>Setting this derivative to zero, we obtain
$<span class="math notranslate nohighlight">\( \frac{\phi_k}{\sum_l \phi_l} = \frac{n_k}{n}\)</span><span class="math notranslate nohighlight">\(
for each \)</span>k<span class="math notranslate nohighlight">\(, where \)</span>n_k = |{i : y^{(i)} = k}|<span class="math notranslate nohighlight">\( is the number of training targets with class \)</span>k<span class="math notranslate nohighlight">\(. Thus, the optimal \)</span>\phi_k<span class="math notranslate nohighlight">\( is just the proportion of data points with class \)</span>k$ in the training set!</p>
</section>
<section id="learning-the-parameters-mu-k-sigma-k">
<h3>7.3.2.2. Learning the Parameters <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span><a class="headerlink" href="#learning-the-parameters-mu-k-sigma-k" title="Permalink to this headline">¶</a></h3>
<p>Next, let’s look at the maximum likelihood term</p>
<div class="math notranslate nohighlight">
\[
\max_{\mu_k, \Sigma_k} \sum_{i : y^{(i)} = k} \log \mathcal{N}(x^{(i)} | \mu_k, \Sigma_k)
\]</div>
<p>over the Gaussian parameters <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span>.</p>
<ul class="simple">
<li><p>Our dataset are all the points <span class="math notranslate nohighlight">\(x\)</span> for which <span class="math notranslate nohighlight">\(y=k\)</span>.</p></li>
<li><p>We want to learn the mean and variance <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span> of a normal distribution that generates this data.</p></li>
</ul>
<p>What is the maximum likelihood <span class="math notranslate nohighlight">\(\mu_k, \Sigma_k\)</span> in this case?</p>
<p>Computing the derivative and setting it to zero, we obtain closed form solutions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mu_k &amp; = \frac{\sum_{i: y^{(i)} = k} x^{(i)}}{n_k} \\
\Sigma_k &amp; = \frac{\sum_{i: y^{(i)} = k} (x^{(i)} - \mu_k)(x^{(i)} - \mu_k)^\top}{n_k} \\
\end{align*}
\end{split}\]</div>
<p>These are just the empirical means and covariances of each class.</p>
</section>
<section id="querying-the-model">
<h3>7.3.2.3. Querying the Model<a class="headerlink" href="#querying-the-model" title="Permalink to this headline">¶</a></h3>
<p>How do we ask the model for predictions? As discussed earler, we can apply Bayes’ rule:
$<span class="math notranslate nohighlight">\(\arg\max_y P_\theta(y|x) = \arg\max_y P_\theta(x|y)P(y).\)</span><span class="math notranslate nohighlight">\(
Thus, we can estimate the probability of \)</span>x<span class="math notranslate nohighlight">\( and under each \)</span>P_\theta(x|y=k)P(y=k)$ and choose the class that explains the data best.</p>
</section>
</section>
<section id="algorithm-gaussian-discriminant-analysis-gda">
<h2>7.3.3. Algorithm: Gaussian Discriminant Analysis (GDA)<a class="headerlink" href="#algorithm-gaussian-discriminant-analysis-gda" title="Permalink to this headline">¶</a></h2>
<p>The above procedure describes an example of generative models—Gaussian Discriminant Analysis (GDA). We can succinctly define GDA in terms of the algorithm components.</p>
<ul class="simple">
<li><p><strong>Type</strong>: Supervised learning (multi-class classification)</p></li>
<li><p><strong>Model family</strong>: Mixtures of Gaussians.</p></li>
<li><p><strong>Objective function</strong>: Log-likelihood.</p></li>
<li><p><strong>Optimizer</strong>: Closed form solution.</p></li>
</ul>
<section id="example-iris-flower-classification">
<h3>7.3.3.1. Example: Iris Flower Classification<a class="headerlink" href="#example-iris-flower-classification" title="Permalink to this headline">¶</a></h3>
<p>Let’s see how this approach can be used in practice on the Iris dataset.</p>
<ul class="simple">
<li><p>We will learn the maximum likelihood GDA parameters</p></li>
<li><p>We will compare the outputs to the true predictions.</p></li>
</ul>
<p>Let’s first start by computing the true parameters on our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement these formulas over the Iris dataset</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of features in our toy dataset</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># number of clases</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># size of the dataset</span>

<span class="c1"># these are the shapes of the parameters</span>
<span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">Sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">phis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># we now compute the parameters</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">iris_y</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span>
    <span class="n">mus</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Sigmas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">phis</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># print out the means</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[5.006 3.428]
 [5.936 2.77 ]
 [6.588 2.974]]
</pre></div>
</div>
</div>
</div>
<p>We can compute predictions using Bayes’ rule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement this in numpy</span>
<span class="k">def</span> <span class="nf">gda_predictions</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This returns class assignments and p(y|x) under the GDA model.</span>
<span class="sd">    </span>
<span class="sd">    We compute \arg\max_y p(y|x) as \arg\max_y p(x|y)p(y)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># adjust shapes</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>    
    
    <span class="c1"># compute probabilities</span>
    <span class="n">py</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">phis</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">pxy</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="n">d</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">)))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
        <span class="o">*</span> <span class="o">-</span><span class="mf">.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mus</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">)),</span> <span class="n">x</span><span class="o">-</span><span class="n">mus</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pyx</span> <span class="o">=</span> <span class="n">pxy</span> <span class="o">*</span> <span class="n">py</span>
    <span class="k">return</span> <span class="n">pyx</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">pyx</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>

<span class="n">idx</span><span class="p">,</span> <span class="n">pyx</span> <span class="o">=</span> <span class="n">gda_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1
 2 2 2 2 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2
 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 2 2
 2 1]
</pre></div>
</div>
</div>
</div>
<p>We visualize the decision boundaries like we did earlier. The figure illustrates that three Gaussians learned through Gaussian Discriminant Analysis split the data points from three classes reasonably.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">.02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">.02</span><span class="p">))</span>
<span class="n">Z</span><span class="p">,</span> <span class="n">pyx</span> <span class="o">=</span> <span class="n">gda_predictions</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()],</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
<span class="n">logpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">pyx</span><span class="p">)</span>

<span class="c1"># Put the result into a color plot</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span> <span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">contours</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">logpy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">contours</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Plot also the training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7-gaussian-discriminant-analysis_57_0.png" src="../_images/lecture7-gaussian-discriminant-analysis_57_0.png" />
</div>
</div>
</section>
<section id="special-cases-of-gda">
<h3>7.3.3.2. Special Cases of GDA<a class="headerlink" href="#special-cases-of-gda" title="Permalink to this headline">¶</a></h3>
<p>Many important generative algorithms are special cases of Gaussian Discriminative Analysis</p>
<ul class="simple">
<li><p>Linear discriminant analysis (LDA): all the covariance matrices <span class="math notranslate nohighlight">\(\Sigma_k\)</span> take the same value.</p></li>
<li><p>Gaussian Naive Bayes: all the covariance matrices <span class="math notranslate nohighlight">\(\Sigma_k\)</span> are diagonal.</p></li>
<li><p>Quadratic discriminant analysis (QDA): another term for GDA.</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="discriminative-vs-generative-algorithms">
<h1>7.4. Discriminative vs. Generative Algorithms<a class="headerlink" href="#discriminative-vs-generative-algorithms" title="Permalink to this headline">¶</a></h1>
<p>We conclude our lectures on generative algorithms by revisting the question of how they compare to discriminative algorithms.</p>
<section id="linear-discriminant-analysis">
<h2>7.4.1. Linear Discriminant Analysis<a class="headerlink" href="#linear-discriminant-analysis" title="Permalink to this headline">¶</a></h2>
<p>When the covariances <span class="math notranslate nohighlight">\(\Sigma_k\)</span> in GDA are equal, we have an algorithm called Linear Discriminant Analysis or LDA.</p>
<p>The probability of the data <span class="math notranslate nohighlight">\(x\)</span> for each class is a multivariate Gaussian with the same covariance <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p>
<div class="math notranslate nohighlight">
\[
P_\theta(x|y=k) = \mathcal{N}(x ; \mu_k, \Sigma).
\]</div>
<p>The probability over <span class="math notranslate nohighlight">\(y\)</span> is Categorical:
<span class="math notranslate nohighlight">\(P_\theta(y=k) = \phi_k\)</span>.</p>
<p>Let’s try this algorithm on the Iris flower dataset.</p>
<p>We compute the model parameters similarly to how we did for GDA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement these formulas over the Iris dataset</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of features in our toy dataset</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># number of clases</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># size of the dataset</span>

<span class="c1"># these are the shapes of the parameters</span>
<span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">Sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">phis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># we now compute the parameters</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">iris_y</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span>
    <span class="n">mus</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Sigmas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># this is now X.T instead of X_k.T</span>
    <span class="n">phis</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># print out the means</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[5.006 3.428]
 [5.936 2.77 ]
 [6.588 2.974]]
</pre></div>
</div>
</div>
</div>
<p>We can compute predictions using Bayes’ rule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement this in numpy</span>
<span class="k">def</span> <span class="nf">gda_predictions</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This returns class assignments and p(y|x) under the GDA model.</span>
<span class="sd">    </span>
<span class="sd">    We compute \arg\max_y p(y|x) as \arg\max_y p(x|y)p(y)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># adjust shapes</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>    
    
    <span class="c1"># compute probabilities</span>
    <span class="n">py</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">phis</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">pxy</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="n">d</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">)))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
        <span class="o">*</span> <span class="o">-</span><span class="mf">.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mus</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigmas</span><span class="p">)),</span> <span class="n">x</span><span class="o">-</span><span class="n">mus</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pyx</span> <span class="o">=</span> <span class="n">pxy</span> <span class="o">*</span> <span class="n">py</span>
    <span class="k">return</span> <span class="n">pyx</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">pyx</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>

<span class="n">idx</span><span class="p">,</span> <span class="n">pyx</span> <span class="o">=</span> <span class="n">gda_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1
 2 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 1 2 2
 1 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2
 2 1]
</pre></div>
</div>
</div>
</div>
<p>We visualize predictions like we did earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">.02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">.02</span><span class="p">))</span>
<span class="n">Z</span><span class="p">,</span> <span class="n">pyx</span> <span class="o">=</span> <span class="n">gda_predictions</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()],</span> <span class="n">mus</span><span class="p">,</span> <span class="n">Sigmas</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
<span class="n">logpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">pyx</span><span class="p">)</span>

<span class="c1"># Put the result into a color plot</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span> <span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">contours</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">logpy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>

<span class="c1"># Plot also the training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture7-gaussian-discriminant-analysis_66_0.png" src="../_images/lecture7-gaussian-discriminant-analysis_66_0.png" />
</div>
</div>
<p>Linear Discriminant Analysis outputs decision boundaries that are linear, just like Logistic/Softmax Regression.</p>
<p>Softmax or Logistic regression also produce linear boundaries. In fact, both types of algorithms make use of the same model class.</p>
<p>What is their difference then?</p>
<section id="what-is-the-lda-model-class">
<h3>7.4.1.1.  What Is the LDA Model Class?<a class="headerlink" href="#what-is-the-lda-model-class" title="Permalink to this headline">¶</a></h3>
<p>We can derive a formula for <span class="math notranslate nohighlight">\(P_\theta(y|x)\)</span> in a Bernoulli Naive Bayes or LDA model when <span class="math notranslate nohighlight">\(K=2\)</span>:</p>
<div class="math notranslate nohighlight">
\[ 
P_\theta(y|x) = \frac{P_\theta(x|y)P_\theta(y)}{\sum_{y'\in \mathcal{Y}}P_\theta(x|y')P_\theta(y')} = \frac{1}{1+\exp(-\gamma^\top x)} 
\]</div>
<p>for some set of parameters <span class="math notranslate nohighlight">\(\gamma\)</span> (whose expression can be derived from <span class="math notranslate nohighlight">\(\theta\)</span>).</p>
<p>This is the same form as Logistic Regression! Does it mean that the two sets of algorithms are equivalent?</p>
<p>No! They assume the same model class <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, they use a different objective <span class="math notranslate nohighlight">\(J\)</span> to select a model in <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>.</p>
</section>
<section id="lda-vs-logistic-regression">
<h3>7.4.1.2. LDA vs. Logistic Regression<a class="headerlink" href="#lda-vs-logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>We describe the procedure of LDA above, and here we summarize the differences between the generative model LDA and the discriminative models NB and logistic regression.</p>
<ul class="simple">
<li><p>Bernoulli Naive Bayes or LDA assumes a logistic form for <span class="math notranslate nohighlight">\(P(y|x)\)</span>. But the converse is not true: logistic regression does not assume a NB or LDA model for <span class="math notranslate nohighlight">\(P(x,y)\)</span>.</p></li>
<li><p>Generative models make stronger modeling assumptions. If these assumptions hold true, the generative models will perform better.</p></li>
<li><p>But if they don’t, logistic regression will be more robust to outliers and model misspecification, and achieve higher accuracy.</p></li>
</ul>
</section>
</section>
<section id="generative-models-vs-discriminative-models">
<h2>7.4.2. Generative Models vs. Discriminative Models<a class="headerlink" href="#generative-models-vs-discriminative-models" title="Permalink to this headline">¶</a></h2>
<p>Finally, we are revisiting the question of when we want to use generative models or discriminative models.</p>
<p>Discriminative algorithms are deservingly very popular.</p>
<ul class="simple">
<li><p>Most state-of-the-art algorithms for classification are discriminative (including neural nets, boosting, SVMs, etc.)</p></li>
<li><p>They are often more accurate because they make fewer modeling assumptions.</p></li>
</ul>
<p>But generative models can do things that discriminative models can’t do.</p>
<ul class="simple">
<li><p><strong>Generation</strong>: we can sample <span class="math notranslate nohighlight">\(x \sim p(x|y)\)</span> to generate new data (images, audio).</p></li>
<li><p><strong>Missing value imputation</strong>: if <span class="math notranslate nohighlight">\(x_j\)</span> is missing, we infer it using <span class="math notranslate nohighlight">\(p(x|y)\)</span>.</p></li>
<li><p><strong>Outlier detection</strong>: we may detect via <span class="math notranslate nohighlight">\(p(x')\)</span> if <span class="math notranslate nohighlight">\(x'\)</span> is an outlier.</p></li>
<li><p><strong>Scalability</strong>: Simple formulas for maximum likelihood parameters.</p></li>
</ul>
<p>And generative algorithms also have many other advantages:</p>
<ul class="simple">
<li><p>Can do more than just prediction: generation, fill-in missing features, etc.</p></li>
<li><p>Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate.</p></li>
<li><p>Often have closed-form solutions, hence are faster to train.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="lecture6-naive-bayes.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 6: Generative Models and Naive Bayes</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture8-unsupervised-learning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 8: Unsupervised Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Cornell University<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>