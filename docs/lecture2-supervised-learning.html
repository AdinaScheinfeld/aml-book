
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 2: Supervised Machine Learning &#8212; Applied Machine Learning</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 3: Linear Regression" href="lecture3-linear-regression.html" />
    <link rel="prev" title="Welcome to your Jupyter Book" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-linear-regression.html">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6-naive-bayes.html">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8-unsupervised-learning.html">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9-density-estimation.html">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10-clustering.html">
   Lecture 10: Clustering
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/lecture2-supervised-learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture2-supervised-learning.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/lecture2-supervised-learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 2: Supervised Machine Learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-supervised-learning">
     What is Supervised Learning?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#an-example">
       An Example
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elements-of-a-supervised-machine-learning-problem">
   2.1. Elements of A Supervised Machine Learning Problem
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-components-of-supervised-machine-learning">
     Three Components of Supervised Machine Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-dataset">
     2.1.1. A Supervised Learning Dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-algorithm">
     2.1.2. A Supervised Learning Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-model-class">
       2.1.2.1. The Model Class
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-optimizer">
       2.1.2.2. The Optimizer
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-model">
     2.1.3. A Supervised Learning Model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-new-predictions">
     2.1.4. Making New Predictions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-supervised-learning">
     2.1.3 Why Supervised Learning?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#applications-of-supervised-learning">
       2.1.3.1. Applications of Supervised Learning
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anatomy-of-a-supervised-learning-problem-the-dataset">
   2.2. Anatomy of a Supervised Learning Problem: The Dataset
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recall-three-components-of-a-supervised-machine-learning-problem">
     Recall: Three Components of a Supervised Machine Learning Problem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-a-supervised-learning-dataset">
     2.2.1. What is a Supervised Learning Dataset?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-dataset-notation">
     2.2.2. A Supervised Learning Dataset: Notation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-dataset-inputs">
     2.2.3. Training Dataset: Inputs
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#attributes">
       2.2.3.1. Attributes
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#features">
       2.2.3.1. Features
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#features-vs-attributes">
         Features vs Attributes
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#features-discrete-vs-continuous">
       2.2.3.3. Features: Discrete vs. Continuous
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#targets">
     2.3.4. Targets
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#targets-regression-vs-classification">
       2.3.4.1. Targets: Regression vs. Classification
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-feature-matrix">
     2.2.5. The Feature Matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anatomy-of-a-supervised-learning-problem-the-learning-algorithm">
   2.3. Anatomy of a Supervised Learning Problem: The Learning Algorithm
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Recall: Three Components of a Supervised Machine Learning Problem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-components-of-a-supervised-machine-learning-algorithm">
     2.3.1. Three Components of a Supervised Machine Learning Algorithm
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     2.3.2. The Model Class
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-model">
       Defining a Model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-model-class">
       Defining a Model Class
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       An Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-objective">
     2.3.3. The Objective
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#notation">
       Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examples">
       Examples
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     2.3.4. The Optimizer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       An Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-components-of-a-supervised-machine-learning-problem">
     2.3.5. Summary: Components of a Supervised Machine Learning Problem
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 2: Supervised Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 2: Supervised Machine Learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-supervised-learning">
     What is Supervised Learning?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#an-example">
       An Example
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elements-of-a-supervised-machine-learning-problem">
   2.1. Elements of A Supervised Machine Learning Problem
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-components-of-supervised-machine-learning">
     Three Components of Supervised Machine Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-dataset">
     2.1.1. A Supervised Learning Dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-algorithm">
     2.1.2. A Supervised Learning Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-model-class">
       2.1.2.1. The Model Class
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-optimizer">
       2.1.2.2. The Optimizer
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-model">
     2.1.3. A Supervised Learning Model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-new-predictions">
     2.1.4. Making New Predictions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-supervised-learning">
     2.1.3 Why Supervised Learning?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#applications-of-supervised-learning">
       2.1.3.1. Applications of Supervised Learning
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anatomy-of-a-supervised-learning-problem-the-dataset">
   2.2. Anatomy of a Supervised Learning Problem: The Dataset
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recall-three-components-of-a-supervised-machine-learning-problem">
     Recall: Three Components of a Supervised Machine Learning Problem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-a-supervised-learning-dataset">
     2.2.1. What is a Supervised Learning Dataset?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-supervised-learning-dataset-notation">
     2.2.2. A Supervised Learning Dataset: Notation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-dataset-inputs">
     2.2.3. Training Dataset: Inputs
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#attributes">
       2.2.3.1. Attributes
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#features">
       2.2.3.1. Features
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#features-vs-attributes">
         Features vs Attributes
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#features-discrete-vs-continuous">
       2.2.3.3. Features: Discrete vs. Continuous
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#targets">
     2.3.4. Targets
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#targets-regression-vs-classification">
       2.3.4.1. Targets: Regression vs. Classification
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-feature-matrix">
     2.2.5. The Feature Matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anatomy-of-a-supervised-learning-problem-the-learning-algorithm">
   2.3. Anatomy of a Supervised Learning Problem: The Learning Algorithm
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Recall: Three Components of a Supervised Machine Learning Problem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-components-of-a-supervised-machine-learning-algorithm">
     2.3.1. Three Components of a Supervised Machine Learning Algorithm
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     2.3.2. The Model Class
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-model">
       Defining a Model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-model-class">
       Defining a Model Class
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       An Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-objective">
     2.3.3. The Objective
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#notation">
       Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examples">
       Examples
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     2.3.4. The Optimizer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Notation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       An Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-components-of-a-supervised-machine-learning-problem">
     2.3.5. Summary: Components of a Supervised Machine Learning Problem
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><left><img width=10% src="img/cornell_tech2.svg"></left></p>
<div class="tex2jax_ignore mathjax_ignore section" id="lecture-2-supervised-machine-learning">
<h1>Lecture 2: Supervised Machine Learning<a class="headerlink" href="#lecture-2-supervised-machine-learning" title="Permalink to this headline">¶</a></h1>
<p>We saw in the previous lecture that there exist three types of machine learning—supervised, unsupervised, and reinforcement learning. This lecture will focus on supervised learning.</p>
<!-- Supervised learning is by far the most common type of machine learning. This lecture will focus on supervised learning, define precisely what it is, and provide a few examples. -->
<p>We will explain what supervised learning is, and introduce mathematical notation that will be useful throughout the course for defining algorithms.</p>
<div class="section" id="what-is-supervised-learning">
<h2>What is Supervised Learning?<a class="headerlink" href="#what-is-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>At a very high level, supervised learning works as follows:</p>
<ol class="simple">
<li><p>First, we collect a dataset of labeled training examples (input-output pairs).</p></li>
<li><p>We train a model to output accurate predictions on this dataset.</p></li>
<li><p>When the model sees new, similar data, it will also be accurate.</p></li>
</ol>
<div class="section" id="an-example">
<h3>An Example<a class="headerlink" href="#an-example" title="Permalink to this headline">¶</a></h3>
<p>Consider an autonomous vehicle that needs to recognize objects on the road: pedestrians, cars, etc.</p>
<center><img width=70% src="img/tesla_data.png"/></center>
<p>One way of building an object detection system would be to specify by hand the properties defining each object: e.g., a car has four wheels, has windows, etc. However this approach does not scale: we can’t handle all the edge cases well.</p>
<p>Instead, we can apply supervised learning: we collect a large dataset of cars and provide them as exmaples to a supervised learning algorithm. The algorithm then automatically learns what features define a car!</p>
<p>This approach vastly outperforms human programmers trying to write rules by hand, and is currently the only viable way to program object detection systems.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="elements-of-a-supervised-machine-learning-problem">
<h1>2.1. Elements of A Supervised Machine Learning Problem<a class="headerlink" href="#elements-of-a-supervised-machine-learning-problem" title="Permalink to this headline">¶</a></h1>
<p>Next, let’s define supervised learning more precisely. To make things concerete, we will use a simple running example—predicting the diabetes risk of a patient from their BMI.</p>
<div class="section" id="three-components-of-supervised-machine-learning">
<h2>Three Components of Supervised Machine Learning<a class="headerlink" href="#three-components-of-supervised-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>It is useful to think of supervised learning as involving three key elements: a dataset, a learning algorithm, and a predictive model.</p>
<p>To apply supervised learning, we define a dataset and a learning algorithm.</p>
<div class="math notranslate nohighlight">
\[ \text{Dataset} + \text{Learning Algorithm} \to \text{Predictive Model} \]</div>
<p>The output is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.</p>
<p>We will now dissect and give examples of each of these three elements.</p>
</div>
<div class="section" id="a-supervised-learning-dataset">
<h2>2.1.1. A Supervised Learning Dataset<a class="headerlink" href="#a-supervised-learning-dataset" title="Permalink to this headline">¶</a></h2>
<p>Let’s start with a simple example of a supervised learning problem: predicting diabetes risk.</p>
<p>We start with a dataset of diabetes patients.</p>
<ul class="simple">
<li><p>For each patient we have a access to their BMI and an estimate of diabetes risk (from 0-400).</p></li>
<li><p>We are interested in understanding how BMI affects an individual’s diabetes risk.</p></li>
</ul>
<p>We are going to load a real diabetes dataset from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, a popular machine learning libraries that we will use throughout the course.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># We will use the UCI Diabetes Dataset</span>
<span class="c1"># It&#39;s a toy dataset often used to demo ML algorithms.</span>
<span class="n">diabetes_X</span><span class="p">,</span> <span class="n">diabetes_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Use only the BMI feature</span>
<span class="n">diabetes_X</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]]</span>

<span class="c1"># The BMI is zero-centered and normalized; we recenter it for ease of presentation</span>
<span class="n">diabetes_X</span> <span class="o">=</span> <span class="n">diabetes_X</span> <span class="o">*</span> <span class="mi">30</span> <span class="o">+</span> <span class="mi">25</span>

<span class="c1"># Collect 20 data points</span>
<span class="n">diabetes_X_train</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
<span class="n">diabetes_y_train</span> <span class="o">=</span> <span class="n">diabetes_y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>

<span class="c1"># Display some of the data points</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bmi</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>422</th>
      <td>27.335902</td>
      <td>233.0</td>
    </tr>
    <tr>
      <th>423</th>
      <td>23.811456</td>
      <td>91.0</td>
    </tr>
    <tr>
      <th>424</th>
      <td>25.331171</td>
      <td>111.0</td>
    </tr>
    <tr>
      <th>425</th>
      <td>23.779122</td>
      <td>152.0</td>
    </tr>
    <tr>
      <th>426</th>
      <td>23.973128</td>
      <td>120.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can also visualize this two-dimensional dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Diabetes Risk&#39;)
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_10_11.png" src="_images/lecture2-supervised-learning_10_11.png" />
</div>
</div>
<p>We see from the above simple visualization that the the diabetes risk increases with the patient’s BMI.</p>
</div>
<div class="section" id="a-supervised-learning-algorithm">
<h2>2.1.2. A Supervised Learning Algorithm<a class="headerlink" href="#a-supervised-learning-algorithm" title="Permalink to this headline">¶</a></h2>
<p>Next, suppose we wanted to predict the risk of a new patient given their BMI. We will use supervised learning for this. We already have a dataset of (BMI, risk) pairs—now we will give an exmaple of a supervised learning algorithm that learns the relationship between BMI and risk from this data.</p>
<p>Again, it will be useful to think about a supervised learning algorithm as having two components: a model class and an optimizer.</p>
<div class="section" id="the-model-class">
<h3>2.1.2.1. The Model Class<a class="headerlink" href="#the-model-class" title="Permalink to this headline">¶</a></h3>
<p>Intuitively, the model class represents the set of possible relationships between BMI and risk that we believe to be true. In practice, we can’t search the space of all possible mappings between BMI and risk (we will see later several reasons for why that’s the case). Therefore, we have to choose a set of possible mappings, and then choose the one we think is the best in this class.</p>
<p>Let’s assume for this example that risk is a linear function of BMI. In other words, for some unknown <span class="math notranslate nohighlight">\(\theta_0, \theta_1 \in \mathbb{R}\)</span>, we have
$<span class="math notranslate nohighlight">\( y = \theta_1 \cdot x + \theta_0, \)</span><span class="math notranslate nohighlight">\(
where \)</span>x<span class="math notranslate nohighlight">\( is the BMI (also called the independent variable), and \)</span>y$ is the diabetes risk score (the dependent variable).</p>
<p>The parameters <span class="math notranslate nohighlight">\(\theta_1, \theta_0\)</span> are the slope and the intercept of the line relates <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>We can visualize this for a few values of <span class="math notranslate nohighlight">\(\theta_1, \theta_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span> <span class="ow">in</span> <span class="n">theta_list</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">theta1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">theta0</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/lecture2-supervised-learning_14_01.png" src="_images/lecture2-supervised-learning_14_01.png" />
</div>
</div>
<p>Our supervised learning algorithm will attempt to choose the linear relationship fits well the training data that we have.</p>
</div>
<div class="section" id="the-optimizer">
<h3>2.1.2.2. The Optimizer<a class="headerlink" href="#the-optimizer" title="Permalink to this headline">¶</a></h3>
<p>Given our assumption that <span class="math notranslate nohighlight">\(x,y\)</span> follow the a linear relationship, the goal of a supervised learning algorithm is to find a good set of parameters consistent with the data.</p>
<p>This is an optimization problem—we want to maximize the fit between the model and the data over the space of all possible models. The component of a supervised learning algorithm that performs this search procedure is called the optimizer.</p>
<p>We will soon dive deeper into optimization algorithms for machine learning, but for now, let’s call the <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code> library to find a <span class="math notranslate nohighlight">\(\theta_1, \theta_0\)</span> that fit the data well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Create linear regression object</span>
<span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train the model using the training sets</span>
<span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Make predictions on the training set</span>
<span class="n">diabetes_y_train_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">)</span>

<span class="c1"># The coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Slope (theta1): </span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Intercept (theta0): </span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Slope (theta1): 	 37.378842160517664
Intercept (theta0): 	 -797.0817390342369
</pre></div>
</div>
</div>
</div>
<p>Here, we used <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to find the best slope and intercept to the above dataset.</p>
</div>
</div>
<div class="section" id="a-supervised-learning-model">
<h2>2.1.3. A Supervised Learning Model<a class="headerlink" href="#a-supervised-learning-model" title="Permalink to this headline">¶</a></h2>
<p>The supervised learning algorithm gave us a pair of parameters <span class="math notranslate nohighlight">\(\theta_1^*, \theta_0^*\)</span>. These define the <em>predictive model</em> <span class="math notranslate nohighlight">\(f^*\)</span>, defined as
$<span class="math notranslate nohighlight">\( f(x) = \theta_1^* \cdot x + \theta_0^*, \)</span><span class="math notranslate nohighlight">\(
where again \)</span>x<span class="math notranslate nohighlight">\( is the BMI, and \)</span>y$ is the diabetes risk score.</p>
<p>We can visualize the linear model that best fits our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1253f9240&gt;]
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_22_11.png" src="_images/lecture2-supervised-learning_22_11.png" />
</div>
</div>
<p>Our visualization seems reasonable: we see that the linear model that we found is close to the observed data and captures the trend we noticed earlier—higher BMI are associated with higher diabetes risk.</p>
</div>
<div class="section" id="making-new-predictions">
<h2>2.1.4. Making New Predictions<a class="headerlink" href="#making-new-predictions" title="Permalink to this headline">¶</a></h2>
<p>Recall that one of the goals of supervised learning is to predict the diabetes of new patients.</p>
<p>Given a new dataset of patients with a known BMI, we can use the model that we have just found to estimate their  risk.</p>
<p>Formally, given an <span class="math notranslate nohighlight">\(x_\text{new}\)</span>, we can output prediction <span class="math notranslate nohighlight">\(y_\text{new}\)</span> as
$<span class="math notranslate nohighlight">\( y_\text{new} = f(x_\text{new}) = \theta_1^* \cdot x_\text{new} + \theta_0. \)</span>$</p>
<p>Let’s illustrate this equation with a specific example from our diabetes dataset.</p>
<p>First, we start by loading more data. We will load three new patients (shown in red below) that we haven’t seen before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect 3 data points</span>
<span class="n">diabetes_X_test</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">diabetes_y_test</span> <span class="o">=</span> <span class="n">diabetes_y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Initial patients&#39;</span><span class="p">,</span> <span class="s1">&#39;New patients&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1259cd390&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_26_1.png" src="_images/lecture2-supervised-learning_26_1.png" />
</div>
</div>
<p>Our linear model provides an estimate of the diabetes risk for these patients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate predictions on the new patients</span>
<span class="n">diabetes_y_test_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">)</span>

<span class="c1"># visualize the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test_pred</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Model&#39;</span><span class="p">,</span> <span class="s1">&#39;Prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;Initial patients&#39;</span><span class="p">,</span> <span class="s1">&#39;New patients&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x125bfb048&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_28_1.png" src="_images/lecture2-supervised-learning_28_1.png" />
</div>
</div>
<p>For each patient, we look up their BMI <span class="math notranslate nohighlight">\(x\)</span> and compute the value <span class="math notranslate nohighlight">\(f(x)\)</span> of the linear model <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. On the above figure, <span class="math notranslate nohighlight">\(f(x)\)</span> is denoted by a red cross.</p>
<p>We can compare the predicted value of <span class="math notranslate nohighlight">\(f(x)\)</span> to the known true risk <span class="math notranslate nohighlight">\(y\)</span> (which the model didn’t see, and which is denoted by a red circle). The model is especially accurate on the leftmost patient: the prediction <span class="math notranslate nohighlight">\(f(x)\)</span> and the true <span class="math notranslate nohighlight">\(y\)</span> almost overlap. The model is somewhat off on the other two points—however, it still correctly identifies them as being at a higher risk.</p>
</div>
<div class="section" id="why-supervised-learning">
<h2>2.1.3 Why Supervised Learning?<a class="headerlink" href="#why-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>We have just seen a simple example of a supervised learning algorithm. Again, we want to emphasize that supervised learning is a powerful tool that can solve many important problems, such as:</p>
<ul class="simple">
<li><p>Making predictions on new data. Here, we just gave a very simple example of this use case. However, you can imagine using much more data to characterize a patient: their age, gender, historical blood pressure, medical notes, etc. Supervised learning is extremely powerful, and can often outperform humans at prediction if given enough data.</p></li>
<li><p>Understanding the mechanisms through which input variables affect targets. Instead of using the predictions from the model, we may investigate the model itself. In the above example, we inspected the slope of the model, and noted that it was positive. Thus, we have inferred from data that a high BMI tends to increase diabetes risk.</p></li>
</ul>
<div class="section" id="applications-of-supervised-learning">
<h3>2.1.3.1. Applications of Supervised Learning<a class="headerlink" href="#applications-of-supervised-learning" title="Permalink to this headline">¶</a></h3>
<p>More generally, supervised learning finds applications in many areas. In fact, many of the most important applications of machine learning are supervised:</p>
<ul class="simple">
<li><p>Classifying medical images. Similarly to how we predicted risk of BMI we may, for example, predict the severity of a cancer tumor from its image.</p></li>
<li><p>Translating between pairs of languages. Most machine translation systems these days are created by training a supervised learning model on large datasets consisting of pairs of sentences in different languages.</p></li>
<li><p>Detecting objects in a self-driving car. Again, we can explain to an algorithm what defines a car by providing many examples of cars. This enables the algorithm to detect new cars.</p></li>
</ul>
<p>We will see many more examples in this course.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="anatomy-of-a-supervised-learning-problem-the-dataset">
<h1>2.2. Anatomy of a Supervised Learning Problem: The Dataset<a class="headerlink" href="#anatomy-of-a-supervised-learning-problem-the-dataset" title="Permalink to this headline">¶</a></h1>
<p>We have seen an example of supervised machine learning.</p>
<p>Let’s now examine more closely the components of a supervised learning problem, starting with the dataset.</p>
<div class="section" id="recall-three-components-of-a-supervised-machine-learning-problem">
<h2>Recall: Three Components of a Supervised Machine Learning Problem<a class="headerlink" href="#recall-three-components-of-a-supervised-machine-learning-problem" title="Permalink to this headline">¶</a></h2>
<p>To apply supervised learning, we define a dataset and a learning algorithm.</p>
<div class="math notranslate nohighlight">
\[ \text{Dataset} + \text{Learning Algorithm} \to \text{Predictive Model} \]</div>
<p>The output is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.</p>
</div>
<div class="section" id="what-is-a-supervised-learning-dataset">
<h2>2.2.1. What is a Supervised Learning Dataset?<a class="headerlink" href="#what-is-a-supervised-learning-dataset" title="Permalink to this headline">¶</a></h2>
<p>Let’s dive deeper into what’s a supervised learning dataset.</p>
<p>We will again use the UCI Diabetes Dataset as our example.</p>
<p>The UCI dataset contains many additional data columns besides <code class="docutils literal notranslate"><span class="pre">bmi</span></code>, including age, sex, and blood pressure. We can ask <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> to give us more information about this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># Load the diabetes dataset</span>
<span class="n">diabetes</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _diabetes_dataset:

Diabetes dataset
----------------

Ten baseline variables, age, sex, body mass index, average blood
pressure, and six blood serum measurements were obtained for each of n =
442 diabetes patients, as well as the response of interest, a
quantitative measure of disease progression one year after baseline.

**Data Set Characteristics:**

  :Number of Instances: 442

  :Number of Attributes: First 10 columns are numeric predictive values

  :Target: Column 11 is a quantitative measure of disease progression one year after baseline

  :Attribute Information:
      - age     age in years
      - sex
      - bmi     body mass index
      - bp      average blood pressure
      - s1      tc, T-Cells (a type of white blood cells)
      - s2      ldl, low-density lipoproteins
      - s3      hdl, high-density lipoproteins
      - s4      tch, thyroid stimulating hormone
      - s5      ltg, lamotrigine
      - s6      glu, blood sugar level

Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).

Source URL:
https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html

For more information see:
Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &quot;Least Angle Regression,&quot; Annals of Statistics (with discussion), 407-499.
(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="a-supervised-learning-dataset-notation">
<h2>2.2.2. A Supervised Learning Dataset: Notation<a class="headerlink" href="#a-supervised-learning-dataset-notation" title="Permalink to this headline">¶</a></h2>
<p>We say that a training dataset of size <span class="math notranslate nohighlight">\(n\)</span> (e.g., <span class="math notranslate nohighlight">\(n\)</span> patients) is a set
$<span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(i)}, y^{(i)}) \mid i = 1,2,...,n\}\)</span>$</p>
<p>Each <span class="math notranslate nohighlight">\(x^{(i)}\)</span> denotes an input (e.g., the measurements for patient <span class="math notranslate nohighlight">\(i\)</span>), and each <span class="math notranslate nohighlight">\(y^{(i)} \in \mathcal{Y}\)</span> is a target (e.g., the diabetes risk). You may think of <span class="math notranslate nohighlight">\(x^{(i)}\)</span> as a column vector containing numbers useful for predicting <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p>
<p>Together, <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span> form a <em>training example</em>.</p>
<p>We can look at the diabetes dataset in this form.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the diabetes dataset</span>
<span class="n">diabetes_X</span><span class="p">,</span> <span class="n">diabetes_y</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Print part of the dataset</span>
<span class="n">diabetes_X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019908</td>
      <td>-0.017646</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068330</td>
      <td>-0.092204</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005671</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002864</td>
      <td>-0.025930</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022692</td>
      <td>-0.009362</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031991</td>
      <td>-0.046641</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In this example, each row <span class="math notranslate nohighlight">\(i\)</span> is a vector defining defining <span class="math notranslate nohighlight">\(x^{(i)}\)</span>. If <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is a column vector, each row <span class="math notranslate nohighlight">\(i\)</span> is <span class="math notranslate nohighlight">\((x^{(i)})^\top\)</span>.</p>
<p>Note that some numbers are unusual (e.g., age is negative). This is because <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> normalized this data to have mean zero and standard deviation one.</p>
</div>
<div class="section" id="training-dataset-inputs">
<h2>2.2.3. Training Dataset: Inputs<a class="headerlink" href="#training-dataset-inputs" title="Permalink to this headline">¶</a></h2>
<p>More precisely, an input <span class="math notranslate nohighlight">\(x^{(i)} \in \mathcal{X}\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector of the form
$<span class="math notranslate nohighlight">\( x^{(i)} = \begin{bmatrix}
x^{(i)}_1 \\
x^{(i)}_2 \\
\vdots \\
x^{(i)}_d
\end{bmatrix}\)</span><span class="math notranslate nohighlight">\(
For example, it could be the values of the \)</span>d<span class="math notranslate nohighlight">\( features for patient \)</span>i$. One of these could be the BMI.</p>
<p>The set <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is called the feature space. Often, we have, <span class="math notranslate nohighlight">\(\mathcal{X} = \mathbb{R}^d\)</span>.</p>
<p>Let’s look at data for one patient. This indeed looks like a column vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>age    0.038076
sex    0.050680
bmi    0.061696
bp     0.021872
s1    -0.044223
s2    -0.034821
s3    -0.043401
s4    -0.002592
s5     0.019908
s6    -0.017646
Name: 0, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="section" id="attributes">
<h3>2.2.3.1. Attributes<a class="headerlink" href="#attributes" title="Permalink to this headline">¶</a></h3>
<p>We refer to the numerical variables describing the patient as <em>attributes</em>. Examples of attributes include:</p>
<ul class="simple">
<li><p>The age of a patient.</p></li>
<li><p>The patient’s gender.</p></li>
<li><p>The patient’s BMI.</p></li>
</ul>
<p>Note that thes attributes in the above example have been mean-centered at zero and re-scaled to have a variance of one.</p>
</div>
<div class="section" id="features">
<h3>2.2.3.1. Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h3>
<p>Often, an input object has many attributes, and we want to use these attributes to define more complex descriptions of the input.</p>
<ul class="simple">
<li><p>Is the patient old and a man? (Useful if old men are at risk).</p></li>
<li><p>Is the BMI above the obesity threshold?</p></li>
</ul>
<p>We call these custom attributes <em>features</em>.</p>
<p>Let’s create an “old man” feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_X</span><span class="p">[</span><span class="s1">&#39;old_man&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">diabetes_X</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">diabetes_X</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">diabetes_X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>old_man</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019908</td>
      <td>-0.017646</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068330</td>
      <td>-0.092204</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005671</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002864</td>
      <td>-0.025930</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022692</td>
      <td>-0.009362</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031991</td>
      <td>-0.046641</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Formally, we may denote features via a function <span class="math notranslate nohighlight">\(\phi : \mathcal{X} \to \mathbb{R}^p\)</span> that takes an input <span class="math notranslate nohighlight">\(x^{(i)} \in \mathcal{X}\)</span> and outputs a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector
$<span class="math notranslate nohighlight">\( \phi(x^{(i)}) = \left[\begin{array}{&#64;{}c&#64;{}}
\phi(x^{(i)})_1 \\
\phi(x^{(i)})_2 \\
\vdots \\
\phi(x^{(i)})_p
\end{array} \right]\)</span><span class="math notranslate nohighlight">\(
We say that \)</span>\phi(x^{(i)})<span class="math notranslate nohighlight">\( is a *featurized* input, and each \)</span>\phi(x^{(i)})_j$ is a <em>feature</em>.</p>
<div class="section" id="features-vs-attributes">
<h4>Features vs Attributes<a class="headerlink" href="#features-vs-attributes" title="Permalink to this headline">¶</a></h4>
<p>In practice, the terms attribute and features are often used interchangeably. Most authors refer to <span class="math notranslate nohighlight">\(x^{(i)}\)</span> as a vector of features.</p>
<p>We will follow this convention and use the term “attribute” only when there is ambiguity between features and attributes.</p>
</div>
</div>
<div class="section" id="features-discrete-vs-continuous">
<h3>2.2.3.3. Features: Discrete vs. Continuous<a class="headerlink" href="#features-discrete-vs-continuous" title="Permalink to this headline">¶</a></h3>
<p>Features can be either discrete or continuous. We will see that some ML algorthims handle these differently.</p>
<p>The BMI feature that we have seen earlier is an example of a continuous feature.</p>
<p>We can visualize its distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;bmi&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_52_11.png" src="_images/lecture2-supervised-learning_52_11.png" />
</div>
</div>
<p>Other features take on one of a finite number of discrete values. The <code class="docutils literal notranslate"><span class="pre">sex</span></code> column is an example of a categorical feature.</p>
<p>In this example, the dataset has been pre-processed such that the two values happen to be <code class="docutils literal notranslate"><span class="pre">0.05068012</span></code> and <code class="docutils literal notranslate"><span class="pre">-0.04464164</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">diabetes_X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">diabetes_X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.05068012 -0.04464164]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_54_21.png" src="_images/lecture2-supervised-learning_54_21.png" />
</div>
</div>
</div>
</div>
<div class="section" id="targets">
<h2>2.3.4. Targets<a class="headerlink" href="#targets" title="Permalink to this headline">¶</a></h2>
<p>For each patient, we may be interested in predicting a quantity of interest, the <em>target</em>. In our example, this is the patient’s diabetes risk.</p>
<p>Formally, when <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span> form a <em>training example</em>, each <span class="math notranslate nohighlight">\(y^{(i)} \in \mathcal{Y}\)</span> is a target. We call <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> the target space.</p>
<p>We plot the distirbution of risk scores below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Diabetes risk score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of patients&#39;</span><span class="p">)</span>
<span class="n">diabetes_y</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;Diabetes risk score&#39;, ylabel=&#39;Number of patients&#39;&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_57_11.png" src="_images/lecture2-supervised-learning_57_11.png" />
</div>
</div>
<div class="section" id="targets-regression-vs-classification">
<h3>2.3.4.1. Targets: Regression vs. Classification<a class="headerlink" href="#targets-regression-vs-classification" title="Permalink to this headline">¶</a></h3>
<p>We distinguish between two broad types of supervised learning problems that differ in the form of the target variable.</p>
<ol class="simple">
<li><p><strong>Regression</strong>: The target variable <span class="math notranslate nohighlight">\(y\)</span> is continuous. We are fitting a curve in a high-dimensional feature space that approximates the shape of the dataset.</p></li>
<li><p><strong>Classification</strong>: The target variable <span class="math notranslate nohighlight">\(y\)</span> is discrete. Each discrete value corresponds to a <em>class</em> and we are looking for a hyperplane that separates the different classes.</p></li>
</ol>
<p>We can easily turn our earlier regression example into classification by discretizing the diabetes risk scores into high or low.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Discretize the targets</span>
<span class="n">diabetes_y_train_discr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">diabetes_y_train</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="mi">150</span><span class="p">])</span>

<span class="c1"># Visualize it</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">diabetes_y_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">diabetes_y_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Low-Risk Patients&#39;</span><span class="p">,</span> <span class="s1">&#39;High-Risk Patients&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x125ffc240&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_60_11.png" src="_images/lecture2-supervised-learning_60_11.png" />
</div>
</div>
<p>Here, red points have a high score and green points have a low score. Let’s try to generate predictions for this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create logistic regression object (note: this is actually a classification algorithm!)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Train the model using the training sets</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train_discr</span><span class="p">)</span>

<span class="c1"># Make predictions on the training set</span>
<span class="n">diabetes_y_train_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span> <span class="p">)</span>

<span class="c1"># Visualize it</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">diabetes_y_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">diabetes_y_train</span><span class="p">[</span><span class="n">diabetes_y_train_discr</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">[</span><span class="n">diabetes_y_train_pred</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">diabetes_y_train</span><span class="p">[</span><span class="n">diabetes_y_train_pred</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">[</span><span class="n">diabetes_y_train_pred</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">diabetes_y_train</span><span class="p">[</span><span class="n">diabetes_y_train_pred</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Low-Risk Patients&#39;</span><span class="p">,</span> <span class="s1">&#39;High-Risk Patients&#39;</span><span class="p">,</span> <span class="s1">&#39;Low-Risk Predictions&#39;</span><span class="p">,</span> <span class="s1">&#39;High-Risk Predictions&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x11847d320&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_62_11.png" src="_images/lecture2-supervised-learning_62_11.png" />
</div>
</div>
<p>The dots inside each circle indicate the predicitons we have made. We correctly predict on each patient.</p>
</div>
</div>
<div class="section" id="the-feature-matrix">
<h2>2.2.5. The Feature Matrix<a class="headerlink" href="#the-feature-matrix" title="Permalink to this headline">¶</a></h2>
<p>Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span>, of the form:
$<span class="math notranslate nohighlight">\( X = \begin{bmatrix}
x^{(1)}_1 &amp; x^{(2)}_1 &amp; \ldots &amp; x^{(n)}_1 \\
x^{(1)}_2 &amp; x^{(2)}_2 &amp; \ldots &amp; x^{(n)}_2 \\
\vdots \\
x^{(1)}_d &amp; x^{(2)}_d &amp; \ldots &amp; x^{(n)}_d
\end{bmatrix}.\)</span>$</p>
<p>Similarly, we can vectorize the target variables into a vector <span class="math notranslate nohighlight">\(y \in \mathbb{R}^n\)</span> of the form
$<span class="math notranslate nohighlight">\( y = \begin{bmatrix}
x^{(1)} \\
x^{(2)} \\
\vdots \\
x^{(n)}
\end{bmatrix}.\)</span>$</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="anatomy-of-a-supervised-learning-problem-the-learning-algorithm">
<h1>2.3. Anatomy of a Supervised Learning Problem: The Learning Algorithm<a class="headerlink" href="#anatomy-of-a-supervised-learning-problem-the-learning-algorithm" title="Permalink to this headline">¶</a></h1>
<p>Let’s now turn our attention to the second key element of supervised learning—the learning algorithm.</p>
<div class="section" id="id1">
<h2>Recall: Three Components of a Supervised Machine Learning Problem<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>To apply supervised learning, we define a dataset and a learning algorithm.</p>
<div class="math notranslate nohighlight">
\[ \text{Dataset} + \text{Learning Algorithm} \to \text{Predictive Model} \]</div>
<p>The output is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.</p>
</div>
<div class="section" id="three-components-of-a-supervised-machine-learning-algorithm">
<h2>2.3.1. Three Components of a Supervised Machine Learning Algorithm<a class="headerlink" href="#three-components-of-a-supervised-machine-learning-algorithm" title="Permalink to this headline">¶</a></h2>
<p>We can also define the high-level structure of a supervised learning algorithm as consisting of three components:</p>
<ul class="simple">
<li><p>A <strong>model class</strong>: the set of possible models we consider.</p></li>
<li><p>An <strong>objective</strong> function, which defines how good a model is.</p></li>
<li><p>An <strong>optimizer</strong>, which finds the best predictive model in the model class according to the objective function</p></li>
</ul>
<p>Let’s look again at our diabetes dataset for an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Load the diabetes dataset</span>
<span class="n">diabetes</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">diabetes_X</span><span class="p">,</span> <span class="n">diabetes_y</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Print part of the dataset</span>
<span class="n">diabetes_X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019908</td>
      <td>-0.017646</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068330</td>
      <td>-0.092204</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005671</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002864</td>
      <td>-0.025930</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022692</td>
      <td>-0.009362</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031991</td>
      <td>-0.046641</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="id2">
<h2>2.3.2. The Model Class<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="defining-a-model">
<h3>Defining a Model<a class="headerlink" href="#defining-a-model" title="Permalink to this headline">¶</a></h3>
<p>We’ll say that a model is a function
$<span class="math notranslate nohighlight">\( f : \mathcal{X} \to \mathcal{Y} \)</span><span class="math notranslate nohighlight">\(
that maps inputs \)</span>x \in \mathcal{X}<span class="math notranslate nohighlight">\( to targets \)</span>y \in \mathcal{Y}$.</p>
<p>Often, models have <em>parameters</em> <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> living in a set <span class="math notranslate nohighlight">\(\Theta\)</span>. We will then write the model as
$<span class="math notranslate nohighlight">\( f_\theta : \mathcal{X} \to \mathcal{Y} \)</span><span class="math notranslate nohighlight">\(
to denote that it's parametrized by \)</span>\theta$.</p>
</div>
<div class="section" id="defining-a-model-class">
<h3>Defining a Model Class<a class="headerlink" href="#defining-a-model-class" title="Permalink to this headline">¶</a></h3>
<p>Formally, the model class is a set
$<span class="math notranslate nohighlight">\(\mathcal{M} \subseteq \{f \mid f : \mathcal{X} \to \mathcal{Y} \}\)</span>$
of possible models that map input features to targets.</p>
<p>When the models <span class="math notranslate nohighlight">\(f_\theta\)</span> are paremetrized by <em>parameters</em> <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> living in some set <span class="math notranslate nohighlight">\(\Theta\)</span>. Thus we can also write
$<span class="math notranslate nohighlight">\(\mathcal{M} = \{f_\theta \mid \theta \in \Theta \}.\)</span>$</p>
</div>
<div class="section" id="id3">
<h3>An Example<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>One simple approach is to assume that <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are related by a linear model of the form
\begin{align*}
y &amp; = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + … + \theta_d \cdot x_d
\end{align*}
where <span class="math notranslate nohighlight">\(x\)</span> is a featurized input and <span class="math notranslate nohighlight">\(y\)</span> is the target.</p>
<p>The <span class="math notranslate nohighlight">\(\theta_j\)</span> are the <em>parameters</em> of the model, <span class="math notranslate nohighlight">\(\Theta = \mathbb{R}^{d+1}\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{M} = \{ \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + ... + \theta_d \cdot x_d \mid \theta \in \mathbb{R}^{d+1} \}\)</span></p>
<!-- By using the notation $x_0 = 1$, we can represent the model in a vectorized form
$$ y = \sum_{j=0}^d \beta_j \cdot x_j = \vec \beta \cdot \vec x. $$
where $\vec x$ is a vector of features. --></div>
</div>
<div class="section" id="the-objective">
<h2>2.3.3. The Objective<a class="headerlink" href="#the-objective" title="Permalink to this headline">¶</a></h2>
<p>Given a training set, how do we pick the parameters <span class="math notranslate nohighlight">\(\theta\)</span> for the model? A natural approach is to select <span class="math notranslate nohighlight">\(\theta\)</span> such that <span class="math notranslate nohighlight">\(f_\theta(x^{(i)})\)</span> is close to <span class="math notranslate nohighlight">\(y^{(i)}\)</span> on a training dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(i)}, y^{(i)}) \mid i = 1,2,...,n\}\)</span></p>
<div class="section" id="notation">
<h3>Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h3>
<p>To capture this intuition, we define an <em>objective function</em> (also called a <em>loss function</em>)
$<span class="math notranslate nohighlight">\(J(f) : \mathcal{M} \to [0, \infty), \)</span><span class="math notranslate nohighlight">\(
which describes the extent to which \)</span>f<span class="math notranslate nohighlight">\( &quot;fits&quot; the data \)</span>\mathcal{D} = {(x^{(i)}, y^{(i)}) \mid i = 1,2,…,n}$.</p>
<p>When <span class="math notranslate nohighlight">\(f\)</span> is parametrized by <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>, the objective becomes a function <span class="math notranslate nohighlight">\(J(\theta) : \Theta \to [0, \infty).\)</span></p>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>What would are some possible objective functions? We will see many, but here are a few examples:</p>
<ul class="simple">
<li><p>Mean squared error: $<span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{2n} \sum_{i=1}^n \left( f_\theta(x^{(i)}) - y^{(i)} \right)^2\)</span>$</p></li>
<li><p>Absolute (L1) error: $<span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{n} \sum_{i=1}^n \left| f_\theta(x^{(i)}) - y^{(i)} \right|\)</span>$</p></li>
</ul>
<p>These are defined for a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(i)}, y^{(i)}) \mid i = 1,2,...,n\}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean squared error: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean absolute error: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean squared error: 1.50
Mean absolute error: 1.00
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h2>2.3.4. The Optimizer<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>Given our assumption that <span class="math notranslate nohighlight">\(x,y\)</span> follow the a linear relationship, the goal of a supervised learning algorithm is to find a good set of parameters consistent with the data.</p>
<div class="section" id="id5">
<h3>Notation<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>At a high-level an optimizer takes an objective <span class="math notranslate nohighlight">\(J\)</span> and a model class <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> and finds a model <span class="math notranslate nohighlight">\(f \in \mathcal{M}\)</span> with the smallest value of the objective <span class="math notranslate nohighlight">\(J\)</span>.</p>
<p>\begin{align*}
\min_{f \in \mathcal{M}} J(f)
\end{align*}</p>
<p>Intuitively, this is the function that bests “fits” the data on the training dataset.</p>
<p>When <span class="math notranslate nohighlight">\(f\)</span> is parametrized by <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>, the optimizer minimizes a function <span class="math notranslate nohighlight">\(J(\theta)\)</span> over all <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>.</p>
</div>
<div class="section" id="id6">
<h3>An Example<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>We will see that behind the scenes, the <code class="docutils literal notranslate"><span class="pre">sklearn.linear_models.LinearRegression</span></code> algorithm optimizes the MSE loss.</p>
<p>\begin{align*}
\min_{\theta \in \mathbb{R}} \frac{1}{2n} \sum_{i=1}^n \left( f_\theta(x^{(i)}) - y^{(i)} \right)^2
\end{align*}</p>
<p>We can easily measure the quality of the fit on the training set and the test set.</p>
<p>Let’s run the above algorithm on our diabetes dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect 20 data points for training</span>
<span class="n">diabetes_X_train</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
<span class="n">diabetes_y_train</span> <span class="o">=</span> <span class="n">diabetes_y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>

<span class="c1"># Create linear regression object</span>
<span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train the model using the training sets</span>
<span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Make predictions on the training set</span>
<span class="n">diabetes_y_train_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">)</span>

<span class="c1"># Collect 3 data points for testing</span>
<span class="n">diabetes_X_test</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">diabetes_y_test</span> <span class="o">=</span> <span class="n">diabetes_y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>

<span class="c1"># generate predictions on the new patients</span>
<span class="n">diabetes_y_test_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The algorithm returns a predictive model. We can visualize its predictions below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Body Mass Index (BMI)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">diabetes_y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="c1"># plt.scatter(diabetes_X_train.loc[:, [&#39;bmi&#39;]], diabetes_y_train_pred, color=&#39;black&#39;, linewidth=1)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]],</span> <span class="n">diabetes_y_test_pred</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Model&#39;</span><span class="p">,</span> <span class="s1">&#39;Prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;Initial patients&#39;</span><span class="p">,</span> <span class="s1">&#39;New patients&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x12f6a46a0&gt;
</pre></div>
</div>
<img alt="_images/lecture2-supervised-learning_85_11.png" src="_images/lecture2-supervised-learning_85_11.png" />
</div>
</div>
<p>Again, the red dots are the true values of <span class="math notranslate nohighlight">\(y\)</span> and the red crosses are the predicitons. Note that although the x-axis is still BMI, we used many additional features to make our predictions. As a result, the predicions are more accurate (the crosses are closer to the dots).</p>
<p>We can also confirm our intuition that the fit is good by evaluating the value of our objective.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training set mean squared error: </span><span class="si">%.2f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">diabetes_y_train</span><span class="p">,</span> <span class="n">diabetes_y_train_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set mean squared error: </span><span class="si">%.2f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">diabetes_y_test</span><span class="p">,</span> <span class="n">diabetes_y_test_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set mean squared error on random inputs: </span><span class="si">%.2f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">diabetes_y_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">diabetes_y_test_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set mean squared error: 1118.22
Test set mean squared error: 667.81
Test set mean squared error on random inputs: 15887.97
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="summary-components-of-a-supervised-machine-learning-problem">
<h2>2.3.5. Summary: Components of a Supervised Machine Learning Problem<a class="headerlink" href="#summary-components-of-a-supervised-machine-learning-problem" title="Permalink to this headline">¶</a></h2>
<p>In conclusion, we defined in this lecture the task of supervised learning as well as its key elements. Formally, to apply supervised learning, we define a dataset and a learning algorithm.</p>
<div class="math notranslate nohighlight">
\[ \underbrace{\text{Dataset}}_\text{Features, Attributes, Targets} + \underbrace{\text{Learning Algorithm}}_\text{Model Class + Objective + Optimizer } \to \text{Predictive Model} \]</div>
<p>A dataset consists of training examples, which are pairs of inputs and targets. Each input is a vector of features or attributes. A learning algorithm can be fully defined by a model class, objective and optimizer.</p>
<p>The output of a supervised learning is a predictive model that maps inputs to targets. For instance, it can predict targets on new inputs.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "aml"
        },
        kernelOptions: {
            kernelName: "aml",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'aml'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Welcome to your Jupyter Book</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture3-linear-regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 3: Linear Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Cornell University<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>