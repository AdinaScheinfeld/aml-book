
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 9: Density Estimation &#8212; Applied Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 10: Clustering" href="lecture10-clustering.html" />
    <link rel="prev" title="Lecture 8: Unsupervised Learning" href="lecture8-unsupervised-learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-supervised-learning.html">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-linear-regression.html">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture6-naive-bayes.html">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8-unsupervised-learning.html">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10-clustering.html">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture12-suppor-vector-machines.html">
   Lecture 12: Support Vector Machines
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/contents/lecture9-density-estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontents/lecture9-density-estimation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/contents/lecture9-density-estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outlier-detection-using-probabilistic-models">
   9.1. Outlier Detection Using Probabilistic Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-unsupervised-learning">
     9.1.1. Review: Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outlier-detection">
     9.1.2. Outlier Detection
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-distribution-and-outliers">
       9.1.2.1. Data Distribution and Outliers
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#density-estimation">
     9.1.3. Density Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#outlier-detection-using-density-estimation">
       9.1.3.1. Outlier Detection Using Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-use-probabilistic-models">
       9.1.3.2. Why Use Probabilistic Models?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
       9.1.3.3. Maximum Likelihood Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#toy-example-flipping-a-random-coin">
       9.1.3.4. Toy Example: Flipping a Random Coin
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-density-estimation">
   9.2. Kernel Density Estimation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-density-estimation">
     9.2.1. Review: Density Estimation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#histogram-density-estimation">
     9.2.2. Histogram Density Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#histogram-density-estimation-example">
       9.2.2.1. Histogram Density Estimation: Example
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#limitations-of-histograms">
       9.2.2.2. Limitations of Histograms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     9.2.3. Kernel Density Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tophat-kernel-density-estimation">
       9.2.3.1 Tophat Kernel Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernels-and-kernel-density-estimation">
       9.2.3.2. Kernels and Kernel Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#types-of-kernels">
       9.2.3.3. Types of Kernels
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-density-estimation-example">
       9.2.3.4. Kernel Density Estimation: Example
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kde-in-higher-dimensions">
       9.2.3.5. KDE in Higher Dimensions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choosing-hyperparameters-for-kernels">
       9.2.3.6. Choosing Hyperparameters for Kernels
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-kernel-density-estimation">
       9.2.3.7. Algorithm: Kernel Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pros-and-cons-of-kde">
       9.2.3.8. Pros and Cons of KDE
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nearest-neighbors">
   9.3. Nearest Neighbors
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-classification">
     9.3.1. Review: Classification
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-classification-algorithm-nearest-neighbors">
     9.3.2. A Simple Classification Algorithm: Nearest Neighbors
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-distance-function">
     9.3.3. Choosing a Distance Function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nearest-neighbors">
     9.3.4. K-Nearest Neighbors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-estimates-data-distribution">
       9.3.4.1. KNN Estimates Data Distribution
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-k-nearest-neighbors">
       9.3.4.2. Algorithm: K-Nearest Neighbors
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#non-parametric-models">
       9.3.4.3 Non-Parametric Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pros-and-cons-of-knn">
       9.3.4.4. Pros and Cons of KNN
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 9: Density Estimation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outlier-detection-using-probabilistic-models">
   9.1. Outlier Detection Using Probabilistic Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-unsupervised-learning">
     9.1.1. Review: Unsupervised Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outlier-detection">
     9.1.2. Outlier Detection
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-distribution-and-outliers">
       9.1.2.1. Data Distribution and Outliers
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#density-estimation">
     9.1.3. Density Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#outlier-detection-using-density-estimation">
       9.1.3.1. Outlier Detection Using Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-use-probabilistic-models">
       9.1.3.2. Why Use Probabilistic Models?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
       9.1.3.3. Maximum Likelihood Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#toy-example-flipping-a-random-coin">
       9.1.3.4. Toy Example: Flipping a Random Coin
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-density-estimation">
   9.2. Kernel Density Estimation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-density-estimation">
     9.2.1. Review: Density Estimation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#histogram-density-estimation">
     9.2.2. Histogram Density Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#histogram-density-estimation-example">
       9.2.2.1. Histogram Density Estimation: Example
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#limitations-of-histograms">
       9.2.2.2. Limitations of Histograms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     9.2.3. Kernel Density Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tophat-kernel-density-estimation">
       9.2.3.1 Tophat Kernel Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernels-and-kernel-density-estimation">
       9.2.3.2. Kernels and Kernel Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#types-of-kernels">
       9.2.3.3. Types of Kernels
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-density-estimation-example">
       9.2.3.4. Kernel Density Estimation: Example
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kde-in-higher-dimensions">
       9.2.3.5. KDE in Higher Dimensions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choosing-hyperparameters-for-kernels">
       9.2.3.6. Choosing Hyperparameters for Kernels
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-kernel-density-estimation">
       9.2.3.7. Algorithm: Kernel Density Estimation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pros-and-cons-of-kde">
       9.2.3.8. Pros and Cons of KDE
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nearest-neighbors">
   9.3. Nearest Neighbors
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-classification">
     9.3.1. Review: Classification
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-classification-algorithm-nearest-neighbors">
     9.3.2. A Simple Classification Algorithm: Nearest Neighbors
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-distance-function">
     9.3.3. Choosing a Distance Function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nearest-neighbors">
     9.3.4. K-Nearest Neighbors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-estimates-data-distribution">
       9.3.4.1. KNN Estimates Data Distribution
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm-k-nearest-neighbors">
       9.3.4.2. Algorithm: K-Nearest Neighbors
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#non-parametric-models">
       9.3.4.3 Non-Parametric Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pros-and-cons-of-knn">
       9.3.4.4. Pros and Cons of KNN
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-9-density-estimation">
<h1>Lecture 9: Density Estimation<a class="headerlink" href="#lecture-9-density-estimation" title="Permalink to this headline">¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="outlier-detection-using-probabilistic-models">
<h1>9.1. Outlier Detection Using Probabilistic Models<a class="headerlink" href="#outlier-detection-using-probabilistic-models" title="Permalink to this headline">¶</a></h1>
<p>After the introduction of unsupervised learning from last lecture, we will now look at a specific problem which unsupervised learning can be used to solve, density estimation.</p>
<p>Density estimation is the problem of estimating a probability distribution from data. Let’s first look at a motivating practical problem that involves performing density estimation: outlier detection.</p>
<section id="review-unsupervised-learning">
<h2>9.1.1. Review: Unsupervised Learning<a class="headerlink" href="#review-unsupervised-learning" title="Permalink to this headline">¶</a></h2>
<p>We have a dataset <em>without</em> labels. Our goal is to learn something interesting about the structure of the data:</p>
<ul class="simple">
<li><p>Clusters hidden in the dataset.</p></li>
<li><p>Useful signal hidden in noise, e.g. human speech over a noisy phone.</p></li>
<li><p>Outliers: particularly unusual and/or interesting datapoints.</p></li>
</ul>
</section>
<section id="outlier-detection">
<h2>9.1.2. Outlier Detection<a class="headerlink" href="#outlier-detection" title="Permalink to this headline">¶</a></h2>
<p>Let’s us first formulate the problem of outlier detection.</p>
<p>Suppose we have a (unsupervised) dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{x^{(i)} \mid i = 1,2,...,n\}\)</span>. For example, each <span class="math notranslate nohighlight">\(x^{(i)}\)</span>  could represent:</p>
<ul class="simple">
<li><p>A summary of traffic logs on a computer network</p></li>
<li><p>The state of a machine in a factory</p></li>
</ul>
<p>The goal of outlier detection is, given a new input <span class="math notranslate nohighlight">\(x'\)</span>, we want to determine whether <span class="math notranslate nohighlight">\(x'\)</span> is “normal” or not. The notion of “normal” is elaborated in the below section.</p>
<section id="data-distribution-and-outliers">
<h3>9.1.2.1. Data Distribution and Outliers<a class="headerlink" href="#data-distribution-and-outliers" title="Permalink to this headline">¶</a></h3>
<p>Formally, we assume that the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is sampled IID from the <em>data distribution</em> <span class="math notranslate nohighlight">\(P_\text{data}\)</span>. We denote this as
$<span class="math notranslate nohighlight">\(x \sim P_\text{data}.\)</span>$</p>
<p>Outlier detection can be thought of as answering the question:</p>
<div class="math notranslate nohighlight">
\[ \text{Does the new point $x'$ originate from $P_\text{data}$?} \]</div>
</section>
</section>
<section id="density-estimation">
<h2>9.1.3. Density Estimation<a class="headerlink" href="#density-estimation" title="Permalink to this headline">¶</a></h2>
<p>So how can we detect outliers? We can use density estimation to help us do that!</p>
<p>Definition: An unsupervised probabilistic model is a probability distribution that maps datapoints to probabilities:
$<span class="math notranslate nohighlight">\(P_\theta(x) : \mathcal{X} \to [0,1].\)</span><span class="math notranslate nohighlight">\(
Probabilistic models often have *parameters* \)</span>\theta \in \Theta$.</p>
<p>(For 9.1, we will assume our interested probabilistic models do have a set of finite parameters <span class="math notranslate nohighlight">\(\theta\)</span> that nicely describe the behavior of the probabilistic model, i.e., they are “parametric”. This means choosing a good model is a matter of choosing a good <span class="math notranslate nohighlight">\(\theta\)</span>)</p>
<p>The task of <strong>density estimation</strong> is to learn a probablisitc model <span class="math notranslate nohighlight">\(P_\theta\)</span> on an unsupervised dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> to approximate the true data distribution <span class="math notranslate nohighlight">\(P_\text{data}\)</span>.</p>
<section id="outlier-detection-using-density-estimation">
<h3>9.1.3.1. Outlier Detection Using Density Estimation<a class="headerlink" href="#outlier-detection-using-density-estimation" title="Permalink to this headline">¶</a></h3>
<p>Recall that in outlier detection, whether an input is “normal” translates to whether the input originate from the data distribution <span class="math notranslate nohighlight">\(P_\text{data}\)</span>. We need to somewhat know <span class="math notranslate nohighlight">\(P_\text{data}\)</span> in order to say if an input is an outlier.</p>
<p>Density estimation is precisely the task of estimating <span class="math notranslate nohighlight">\(P_\text{data}\)</span> with <span class="math notranslate nohighlight">\(P_\theta\)</span>.</p>
<p>Thus, we can perform outlier detection with probabilistic models:</p>
<ol class="simple">
<li><p><strong>Density estimation</strong>: We fit <span class="math notranslate nohighlight">\(P_\theta\)</span> on <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> to approximate <span class="math notranslate nohighlight">\(P_\text{data}\)</span></p></li>
<li><p><strong>Outlier detection</strong>: Given <span class="math notranslate nohighlight">\(x'\)</span>, we use <span class="math notranslate nohighlight">\(P_\theta(x')\)</span> to determine if <span class="math notranslate nohighlight">\(x'\)</span> is an outlier.</p></li>
</ol>
</section>
<section id="why-use-probabilistic-models">
<h3>9.1.3.2. Why Use Probabilistic Models?<a class="headerlink" href="#why-use-probabilistic-models" title="Permalink to this headline">¶</a></h3>
<p>There are many other tasks that we can solve with a good model <span class="math notranslate nohighlight">\(P_\theta\)</span>.</p>
<ol class="simple">
<li><p>Generation: sample new objects from <span class="math notranslate nohighlight">\(P_\theta\)</span>, such as images.</p></li>
<li><p>Structure learning: find interesting structure in <span class="math notranslate nohighlight">\(P_\text{data}\)</span></p></li>
<li><p>Density estimation: approximate <span class="math notranslate nohighlight">\(P_\theta \approx P_\text{data}\)</span> and use it to solve any downstream task (generation, clustering, outlier detection, etc.).</p></li>
</ol>
<p>We are going to be interested in the latter.</p>
</section>
<section id="maximum-likelihood-estimation">
<h3>9.1.3.3. Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h3>
<p>So how exactly can one learn a probabilistic model <span class="math notranslate nohighlight">\(P_\theta(x)\)</span>, i.e., learn good/optimal <span class="math notranslate nohighlight">\(\theta\)</span>, that estimates the data distribution <span class="math notranslate nohighlight">\(P_\text{data}\)</span>?</p>
<p>One way we can do that is by optimizing
the <em>maximum log-likelihood</em> objective
$<span class="math notranslate nohighlight">\(
\max_\theta \ell(\theta) = \max_{\theta} \frac{1}{n}\sum_{i=1}^n \log P_\theta({x}^{(i)}).
\)</span>$</p>
<p>This asks that <span class="math notranslate nohighlight">\(P_\theta\)</span> assign a high probability to the training instances in the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<p>Maximizing likelihood is closely related to minimizing the Kullback-Leibler (KL) divergence <span class="math notranslate nohighlight">\(D(\cdot\|\cdot)\)</span> between the model distribution and the data distribution.
$<span class="math notranslate nohighlight">\(
D(P_\text{data} \| P_\theta) = \sum_{{\bf x}} P_\text{data}({\bf x}) \log \frac{P_\text{data}({\bf x})}{P_\theta({\bf x})}.
\)</span>$</p>
<p>The KL divergence is always non-negative, and equals zero when <span class="math notranslate nohighlight">\(P_\text{data}\)</span> and <span class="math notranslate nohighlight">\(P_\theta\)</span> are identical. This makes it a natural measure of similarity that’s useful for comparing distributions.</p>
</section>
<section id="toy-example-flipping-a-random-coin">
<h3>9.1.3.4. Toy Example: Flipping a Random Coin<a class="headerlink" href="#toy-example-flipping-a-random-coin" title="Permalink to this headline">¶</a></h3>
<p>How should we choose <span class="math notranslate nohighlight">\(P_\theta(x)\)</span>, i.e., choose <span class="math notranslate nohighlight">\(\theta\)</span>, if 3 out of 5 coin tosses are heads? Let’s apply maximum likelihood learning.</p>
<ul class="simple">
<li><p>Our model is <span class="math notranslate nohighlight">\(P_\theta(x=H)=\theta\)</span> and <span class="math notranslate nohighlight">\(P_\theta(x=T)=1-\theta\)</span></p></li>
<li><p>Our data is: <span class="math notranslate nohighlight">\(\{H,H,T,H,T\}\)</span></p></li>
<li><p>The likelihood of the data is <span class="math notranslate nohighlight">\(\prod_{i} P_\theta(x_i)=\theta \cdot \theta \cdot (1-\theta) \cdot \theta \cdot (1-\theta)\)</span>.</p></li>
</ul>
<p>We optimize for <span class="math notranslate nohighlight">\(\theta\)</span> which makes the data most likely. What is the solution in this case?</p>
<p>Below we plot the likelihood of the observed data (3 heads out of 5 coin tosses) vs <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># our dataset is {H, H, T, H, T}; if theta = P(x=H), we get:</span>
<span class="n">coin_likelihood</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">theta</span><span class="o">*</span><span class="n">theta</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">theta</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span>

<span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># plot the likedlihood at each theta</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">coin_likelihood</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1168f7390&gt;]
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_15_1.png" src="../_images/lecture9-density-estimation_15_1.png" />
</div>
</div>
<p>You can see that the optimizing <span class="math notranslate nohighlight">\(\theta\)</span> is <span class="math notranslate nohighlight">\(0.6\)</span>.</p>
<p>Given a trained model <span class="math notranslate nohighlight">\(P_\theta\)</span> with <span class="math notranslate nohighlight">\(\theta = 0.6\)</span>, we can ask for the probability of seeing a sequence of ten tails:</p>
<div class="math notranslate nohighlight">
\[P(\text{ten tails}) =  \prod_{j=1}^{10} P_\theta(x^{(j)}=T) = (1-0.6)^{10} \approx 10^{-4}\]</div>
<p>This gives us an estimate of whether certain sequences are likely or not. If they are not, then they are outliers!</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="kernel-density-estimation">
<h1>9.2. Kernel Density Estimation<a class="headerlink" href="#kernel-density-estimation" title="Permalink to this headline">¶</a></h1>
<p>Next, we will look at a more sophisticated technique for estimating densities.</p>
<section id="review-density-estimation">
<h2>9.2.1. Review: Density Estimation<a class="headerlink" href="#review-density-estimation" title="Permalink to this headline">¶</a></h2>
<p>Recall that the problem of density estimation is to approximate the data distribution <span class="math notranslate nohighlight">\(P_\text{data}\)</span> with the model <span class="math notranslate nohighlight">\(P\)</span>.
$<span class="math notranslate nohighlight">\( P \approx P_\text{data}. \)</span>$</p>
<p>(Notice that the probabilistic model <span class="math notranslate nohighlight">\(P\)</span> needs not be nicely characterized by a set of parameters <span class="math notranslate nohighlight">\(\theta\)</span>. They can be “non-parametric”)</p>
<p>It’s also a general learning task. We can solve many downstream tasks using a good model <span class="math notranslate nohighlight">\(P\)</span>:</p>
<ul class="simple">
<li><p>Outlier and novelty detection</p></li>
<li><p>Generating new samples <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p>Visualizing and understanding the structure of <span class="math notranslate nohighlight">\(P_\text{data}\)</span></p></li>
</ul>
</section>
<section id="histogram-density-estimation">
<h2>9.2.2. Histogram Density Estimation<a class="headerlink" href="#histogram-density-estimation" title="Permalink to this headline">¶</a></h2>
<p>Perhaps, one of the simplest approaches to density estimation (constructing an estimating probabilistic model) is by forming a histogram. This definitely seems conceptually simpler than maximizing the likelihood of data with respect to a parameterized probabilistic model from earlier.</p>
<p>To construct a histogram, we partition the input space <span class="math notranslate nohighlight">\(x\)</span> into a <span class="math notranslate nohighlight">\(d\)</span>-dimensional grid and counts the number of points in each cell/bin.</p>
<section id="histogram-density-estimation-example">
<h3>9.2.2.1. Histogram Density Estimation: Example<a class="headerlink" href="#histogram-density-estimation-example" title="Permalink to this headline">¶</a></h3>
<p>This is best illustrated by an example.</p>
<p>Let’s start by creating a simple 1D dataset coming from a mixture of two Gaussians:</p>
<div class="math notranslate nohighlight">
\[P_\text{data}(x) = 0.3 \cdot \mathcal{N}(x ; \mu=0, \sigma=1) + 0.7 \cdot \mathcal{N}(x ; \mu=5, \sigma=1)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># number of points</span>
<span class="c1"># concat samples from two Gaussians:</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.3</span> <span class="o">*</span> <span class="n">N</span><span class="p">)),</span> 
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.7</span> <span class="o">*</span> <span class="n">N</span><span class="p">))</span>
<span class="p">))[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="c1"># print out X</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387
  6.74481176  4.2387931   5.3190391   4.75062962  6.46210794  2.93985929
  4.6775828   4.61594565  6.13376944  3.90010873  4.82757179  4.12214158
  5.04221375  5.58281521]
</pre></div>
</div>
</div>
</div>
<p>We can now estimate the density using a histogram, counting the number of datapoints in each bin.</p>
<p>Below, we visualize the result histogram:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># We set the number of bins to be equal to 10 - 1 = 9 (np.linspace gets us boundaries of the bins)</span>
<span class="c1"># Though the figure seems like it has less than 9 bins because some bins do not have any data</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># plot the histogram</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s1">&#39;.k&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># plot the points in X</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.02, 0.25)
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_24_1.png" src="../_images/lecture9-density-estimation_24_1.png" />
</div>
</div>
</section>
<section id="limitations-of-histograms">
<h3>9.2.2.2. Limitations of Histograms<a class="headerlink" href="#limitations-of-histograms" title="Permalink to this headline">¶</a></h3>
<p>Though conceptually simple, histogram-based methods have a number of shortcomings:</p>
<ul class="simple">
<li><p>The histogram is not “smooth”. We usually expect elements in the same local region of the data distribution to have somewhat similar probabilities (no sudden change).</p></li>
<li><p>The shape of the histogram depends on the bin positions.</p></li>
<li><p>The number of grid cells increases exponentially with dimension <span class="math notranslate nohighlight">\(d\)</span>.</p></li>
</ul>
<p>Let’s visualize what we mean when we say that shape of the histogram depends on the histogram bins. Below we plot two figures, where the bins of the right figure are the bins of the left figure but shifted by 0.75:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># plot the histogram</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="o">+</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># plot the histogram with bin centers shifted by 0.75</span>
<span class="k">for</span> <span class="n">axi</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">():</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s1">&#39;.k&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># plot the points in X</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture9-density-estimation_27_0.png" src="../_images/lecture9-density-estimation_27_0.png" />
</div>
</div>
<p>As you can see, the two figures look very different even by just a small shift of bins.</p>
</section>
</section>
<section id="id1">
<h2>9.2.3. Kernel Density Estimation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Kernel density estimation (KDE) is a different approach to density estimation that address some of the issues of histogram density estimation.</p>
<ul class="simple">
<li><p>In histogram density estimation, the density is proportional to:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P_\theta(x) \propto \text{\# of points in the bin where $x$ falls} \]</div>
<ul class="simple">
<li><p>In kernel density estimation, the density is proportional to:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P_\theta(x) \propto \text{\# of points in $\mathcal{D}$ &quot;near&quot; $x$} \]</div>
<p>Here, <span class="math notranslate nohighlight">\(\propto\)</span> means “proportional to” (up to a normalizing constant).</p>
<p>In other words:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(x\)</span> is in an area with many points in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, then <span class="math notranslate nohighlight">\(P_\theta(x)\)</span> is large</p></li>
<li><p>If <span class="math notranslate nohighlight">\(x\)</span> is in an area with few points in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, then <span class="math notranslate nohighlight">\(P_\theta(x)\)</span> is small</p></li>
</ul>
<p>You can also view KDE as counting the % of points that fall in a “bin” centered at <span class="math notranslate nohighlight">\(x\)</span>.</p>
<section id="tophat-kernel-density-estimation">
<h3>9.2.3.1 Tophat Kernel Density Estimation<a class="headerlink" href="#tophat-kernel-density-estimation" title="Permalink to this headline">¶</a></h3>
<p>The simplest form of this strategy (Tophat KDE) assumes a model of the form
$<span class="math notranslate nohighlight">\(P_\delta(x) = \frac{N(x; \delta)}{n},\)</span><span class="math notranslate nohighlight">\(
where
\)</span><span class="math notranslate nohighlight">\( N(x; \delta) = |\{x^{(i)} : ||x^{(i)} - x || \leq \delta/2\}|, \)</span><span class="math notranslate nohighlight">\(
is the number of points that are within a bin of with \)</span>\delta<span class="math notranslate nohighlight">\( centered at \)</span>x$.</p>
<p>This is best understood via a picture. Below we fit Tophat KDE on the 1D mixture of two Gaussians dataset from 9.2.2.1. and visualize it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KernelDensity</span>

<span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;tophat&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># fit a KDE model</span>
<span class="n">x_ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># choose 1000 points on x-axis</span>
<span class="n">log_density</span> <span class="o">=</span> <span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span> <span class="c1"># compute density at 1000 points</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_density</span><span class="p">))</span> <span class="c1"># plot the density estimate</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s1">&#39;.k&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># plot the points in X as black dots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.02, 0.32)
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_33_1.png" src="../_images/lecture9-density-estimation_33_1.png" />
</div>
</div>
<p>The above algorithm no longer depends on the position of bins like histogram density estimation.</p>
<p>However, it still has the problem of producing a density estimate that is not smooth.</p>
<p>We are going to resolve this by replacing histogram counts with weighted averages:
\begin{align*}
P_\theta(x)
&amp; \propto \text{# of points in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> “near” <span class="math notranslate nohighlight">\(x\)</span>, weighted by distance from <span class="math notranslate nohighlight">\(x\)</span>} \
&amp; = \sum_{i=1}^n \text{weight}(x^{(i)}, x).
\end{align*}</p>
<p>We count all points <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, but each gets a <span class="math notranslate nohighlight">\(\text{weight}(x^{(i)}, x)\)</span> that is large if <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and <span class="math notranslate nohighlight">\(x\)</span> are “similar”. We will talk about this “similarity” in the following section.</p>
</section>
<section id="kernels-and-kernel-density-estimation">
<h3>9.2.3.2. Kernels and Kernel Density Estimation<a class="headerlink" href="#kernels-and-kernel-density-estimation" title="Permalink to this headline">¶</a></h3>
<p>A <em>kernel function</em> <span class="math notranslate nohighlight">\(K : \mathcal{X} \times \mathcal{X} \to [0, \infty]\)</span> maps pairs of vectors <span class="math notranslate nohighlight">\(x, z \in \mathcal{X}\)</span> to a real-valued score <span class="math notranslate nohighlight">\(K(x,z)\)</span>.</p>
<ul class="simple">
<li><p>A kernel represents the similarity between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
<li><p>We will see many ways of defining “similarity”; they will all fit the framework that follows.</p></li>
</ul>
<p>A kernelized density model <span class="math notranslate nohighlight">\(P\)</span> takes the form:
$<span class="math notranslate nohighlight">\(P(x) \propto \sum_{i=1}^n K(x, x^{(i)}).\)</span>$
This can be interpreted in several ways:</p>
<ul class="simple">
<li><p>We count the number of points “near” <span class="math notranslate nohighlight">\(x\)</span>, but each <span class="math notranslate nohighlight">\(x^{(i)}\)</span> has a weight <span class="math notranslate nohighlight">\(K(x, x^{(i)})\)</span> that depends on similarity between <span class="math notranslate nohighlight">\(x, x^{(i)}\)</span>.</p></li>
<li><p>We place a “micro-density” <span class="math notranslate nohighlight">\(K(x, x^{(i)})\)</span> at each <span class="math notranslate nohighlight">\(x^{(i)}\)</span>; the final density <span class="math notranslate nohighlight">\(P(x)\)</span> is their sum.</p></li>
</ul>
</section>
<section id="types-of-kernels">
<h3>9.2.3.3. Types of Kernels<a class="headerlink" href="#types-of-kernels" title="Permalink to this headline">¶</a></h3>
<p>Here, we introduce a few kernels that are popular for density estimation.</p>
<p>The following kernels are available in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<ul class="simple">
<li><p>Gaussian kernel <span class="math notranslate nohighlight">\(K(x,z; \delta) \propto \exp(-||x-z||^2/2\delta^2)\)</span></p></li>
<li><p>Tophat kernel <span class="math notranslate nohighlight">\(K(x,z; \delta) = 1 \text{ if } ||x-z|| \leq \delta/2\)</span> else <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>Epanechnikov kernel <span class="math notranslate nohighlight">\(K(x,z; \delta) \propto 1 - ||x-z||^2/\delta^2\)</span></p></li>
<li><p>Exponential kernel <span class="math notranslate nohighlight">\(K(x,z; \delta) \propto \exp(-||x-z||/\delta)\)</span></p></li>
<li><p>Linear kernel <span class="math notranslate nohighlight">\(K(x,z; \delta) \propto (1 - ||x-z||/\delta)^+\)</span></p></li>
</ul>
<p>It’s easier to understand these kernels by looking at the corresponding visual figures plotted below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html</span>
<span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">X_src</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">format_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;0&#39;</span>
    <span class="k">elif</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;$\delta/2$&#39;</span>
    <span class="k">elif</span> <span class="n">x</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;-$\delta/2$&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;</span><span class="si">%i</span><span class="s1">$\delta$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;tophat&#39;</span><span class="p">,</span> <span class="s1">&#39;epanechnikov&#39;</span><span class="p">,</span>
                            <span class="s1">&#39;exponential&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;cosine&#39;</span><span class="p">]):</span>
    <span class="n">axi</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">log_dens</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_src</span><span class="p">)</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">X_plot</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dens</span><span class="p">),</span> <span class="s1">&#39;-k&#39;</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#AAAAFF&#39;</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">2.6</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>

    <span class="n">axi</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">format_func</span><span class="p">))</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">NullLocator</span><span class="p">())</span>

    <span class="n">axi</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">2.9</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Available Kernels&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Available Kernels&#39;)
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_42_1.png" src="../_images/lecture9-density-estimation_42_1.png" />
</div>
</div>
</section>
<section id="kernel-density-estimation-example">
<h3>9.2.3.4. Kernel Density Estimation: Example<a class="headerlink" href="#kernel-density-estimation-example" title="Permalink to this headline">¶</a></h3>
<p>Let’s look at an example in the context of the 1D points (mixture of two Gaussians) we have seen earlier.</p>
<p>We will fit a model of the form
$<span class="math notranslate nohighlight">\(P(x) = \sum_{i=1}^n K(x, x^{(i)})\)</span><span class="math notranslate nohighlight">\(
with a Gaussian kernel \)</span>K(x,z; \delta) \propto \exp(-||x-z||^2/2\delta^2)$.</p>
<p>Below we plot the density estimate as blue line, the datapoints as black dots, and example kernels as red dotted line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KernelDensity</span>

<span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># fit a KDE model</span>
<span class="n">x_ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># choose 1000 points on x-axis</span>
<span class="n">log_density</span> <span class="o">=</span> <span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span> <span class="c1"># compute density at 1000 points</span>
<span class="n">gaussian_kernel</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">z</span> <span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mf">0.75</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># gaussian kernel</span>
<span class="n">kernel_linspace</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span><span class="n">x</span><span class="o">+</span><span class="mf">1.2</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_density</span><span class="p">))</span> <span class="c1"># plot the density estimate</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s1">&#39;.k&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># plot the points in X as black dots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kernel_linspace</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="mf">0.07</span><span class="o">*</span><span class="n">gaussian_kernel</span><span class="p">(</span><span class="mi">4</span><span class="p">)(</span><span class="n">kernel_linspace</span><span class="p">(</span><span class="mi">4</span><span class="p">)),</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kernel_linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="mf">0.07</span><span class="o">*</span><span class="n">gaussian_kernel</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">kernel_linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">)),</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kernel_linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.07</span><span class="o">*</span><span class="n">gaussian_kernel</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">kernel_linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.02, 0.32)
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_44_1.png" src="../_images/lecture9-density-estimation_44_1.png" />
</div>
</div>
<p>Recall that the dataset is a mixture of two Gaussians centered at <span class="math notranslate nohighlight">\(x = 0\)</span> and <span class="math notranslate nohighlight">\(x = 5\)</span>. This estimate does look consistent with that.</p>
</section>
<section id="kde-in-higher-dimensions">
<h3>9.2.3.5. KDE in Higher Dimensions<a class="headerlink" href="#kde-in-higher-dimensions" title="Permalink to this headline">¶</a></h3>
<p>In principle, kernel density estimation also works in higher dimensions.</p>
<p>However, the number of datapoints needed for a good fit incrases expoentially with the dimension, which limits the applications of this model in high dimensions.</p>
</section>
<section id="choosing-hyperparameters-for-kernels">
<h3>9.2.3.6. Choosing Hyperparameters for Kernels<a class="headerlink" href="#choosing-hyperparameters-for-kernels" title="Permalink to this headline">¶</a></h3>
<p>Each kernel has a notion of “bandwidth” <span class="math notranslate nohighlight">\(\delta\)</span>. This is a hyperparameter that controls the “smoothness” of the fit.</p>
<ul class="simple">
<li><p>We can choose it using inspection or heuristics like we did for <span class="math notranslate nohighlight">\(K\)</span> in <span class="math notranslate nohighlight">\(K\)</span>-Means.</p></li>
<li><p>Because we have a probabilistic model, we can also estimate likelihood on a holdout dataset (more on this later!)</p></li>
</ul>
<p>Let’s illustrate how the bandwidth affects smoothness via an example.</p>
<p>Below we plot two figures, left with very high bandwidth and right with very low bandwidth.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KernelDensity</span>

<span class="n">kde1</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># fit a KDE model</span>
<span class="n">kde2</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># fit a KDE model</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">kde1</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)))</span> <span class="c1"># plot the density estimate</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">kde2</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)))</span> <span class="c1"># plot the density estimate</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bandwidth Too High&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bandwidth Too Low&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">axi</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">():</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s1">&#39;.k&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># plot the points in X</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lecture9-density-estimation_49_0.png" src="../_images/lecture9-density-estimation_49_0.png" />
</div>
</div>
<p>You can see that the left figure is too smoothed out, making us lose a lot of information. On the other hand, the right figure is very sharp, making it less generalizable.</p>
</section>
<section id="algorithm-kernel-density-estimation">
<h3>9.2.3.7. Algorithm: Kernel Density Estimation<a class="headerlink" href="#algorithm-kernel-density-estimation" title="Permalink to this headline">¶</a></h3>
<p>We summarize the kernel density estimation algorithm below:</p>
<ul class="simple">
<li><p><strong>Type</strong>: Unsupervised learning (density estimation).</p></li>
<li><p><strong>Model family</strong>: Non-parametric. Sum of <span class="math notranslate nohighlight">\(n\)</span> kernels.</p></li>
<li><p><strong>Objective function</strong>: Log-likelihood to choose optimal bandwidth.</p></li>
<li><p><strong>Optimizer</strong>: Grid search.</p></li>
</ul>
</section>
<section id="pros-and-cons-of-kde">
<h3>9.2.3.8. Pros and Cons of KDE<a class="headerlink" href="#pros-and-cons-of-kde" title="Permalink to this headline">¶</a></h3>
<p>We have introduced kernel density estimation which overcomes some of the drawbacks of histogram density estimation introduced earlier. We summarize the pros and cons of KDE below:</p>
<p>Pros:</p>
<ul class="simple">
<li><p>Can approximate any data distribution arbitrarily well.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Need to store entire dataset to make queries, which is computationally prohibitive.</p></li>
<li><p>Number of data needed scale exponentially with dimension (“curse of dimensionality”).</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="nearest-neighbors">
<h1>9.3. Nearest Neighbors<a class="headerlink" href="#nearest-neighbors" title="Permalink to this headline">¶</a></h1>
<p>We are now going to take a little detour back to supervised learning and apply some of these density esimation ideas to supervised learning.</p>
<p>Precisely, we will look at an algorithm called Nearest Neighbors.</p>
<section id="review-classification">
<h2>9.3.1. Review: Classification<a class="headerlink" href="#review-classification" title="Permalink to this headline">¶</a></h2>
<p>Consider a supervised training dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(n)}, y^{(n)})\}\)</span>.</p>
<p>We distinguish between two types of supervised learning problems depending on the targets <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p>
<ol class="simple">
<li><p><strong>Regression</strong>: The target variable <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> is continuous:  <span class="math notranslate nohighlight">\(\mathcal{Y} \subseteq \mathbb{R}\)</span>.</p></li>
<li><p><strong>Classification</strong>: The target variable <span class="math notranslate nohighlight">\(y\)</span> is discrete and takes on one of <span class="math notranslate nohighlight">\(K\)</span> possible values:  <span class="math notranslate nohighlight">\(\mathcal{Y} = \{y_1, y_2, \ldots y_K\}\)</span>. Each discrete value corresponds to a <em>class</em> that we want to predict.</p></li>
</ol>
</section>
<section id="a-simple-classification-algorithm-nearest-neighbors">
<h2>9.3.2. A Simple Classification Algorithm: Nearest Neighbors<a class="headerlink" href="#a-simple-classification-algorithm-nearest-neighbors" title="Permalink to this headline">¶</a></h2>
<p>Suppose we are given a training dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(n)}, y^{(n)})\}\)</span>. At inference time, we receive a query point <span class="math notranslate nohighlight">\(x'\)</span> and we want to predict its label <span class="math notranslate nohighlight">\(y'\)</span>.</p>
<p>A really simple but surprisingly effective way of returning <span class="math notranslate nohighlight">\(y'\)</span> is the <em>nearest neighbors</em> approach.</p>
<ul class="simple">
<li><p>Given a query datapoint <span class="math notranslate nohighlight">\(x'\)</span>, find the training example <span class="math notranslate nohighlight">\((x, y)\)</span> in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> that’s closest to <span class="math notranslate nohighlight">\(x'\)</span>, in the sense that <span class="math notranslate nohighlight">\(x\)</span> is “nearest” to <span class="math notranslate nohighlight">\(x'\)</span></p></li>
<li><p>Return <span class="math notranslate nohighlight">\(y\)</span>, the label of the “nearest neighbor” <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<p>In the example below on the Iris dataset, the red cross denotes the query <span class="math notranslate nohighlight">\(x'\)</span>. The closest class to it is “Virginica”. (We’re only using the first two features in the dataset for simplicity.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Plot also the training points</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">7.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal Width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Query Point&#39;</span><span class="p">,</span> <span class="s1">&#39;Training Data&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">p1</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">p2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Setosa&#39;</span><span class="p">,</span> <span class="s1">&#39;Versicolour&#39;</span><span class="p">,</span> <span class="s1">&#39;Virginica&#39;</span><span class="p">,</span> <span class="s1">&#39;Query&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x12982b4e0&gt;
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_58_1.png" src="../_images/lecture9-density-estimation_58_1.png" />
</div>
</div>
</section>
<section id="choosing-a-distance-function">
<h2>9.3.3. Choosing a Distance Function<a class="headerlink" href="#choosing-a-distance-function" title="Permalink to this headline">¶</a></h2>
<p>How do we select the point <span class="math notranslate nohighlight">\(x\)</span> that is the closest to the query point <span class="math notranslate nohighlight">\(x'\)</span>? There are many options:</p>
<ul class="simple">
<li><p>The Euclidean distance <span class="math notranslate nohighlight">\(|| x - x' ||_2 = \sqrt{\sum_{j=1}^d |x_j - x'_j|^2)}\)</span> is a popular choice.</p></li>
</ul>
<ul class="simple">
<li><p>The Minkowski distance <span class="math notranslate nohighlight">\(|| x - x' ||_p = (\sum_{j=1}^d |x_j - x'_j|^p)^{1/p}\)</span> generalizes the Euclidean, L1 and other distances.</p></li>
</ul>
<ul class="simple">
<li><p>The Mahalanobis distance <span class="math notranslate nohighlight">\(\sqrt{x^\top V x}\)</span> for a positive semidefinite matrix <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{d \times d}\)</span> also generalizes the Euclidean distnace.</p></li>
</ul>
<ul class="simple">
<li><p>Discrete-valued inputs can be examined via the Hamming distance <span class="math notranslate nohighlight">\(|\{j : x_j \neq x_j'\}|\)</span> and other distances.</p></li>
</ul>
<p>Let’s apply Nearest Neighbors to the above dataset using the Euclidean distance (or equiavalently, Minkowski with <span class="math notranslate nohighlight">\(p=2\)</span>)</p>
<p>Below we color each position in the grid with the corresponding class color (according to Nearest Neighbors with Euclidean distance)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Train a Nearest Neighbors Model</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;minkowski&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">],</span> <span class="n">iris_y</span><span class="p">)</span>

<span class="c1"># Create color maps</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="s1">&#39;cornflowerblue&#39;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">])</span>

<span class="c1"># Plot the decision boundary. For that, we will assign a color to each</span>
<span class="c1"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

<span class="c1"># Put the result into a color plot</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

<span class="c1"># Plot also the training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal Width&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Sepal Width&#39;)
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_65_1.png" src="../_images/lecture9-density-estimation_65_1.png" />
</div>
</div>
<p>In the above example, the regions of the 2D space that are assigned to each class are highly irregular. In areas where the two classes overlap, the decision of the boundary flips between the classes, depending on which point is closest to it.</p>
</section>
<section id="k-nearest-neighbors">
<h2>9.3.4. K-Nearest Neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Permalink to this headline">¶</a></h2>
<p>Intuitively, we expect the true decision boundary to be smooth. Therefore, we average <span class="math notranslate nohighlight">\(K\)</span> nearest neighbors at a query point.</p>
<ul class="simple">
<li><p>Given a query datapoint <span class="math notranslate nohighlight">\(x'\)</span>, find the <span class="math notranslate nohighlight">\(K\)</span> training examples <span class="math notranslate nohighlight">\(\mathcal{N} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(K)}, y^{(K)})\} \subseteq D\)</span> that are closest to <span class="math notranslate nohighlight">\(x'\)</span>.</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(y_\mathcal{N}\)</span>, the consensus label of the neighborhood <span class="math notranslate nohighlight">\(\mathcal{N}\)</span>.</p></li>
</ul>
<p>The consesus <span class="math notranslate nohighlight">\(y_\mathcal{N}\)</span> can be determined by voting, weighted average, kernels, etc.</p>
<p>Let’s look at Nearest Neighbors with a neighborhood of 30. Once again, we color each position in the grid with the corresponding class color. The decision boundary is much smoother than before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html</span>
<span class="c1"># Train a Nearest Neighbors Model</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;minkowski&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">],</span> <span class="n">iris_y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

<span class="c1"># Put the result into a color plot</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

<span class="c1"># Plot also the training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal Width&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Sepal Width&#39;)
</pre></div>
</div>
<img alt="../_images/lecture9-density-estimation_70_1.png" src="../_images/lecture9-density-estimation_70_1.png" />
</div>
</div>
<section id="knn-estimates-data-distribution">
<h3>9.3.4.1. KNN Estimates Data Distribution<a class="headerlink" href="#knn-estimates-data-distribution" title="Permalink to this headline">¶</a></h3>
<p>Suppose that the output <span class="math notranslate nohighlight">\(y'\)</span> of KNN is the average target in the neighborhood <span class="math notranslate nohighlight">\(\mathcal{N}(x')\)</span> around the query <span class="math notranslate nohighlight">\(x'\)</span>.
Observe that we can write:
$<span class="math notranslate nohighlight">\(y' = \frac{1}{K} \sum_{(x, y) \in \mathcal{N}(x')} y \approx \mathbb{E}[y \mid x'].\)</span>$</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x \approx x'\)</span> and when <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is reasonably smooth, each <span class="math notranslate nohighlight">\(y\)</span> for <span class="math notranslate nohighlight">\((x,y) \in \mathcal{N}(x')\)</span> is approximately a sample from <span class="math notranslate nohighlight">\(\mathbb{P}(y\mid x')\)</span> (since <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> doesn’t change much around <span class="math notranslate nohighlight">\(x'\)</span>, <span class="math notranslate nohighlight">\(\mathbb{P}(y\mid x') \approx \mathbb{P}(y\mid x)\)</span>).</p></li>
</ul>
<ul class="simple">
<li><p>Thus <span class="math notranslate nohighlight">\(y'\)</span> is essentially a Monte Carlo estimate of <span class="math notranslate nohighlight">\(\mathbb{E}[y \mid x']\)</span> (the average of <span class="math notranslate nohighlight">\(K\)</span> samples from <span class="math notranslate nohighlight">\(\mathbb{P}(y\mid x')\)</span>).</p></li>
</ul>
</section>
<section id="algorithm-k-nearest-neighbors">
<h3>9.3.4.2. Algorithm: K-Nearest Neighbors<a class="headerlink" href="#algorithm-k-nearest-neighbors" title="Permalink to this headline">¶</a></h3>
<p>We summarize the K-Nearest Neighbors algorithm below:</p>
<ul class="simple">
<li><p><strong>Type</strong>: Supervised learning (regression and classification)</p></li>
<li><p><strong>Model family</strong>: Consensus over <span class="math notranslate nohighlight">\(K\)</span> training instances.</p></li>
<li><p><strong>Objective function</strong>: Euclidean, Minkowski, Hamming, etc.</p></li>
<li><p><strong>Optimizer</strong>: Non at training. Nearest neighbor search at inference using specialized search algorithms (Hashing, KD-trees).</p></li>
<li><p><strong>Probabilistic interpretation</strong>: Directly approximating the density <span class="math notranslate nohighlight">\(P_\text{data}(y|x)\)</span>.</p></li>
</ul>
</section>
<section id="non-parametric-models">
<h3>9.3.4.3 Non-Parametric Models<a class="headerlink" href="#non-parametric-models" title="Permalink to this headline">¶</a></h3>
<p>Nearest neighbors is an example of a <em>non-parametric</em> model. Parametric vs. non-parametric are is a key distinguishing characteristic for machine learning models.</p>
<p>A parametric model <span class="math notranslate nohighlight">\(f_\theta(x) : \mathcal{X} \times \Theta \to \mathcal{Y}\)</span> is defined by a finite set of parameters <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> whose dimensionality is constant with respect to the dataset. Linear models of the form
$<span class="math notranslate nohighlight">\( f_\theta(x) = \theta^\top x \)</span>$
are an example of a parametric model.</p>
<p>On the other hand, for a non-parametric model, the function <span class="math notranslate nohighlight">\(f\)</span> uses the entire training dataset (or a post-proccessed version of it) to make predictions, as in <span class="math notranslate nohighlight">\(K\)</span>-Nearest Neighbors. In other words, the complexity of the model increases with dataset size. Because of this, non-parametric models have the advantage of not losing any information at training time. Nevertheless, they are also computationally less tractable and may easily overfit the training set.</p>
</section>
<section id="pros-and-cons-of-knn">
<h3>9.3.4.4. Pros and Cons of KNN<a class="headerlink" href="#pros-and-cons-of-knn" title="Permalink to this headline">¶</a></h3>
<p>We have introduced KNN, our first non-parametric model for supervised learning. We summarize the pros and cons of the algorithm below:</p>
<p>Pros:</p>
<ul class="simple">
<li><p>Can approximate any data distribution arbtrarily well.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Need to store entire dataset to make queries, which is computationally prohibitive.</p></li>
<li><p>Number of data needed scale exponentially with dimension (“curse of dimensionality”).</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="lecture8-unsupervised-learning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 8: Unsupervised Learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture10-clustering.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 10: Clustering</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Cornell University<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>