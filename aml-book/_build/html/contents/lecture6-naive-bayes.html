
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 6: Generative Models and Naive Bayes &#8212; Applied ML</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 7: Gaussian Discriminant Analysis" href="lecture7-gaussian-discriminant-analysis.html" />
    <link rel="prev" title="Lecture 5: Regularization" href="lecture5-regularization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied ML</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Applied Machine Learning - Index
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture1-introduction.html">
   Lecture 1: Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-supervised-learning.html">
   Lecture 2: Supervised Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-linear-regression.html">
   Lecture 3: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-classification.html">
   Lecture 4: Classification and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-regularization.html">
   Lecture 5: Regularization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture7-gaussian-discriminant-analysis.html">
   Lecture 7: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture8-unsupervised-learning.html">
   Lecture 8: Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture9-density-estimation.html">
   Lecture 9: Density Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture10-clustering.html">
   Lecture 10: Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture12-suppor-vector-machines.html">
   Lecture 12: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture13-svm-dual.html">
   Lecture 13: Dual Formulation of Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture14-kernels.html">
   Lecture 14: Kernels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture15-decision-trees.html">
   Lecture 15: Tree-Based Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture16-boosting.html">
   Lecture 16: Boosting
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/contents/lecture6-naive-bayes.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kuleshov-group/aml-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kuleshov-group/aml-book/issues/new?title=Issue%20on%20page%20%2Fcontents/lecture6-naive-bayes.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/kuleshov-group/aml-book/master?urlpath=tree/docs/contents/lecture6-naive-bayes.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-classification">
   6.1. Text Classification
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-classification">
     Review: Classification
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     6.1.1. Text Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-the-twenty-newsgroups-classification-dataset">
       6.1.1.1. Example: The Twenty Newsgroups Classification Dataset
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-representations-for-text">
     6.1.2. Feature Representations for Text
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bag-of-words-representations">
       6.1.2.1. Bag of Words Representations
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#practical-considerations">
       6.1.2.2. Practical Considerations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-using-bow-features">
     6.1.3. Classification Using BoW Features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-models">
   6.2. Generative Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#background">
     6.2.1. Background
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-supervised-learning-models">
       6.2.1.1. Review of Supervised Learning Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-probabilistic-models">
       6.2.1.2. Review of Probabilistic Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-conditional-maximum-likelihood">
       6.2.1.2. Review of Conditional Maximum Likelihood
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative-models">
     6.2.2. Discriminative vs. Generative Models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discriminative-models">
       6.2.2.1. Discriminative Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       6.2.2.2. Generative Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#predictions-from-generative-models">
       6.2.2.3. Predictions From Generative Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probabilistic-interpretations">
       6.2.2.4. Probabilistic Interpretations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-vs-discriminative-approaches">
     6.3. Generative vs. Discriminative Approaches
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes">
   6.3. Naive Bayes
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-models-for-text-classification">
     6.3.1. Generative Models for Text Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-text-classification">
       6.3.1.1. Review of Text Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-generative-model-for-text-classification">
       6.3.1.2. A Generative Model for Text Classification
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#categorical-distributions">
         Categorical Distributions
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-first-attempt-at-a-generative-model">
     6.3.2. A First Attempt at a Generative Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-problem-high-dimensionality">
       6.3.2.1. A Problem: High Dimensionality
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-naive-bayes-model">
     6.3.3. The Naive Bayes Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-naive-bayes-assumption-machine-learning">
       6.3.3.1. The Naive Bayes Assumption Machine Learning
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#is-naive-bayes-a-good-assumption">
         Is Naive Bayes a Good Assumption?
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prior-distributions-for-naive-bayes">
       6.3.3.2. Prior Distributions for Naive Bayes
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bernoulli-naive-bayes-model">
       6.3.3.3. Bernoulli Naive Bayes Model
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-a-naive-bayes-model">
   6.4. Learning a Naive Bayes Model
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-maximum-likelihood-learning">
     Review: Maximum Likelihood Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-a-bernoulli-naive-bayes-model">
     6.4.1. Learning a Bernoulli Naive Bayes Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-phi">
       6.4.1.1. Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\phi\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-psi-jk">
       6.4.1.2.  Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\psi_{jk}\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-predictions-using-the-model">
     6.4.2. Making Predictions Using the Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#classification-dataset-twenty-newsgroups">
       Classification Dataset: Twenty Newsgroups
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#text-classification-with-naive-bayes">
       6.4.2.1. Text Classification with Naive Bayes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-bernoulli-naive-bayes">
     6.4.3. Algorithm: Bernoulli Naive Bayes
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 6: Generative Models and Naive Bayes</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 6: Generative Models and Naive Bayes
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-classification">
   6.1. Text Classification
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-classification">
     Review: Classification
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     6.1.1. Text Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-the-twenty-newsgroups-classification-dataset">
       6.1.1.1. Example: The Twenty Newsgroups Classification Dataset
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-representations-for-text">
     6.1.2. Feature Representations for Text
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bag-of-words-representations">
       6.1.2.1. Bag of Words Representations
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#practical-considerations">
       6.1.2.2. Practical Considerations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-using-bow-features">
     6.1.3. Classification Using BoW Features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-models">
   6.2. Generative Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#background">
     6.2.1. Background
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-supervised-learning-models">
       6.2.1.1. Review of Supervised Learning Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-probabilistic-models">
       6.2.1.2. Review of Probabilistic Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-conditional-maximum-likelihood">
       6.2.1.2. Review of Conditional Maximum Likelihood
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative-models">
     6.2.2. Discriminative vs. Generative Models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discriminative-models">
       6.2.2.1. Discriminative Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       6.2.2.2. Generative Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#predictions-from-generative-models">
       6.2.2.3. Predictions From Generative Models
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probabilistic-interpretations">
       6.2.2.4. Probabilistic Interpretations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-vs-discriminative-approaches">
     6.3. Generative vs. Discriminative Approaches
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes">
   6.3. Naive Bayes
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-models-for-text-classification">
     6.3.1. Generative Models for Text Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#review-of-text-classification">
       6.3.1.1. Review of Text Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-generative-model-for-text-classification">
       6.3.1.2. A Generative Model for Text Classification
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#categorical-distributions">
         Categorical Distributions
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-first-attempt-at-a-generative-model">
     6.3.2. A First Attempt at a Generative Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-problem-high-dimensionality">
       6.3.2.1. A Problem: High Dimensionality
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-naive-bayes-model">
     6.3.3. The Naive Bayes Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-naive-bayes-assumption-machine-learning">
       6.3.3.1. The Naive Bayes Assumption Machine Learning
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#is-naive-bayes-a-good-assumption">
         Is Naive Bayes a Good Assumption?
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prior-distributions-for-naive-bayes">
       6.3.3.2. Prior Distributions for Naive Bayes
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bernoulli-naive-bayes-model">
       6.3.3.3. Bernoulli Naive Bayes Model
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-a-naive-bayes-model">
   6.4. Learning a Naive Bayes Model
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-maximum-likelihood-learning">
     Review: Maximum Likelihood Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-a-bernoulli-naive-bayes-model">
     6.4.1. Learning a Bernoulli Naive Bayes Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-phi">
       6.4.1.1. Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\phi\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#learning-the-parameters-psi-jk">
       6.4.1.2.  Learning the Parameters
       <span class="math notranslate nohighlight">
        \(\psi_{jk}\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-predictions-using-the-model">
     6.4.2. Making Predictions Using the Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#classification-dataset-twenty-newsgroups">
       Classification Dataset: Twenty Newsgroups
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#text-classification-with-naive-bayes">
       6.4.2.1. Text Classification with Naive Bayes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-bernoulli-naive-bayes">
     6.4.3. Algorithm: Bernoulli Naive Bayes
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><left><img width=25% src="img/cornell_tech2.svg"></left></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-6-generative-models-and-naive-bayes">
<h1>Lecture 6: Generative Models and Naive Bayes<a class="headerlink" href="#lecture-6-generative-models-and-naive-bayes" title="Permalink to this headline">¶</a></h1>
<p>This lecture will introduce a new family of machine learning models called generative models. Our motivating example will be text classification, and we will derive for this task an famous algorithm called Naive Bayes.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="text-classification">
<h1>6.1. Text Classification<a class="headerlink" href="#text-classification" title="Permalink to this headline">¶</a></h1>
<p>We will start with a quick detour to talk about an important application area of machine learning: text classification. Afterwards, we will see how text classification motivates new classification algorithms.</p>
<section id="review-classification">
<h2>Review: Classification<a class="headerlink" href="#review-classification" title="Permalink to this headline">¶</a></h2>
<p>Consider a training dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(n)}, y^{(n)})\}\)</span>.</p>
<p>Recall that we distinguish between two types of supervised learning problems depnding on the targets <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p>
<ol class="simple">
<li><p><strong>Regression</strong>: The target variable <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> is continuous:  <span class="math notranslate nohighlight">\(\mathcal{Y} \subseteq \mathbb{R}\)</span>.</p></li>
<li><p><strong>Classification</strong>: The target variable <span class="math notranslate nohighlight">\(y\)</span> is discrete and takes on one of <span class="math notranslate nohighlight">\(K\)</span> possible values:  <span class="math notranslate nohighlight">\(\mathcal{Y} = \{y_1, y_2, \ldots y_K\}\)</span>. Each discrete value corresponds to a <em>class</em> that we want to predict.</p></li>
</ol>
</section>
<section id="id1">
<h2>6.1.1. Text Classification<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>An interesting instance of a classification problem is classifying text. Text classification includes a lot of applied problems: spam filtering, fraud detection, medical record classification, etc.</p>
<p>A key challenge in text classification is that inputs <span class="math notranslate nohighlight">\(x\)</span> are sequences of words of an arbitrary length. Previously, all our algorithms assumed fixed-length input vectors, and thus they do not directly apply.</p>
<p>Furthermore, the dimensionality of text inputs is usually very large, either proportional to the number of words or to the size of the vocabulary. Hence, we will need to develop specialized algorithms for this task.</p>
<section id="example-the-twenty-newsgroups-classification-dataset">
<h3>6.1.1.1. Example: The Twenty Newsgroups Classification Dataset<a class="headerlink" href="#example-the-twenty-newsgroups-classification-dataset" title="Permalink to this headline">¶</a></h3>
<p>To illustrate the text classification problem, we will use a popular dataset called <code class="docutils literal notranslate"><span class="pre">20-newsgroups</span></code>.</p>
<ul class="simple">
<li><p>It contains ~20,000 documents collected approximately evenly from 20 different online newsgroups.</p></li>
<li><p>Each newgroup covers a different topic such as medicine, computer graphics, or religion.</p></li>
<li><p>This dataset is often used to benchmark text classification and other types of algorithms.</p></li>
</ul>
<p>Let’s load this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</span>
    
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="c1"># for this lecture, we will restrict our attention to just 4 different newsgroups:</span>
<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alt.atheism&#39;</span><span class="p">,</span> <span class="s1">&#39;soc.religion.christian&#39;</span><span class="p">,</span> <span class="s1">&#39;comp.graphics&#39;</span><span class="p">,</span> <span class="s1">&#39;sci.med&#39;</span><span class="p">]</span>

<span class="c1"># load the dataset</span>
<span class="n">twenty_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># print some information on it</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">DESCR</span><span class="p">[:</span><span class="mi">1100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _20newsgroups_dataset:

The 20 newsgroups text dataset
------------------------------

The 20 newsgroups dataset comprises around 18000 newsgroups posts on
20 topics split in two subsets: one for training (or development)
and the other one for testing (or for performance evaluation). The split
between the train and test set is based upon a messages posted before
and after a specific date.

This module contains two loaders. The first one,
:func:`sklearn.datasets.fetch_20newsgroups`,
returns a list of the raw texts that can be fed to text feature
extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`
with custom parameters so as to extract feature vectors.
The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,
returns ready-to-use features, i.e., it is not necessary to use a feature
extractor.

**Data Set Characteristics:**

    =================   ==========
    Classes                     20
    Samples total            18846
    Dimensionality               1
    Features                  text
    =================   ==========

Usage
~~~~~
</pre></div>
</div>
</div>
</div>
<p>This dataset contains posts on online forums focusing on a number of topics: medicine, computer graphics, religion, and others.</p>
<p>We can print some topics contained in the dataset as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The set of targets in this dataset are the newgroup topics:</span>
<span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;sci.med&#39;, &#39;soc.religion.christian&#39;]
</pre></div>
</div>
</div>
</div>
<p>To give us a better idea of what the dataset contains, let’s print one element.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s examine one data point</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>From: s0612596@let.rug.nl (M.M. Zwart)
Subject: catholic church poland
Organization: Faculteit der Letteren, Rijksuniversiteit Groningen, NL
Lines: 10

Hello,

I&#39;m writing a paper on the role of the catholic church in Poland after 1989. 
Can anyone tell me more about this, or fill me in on recent books/articles(
in english, german or french). Most important for me is the role of the 
church concerning the abortion-law, religious education at schools,
birth-control and the relation church-state(government). Thanx,

                                                 Masja,
&quot;M.M.Zwart&quot;&lt;s0612596@let.rug.nl&gt;
</pre></div>
</div>
</div>
</div>
<p>This is a text document about religion. Our goal will be to predict the topic of each text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We have about 2k data points in total</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2257
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="feature-representations-for-text">
<h2>6.1.2. Feature Representations for Text<a class="headerlink" href="#feature-representations-for-text" title="Permalink to this headline">¶</a></h2>
<p>Each data point <span class="math notranslate nohighlight">\(x\)</span> in this dataset is a squence of characters of an arbitrary length.
How do we transform these into <span class="math notranslate nohighlight">\(d\)</span>-dimensional features <span class="math notranslate nohighlight">\(\phi(x)\)</span> that can be used with our machine learning algorithms?</p>
<ul class="simple">
<li><p>We may devise hand-crafted features by inspecting the data:</p>
<ul>
<li><p>Does the message contain the word “church”? Does the email of the user originate outside the United States? Is the organization a university? etc.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>We can count the number of occurrences of each word:</p>
<ul>
<li><p>Does this message contain “Aardvark”, yes or no?</p></li>
<li><p>Does this message contain “Apple”, yes or no?</p></li>
<li><p>… Does this message contain “Zebra”, yes or no?</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Finally, many modern deep learning methods can directly work with sequences of characters of an arbitrary length.</p></li>
</ul>
<section id="bag-of-words-representations">
<h3>6.1.2.1. Bag of Words Representations<a class="headerlink" href="#bag-of-words-representations" title="Permalink to this headline">¶</a></h3>
<p>Perhaps the most widely used approach to representing text documents is called “bag of words”.</p>
<p>We start by defining a vocabulary <span class="math notranslate nohighlight">\(V\)</span> containing all the possible words we are interested in, e.g.:</p>
<div class="math notranslate nohighlight">
\[ 
V = \{\text{church}, \text{doctor}, \text{fervently}, \text{purple}, \text{slow}, ...\} 
\]</div>
<p>A bag of words representation of a document <span class="math notranslate nohighlight">\(x\)</span> is a function <span class="math notranslate nohighlight">\(\phi(x) \to \{0,1\}^{|V|}\)</span> that outputs a feature vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\phi(x) = \left( 
\begin{array}{c}
0 \\
1 \\
0 \\
\vdots \\
0 \\
\vdots \\
\end{array}
\right)
\begin{array}{l}
\;\text{church} \\
\;\text{doctor} \\
\;\text{fervently} \\
\\
\;\text{purple} \\
\\
\end{array}
\end{split}\]</div>
<p>of dimension <span class="math notranslate nohighlight">\(V\)</span>. The <span class="math notranslate nohighlight">\(j\)</span>-th component <span class="math notranslate nohighlight">\(\phi(x)_j\)</span> equals <span class="math notranslate nohighlight">\(1\)</span> if <span class="math notranslate nohighlight">\(x\)</span> convains the <span class="math notranslate nohighlight">\(j\)</span>-th word in <span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>Let’s see an example of this approach on <code class="docutils literal notranslate"><span class="pre">20-newsgroups</span></code>.</p>
<p>We start by computing these features using the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># vectorize the training set</span>
<span class="n">count_vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2257, 35788)
</pre></div>
</div>
</div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, we can retrieve the index of <span class="math notranslate nohighlight">\(\phi(x)\)</span> associated with each <code class="docutils literal notranslate"><span class="pre">word</span></code> using the expression <code class="docutils literal notranslate"><span class="pre">count_vect.vocabulary_.get(word)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The CountVectorizer class records the index j associated with each word in V</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Index for the word &quot;church&quot;: &#39;</span><span class="p">,</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;church&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Index for the word &quot;computer&quot;: &#39;</span><span class="p">,</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;computer&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index for the word &quot;church&quot;:  8609
Index for the word &quot;computer&quot;:  9338
</pre></div>
</div>
</div>
</div>
<p>Our featurized dataset is in the matrix <code class="docutils literal notranslate"><span class="pre">X_train</span></code>. We can use the above indices to retrieve the 0-1 value that has been computed for each word:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can examine if any of these words are present in our previous datapoint</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

<span class="c1"># let&#39;s see if it contains these two words?</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---&#39;</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Value at the index for the word &quot;church&quot;: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;church&#39;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Value at the index for the word &quot;computer&quot;: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;computer&#39;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Value at the index for the word &quot;doctor&quot;: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;doctor&#39;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Value at the index for the word &quot;important&quot;: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;important&#39;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>From: s0612596@let.rug.nl (M.M. Zwart)
Subject: catholic church poland
Organization: Faculteit der Letteren, Rijksuniversiteit Groningen, NL
Lines: 10

Hello,

I&#39;m writing a paper on the role of the catholic church in Poland after 1989. 
Can anyone tell me more about this, or fill me in on recent books/articles(
in english, german or french). Most important for me is the role of the 
church concerning the abortion-law, religious education at schools,
birth-control and the relation church-state(government). Thanx,

                                                 Masja,
&quot;M.M.Zwart&quot;&lt;s0612596@let.rug.nl&gt;

------------------------------------------------------------
Value at the index for the word &quot;church&quot;:  1
Value at the index for the word &quot;computer&quot;:  0
Value at the index for the word &quot;doctor&quot;:  0
Value at the index for the word &quot;important&quot;:  1
</pre></div>
</div>
</div>
</div>
</section>
<section id="practical-considerations">
<h3>6.1.2.2. Practical Considerations<a class="headerlink" href="#practical-considerations" title="Permalink to this headline">¶</a></h3>
<p>In practice, we may use some additional modifications of this techinque:</p>
<ul class="simple">
<li><p>Sometimes, the feature <span class="math notranslate nohighlight">\(\phi(x)_j\)</span> for the <span class="math notranslate nohighlight">\(j\)</span>-th word holds the count of occurrences of word <span class="math notranslate nohighlight">\(j\)</span> instead of just the binary occurrence.</p></li>
</ul>
<ul class="simple">
<li><p>The raw text is usually preprocessed. One common technique is <em>stemming</em>, in which we only keep the root of the word.</p>
<ul>
<li><p>e.g. “slowly”, “slowness”, both map to “slow”</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Filtering for common <em>stopwords</em> such as “the”, “a”, “and”. Similarly, rare words are also typically excluded.</p></li>
</ul>
</section>
</section>
<section id="classification-using-bow-features">
<h2>6.1.3. Classification Using BoW Features<a class="headerlink" href="#classification-using-bow-features" title="Permalink to this headline">¶</a></h2>
<p>Let’s now have a look at the performance of classification over bag of words features.
Now that we have a feature representation <span class="math notranslate nohighlight">\(\phi(x)\)</span>, we can apply the classifier of our choice, such as logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Create an instance of Softmax and fit the data.</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LogisticRegression(C=100000.0, multi_class=&#39;multinomial&#39;, verbose=True)
</pre></div>
</div>
</div>
</div>
<p>And now we can use this model for predicting on new inputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_new</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;God is love&#39;</span><span class="p">,</span> <span class="s1">&#39;OpenGL on the GPU is fast&#39;</span><span class="p">]</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs_new</span><span class="p">)</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs_new</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%r</span><span class="s1"> =&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">category</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;God is love&#39; =&gt; soc.religion.christian
&#39;OpenGL on the GPU is fast&#39; =&gt; comp.graphics
</pre></div>
</div>
</div>
</div>
<p>The results look good!</p>
<p>Inn summary, classifying text normally requires specifiyng features over the raw data. A widely used representation is “bag of words”, in which features are occurrences or counts of words. Once text is featurized, any off-the-shelf supervised learning algorithm can be applied, but some work better than others, as we will see next.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="generative-models">
<h1>6.2. Generative Models<a class="headerlink" href="#generative-models" title="Permalink to this headline">¶</a></h1>
<p>In this lecture, we are going to look at generative algorithms and their applications to text classification.</p>
<p>We will start by defining the concept of a generative <em>model</em>.</p>
<section id="background">
<h2>6.2.1. Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<section id="review-of-supervised-learning-models">
<h3>6.2.1.1. Review of Supervised Learning Models<a class="headerlink" href="#review-of-supervised-learning-models" title="Permalink to this headline">¶</a></h3>
<p>A supervised learning model is a function</p>
<div class="math notranslate nohighlight">
\[ 
f_\theta : \mathcal{X} \to \mathcal{Y} 
\]</div>
<p>that maps inputs <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span> to targets <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span>.</p>
<p>Models have <em>parameters</em> <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> living in a set <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
<p>For example, logistic regression is a binary classification algorithm which uses a model</p>
<div class="math notranslate nohighlight">
\[
f_\theta : \mathcal{X} \to [0,1]
\]</div>
<p>of the form</p>
<div class="math notranslate nohighlight">
\[ 
f_\theta(x) = \sigma(\theta^\top x) = \frac{1}{1 + \exp(-\theta^\top x)}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + \exp(-z)}\)</span> is the <em>sigmoid</em> or <em>logistic</em> function.</p>
</section>
<section id="review-of-probabilistic-models">
<h3>6.2.1.2. Review of Probabilistic Models<a class="headerlink" href="#review-of-probabilistic-models" title="Permalink to this headline">¶</a></h3>
<p>Many supervised learning models have a probabilistic interpretation.
Often a model <span class="math notranslate nohighlight">\(f_\theta\)</span> defines a probability distribution of the form</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
P_\theta(x,y) : \mathcal{X} \times \mathcal{Y} \to [0,1] &amp;&amp; \text{or} &amp;&amp; P_\theta(y|x) : \mathcal{X} \times \mathcal{Y} \to [0,1].
\end{align*}
\]</div>
<p>We refer to these as <em>probabilistic models</em>.</p>
<p>For example, our logistic model defines (“parameterizes”) a probability distribution <span class="math notranslate nohighlight">\(P_\theta(y|x) : \mathcal{X} \times \mathcal{Y} \to [0,1]\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P_\theta(y=1 | x) &amp; = \sigma(\theta^\top x) \\
P_\theta(y=0 | x) &amp; = 1-\sigma(\theta^\top x).
\end{align*}
\end{split}\]</div>
</section>
<section id="review-of-conditional-maximum-likelihood">
<h3>6.2.1.2. Review of Conditional Maximum Likelihood<a class="headerlink" href="#review-of-conditional-maximum-likelihood" title="Permalink to this headline">¶</a></h3>
<p>When a machine learning model defines a <em>conditional</em> model <span class="math notranslate nohighlight">\(P_\theta(y | x)\)</span>, we can maximize the <em>conditional maximum likelihood</em>:</p>
<div class="math notranslate nohighlight">
\[ \max_\theta \frac{1}{n}\sum_{i=1}^n \log P_\theta(y^{(i)}|{x}^{(i)}). \]</div>
<p>This says that we should choose parameters <span class="math notranslate nohighlight">\(\theta\)</span> such that for each input <span class="math notranslate nohighlight">\(x^{(i)}\)</span> in the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, <span class="math notranslate nohighlight">\(P_\theta\)</span> assigns a high probability to the correct target <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p>
<p>In the logistic regression example, we optimize the following objective defined over a binary classification dataset  <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(n)}, y^{(n)})\}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\ell(\theta) &amp; = \frac{1}{n}\sum_{i=1}^n \log P_\theta (y^{(i)} \mid x^{(i)}) \\
&amp; = \frac{1}{n}\sum_{i=1}^n {y^{(i)}} \cdot \log \sigma(\theta^\top x^{(i)}) + (1-y^{(i)}) \cdot \log (1-\sigma(\theta^\top x^{(i)})).
\end{align*}
\end{split}\]</div>
<p>This objective is also often called the log-loss, or cross-entropy.</p>
<p>This asks the model to ouput a large score <span class="math notranslate nohighlight">\(\sigma(\theta^\top x^{(i)})\)</span> (a score that’s close to one) if <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span>, and a score that’s small (close to zero) if <span class="math notranslate nohighlight">\(y^{(i)}=0\)</span>.</p>
</section>
</section>
<section id="discriminative-vs-generative-models">
<h2>6.2.2. Discriminative vs. Generative Models<a class="headerlink" href="#discriminative-vs-generative-models" title="Permalink to this headline">¶</a></h2>
<p>Most machine learning models can be understood as being either <em>discriminative</em> or <em>generative</em>.</p>
<section id="discriminative-models">
<h3>6.2.2.1. Discriminative Models<a class="headerlink" href="#discriminative-models" title="Permalink to this headline">¶</a></h3>
<p>Every supervised learning algorithm we have seen so far has relied on discriminative models.
For example, logistic regression is an example of a discriminative machine learning model because :</p>
<ul class="simple">
<li><p>It directly transforms <span class="math notranslate nohighlight">\(x\)</span> into a score for each class <span class="math notranslate nohighlight">\(y\)</span> (e.g., via the formula <span class="math notranslate nohighlight">\(y=\sigma(\theta^\top x)\)</span>)</p></li>
<li><p>It can be interpreted as defining a <em>conditional</em> probability <span class="math notranslate nohighlight">\(P_\theta(y|x)\)</span></p></li>
</ul>
<p>Discriminative models are one that simply take an input <span class="math notranslate nohighlight">\(x\)</span> and output a <span class="math notranslate nohighlight">\(y\)</span> or a probability over <span class="math notranslate nohighlight">\(y\)</span>. The <span class="math notranslate nohighlight">\(x\)</span> is strictly an input that is never modeled in the same way as <span class="math notranslate nohighlight">\(y\)</span>.</p>
</section>
<section id="id2">
<h3>6.2.2.2. Generative Models<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Another approach to classification is to use <em>generative</em> models.</p>
<ul class="simple">
<li><p>A generative approach first builds a model of <span class="math notranslate nohighlight">\(x\)</span> for each class:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
P_\theta(x | y=k) \; \text{for each class $k$}.
\]</div>
<p><span class="math notranslate nohighlight">\(P_\theta(x | y=k)\)</span> <em>scores</em> each <span class="math notranslate nohighlight">\(x\)</span> according to how well it matches class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<ul class="simple">
<li><p>A class probability <span class="math notranslate nohighlight">\(P_\theta(y=k)\)</span> encoding our prior beliefs</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
P_\theta(y=k) \; \text{for each class $k$}.
\]</div>
<p>These are often just the % of each class in the data.</p>
<p>In the context of spam classification, we would fit two models on a corpus of emails <span class="math notranslate nohighlight">\(x\)</span> with spam/non-spam labels <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
P_\theta(x|y=\text{0}) &amp;&amp; \text{and} &amp;&amp; P_\theta(x|y=\text{1})
\end{align*}
\]</div>
<p>as well as define priors <span class="math notranslate nohighlight">\(P_\theta(y=\text{0}), P_\theta(y=\text{1})\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P_\theta(x | y=1)\)</span> <em>scores</em> each <span class="math notranslate nohighlight">\(x\)</span> based on how much it looks like spam.</p></li>
<li><p><span class="math notranslate nohighlight">\(P_\theta(x | y=0)\)</span> <em>scores</em> each <span class="math notranslate nohighlight">\(x\)</span> based on how much it looks like non-spam.</p></li>
</ul>
</section>
<section id="predictions-from-generative-models">
<h3>6.2.2.3. Predictions From Generative Models<a class="headerlink" href="#predictions-from-generative-models" title="Permalink to this headline">¶</a></h3>
<p>Given a new <span class="math notranslate nohighlight">\(x'\)</span>, we return the class that is the most likely to have generated it:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\arg \max_k P_\theta(y=k | x') &amp; = \arg \max_k  \frac{P_\theta(x' | y=k) P_\theta(y=k)}{P_\theta(x')} \\
&amp; = \arg \max_k P_\theta(x' | y=k) P_\theta(y=k),
\end{align*}
\end{split}\]</div>
<p>where we have applied Bayes’ rule in the first line.</p>
<p>In the context of spam classification, given a new <span class="math notranslate nohighlight">\(x'\)</span>, we would compare the probabilities of both models:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
P_\theta(x'|y=\text{0})P_\theta(y=\text{0}) &amp;&amp; \text{vs.} &amp;&amp; P_\theta(x'|y=\text{1})P_\theta(y=\text{1})
\end{align*}
\]</div>
<p>We output the class that’s more likely to have generated <span class="math notranslate nohighlight">\(x'\)</span>.</p>
</section>
<section id="probabilistic-interpretations">
<h3>6.2.2.4. Probabilistic Interpretations<a class="headerlink" href="#probabilistic-interpretations" title="Permalink to this headline">¶</a></h3>
<p>A <em>generative</em> model defines <span class="math notranslate nohighlight">\(P_\theta(x|y)\)</span> and <span class="math notranslate nohighlight">\(P_\theta(y)\)</span>, thus it also defines a distribution of the form <span class="math notranslate nohighlight">\(P_\theta(x,y)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\underbrace{P_\theta(x,y) : \mathcal{X} \times \mathcal{Y} \to [0,1]}_\text{generative model} &amp; \;\; &amp; \underbrace{P_\theta(y|x) : \mathcal{X} \times \mathcal{Y} \to [0,1]}_\text{discriminative model}
\end{align*}
\]</div>
<p>Discriminative models don’t define any probability over the <span class="math notranslate nohighlight">\(x\)</span>’s. Generative models do.</p>
<p>We can learn a generative model <span class="math notranslate nohighlight">\(P_\theta(x, y)\)</span> by maximizing the <em>likelihood</em>:</p>
<div class="math notranslate nohighlight">
\[ 
\max_\theta \frac{1}{n}\sum_{i=1}^n \log P_\theta({x}^{(i)}, y^{(i)}). 
\]</div>
<p>This says that we should choose parameters <span class="math notranslate nohighlight">\(\theta\)</span> such that the model <span class="math notranslate nohighlight">\(P_\theta\)</span> assigns a high probability to each training example <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span> in the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</section>
</section>
<section id="generative-vs-discriminative-approaches">
<h2>6.3. Generative vs. Discriminative Approaches<a class="headerlink" href="#generative-vs-discriminative-approaches" title="Permalink to this headline">¶</a></h2>
<p>What are the pros and cons of generative and discirminative methods?</p>
<ul class="simple">
<li><p>If we only care about prediction, we don’t need a model of <span class="math notranslate nohighlight">\(P(x)\)</span>. It’s simpler to only model <span class="math notranslate nohighlight">\(P(y|x)\)</span> (what we care about).</p>
<ul>
<li><p>In practice, discriminative models are often be more accurate.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>If we care about other tasks (generation, dealing with missing values, etc.) or if we know the true model is generative, we want to use the generative approach.</p></li>
</ul>
<p>More on this later!</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="naive-bayes">
<h1>6.3. Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h1>
<p>Next, we are going to define Naive Bayes, a famous generative classification algorithm.
We will then apply Naive Bayes to the text classification problem.</p>
<section id="generative-models-for-text-classification">
<h2>6.3.1. Generative Models for Text Classification<a class="headerlink" href="#generative-models-for-text-classification" title="Permalink to this headline">¶</a></h2>
<section id="review-of-text-classification">
<h3>6.3.1.1. Review of Text Classification<a class="headerlink" href="#review-of-text-classification" title="Permalink to this headline">¶</a></h3>
<p>An common type of classification problem is classifying text.</p>
<ul class="simple">
<li><p>Includes a lot applied problems: spam filtering, fraud detection, medical record classification, etc.</p></li>
<li><p>Inputs <span class="math notranslate nohighlight">\(x\)</span> are sequences of words of an arbitrary length.</p></li>
<li><p>The dimensionality of text inputs is usually very large, proportional to the size of the vocabulary.</p></li>
</ul>
</section>
<section id="a-generative-model-for-text-classification">
<h3>6.3.1.2. A Generative Model for Text Classification<a class="headerlink" href="#a-generative-model-for-text-classification" title="Permalink to this headline">¶</a></h3>
<p>In binary text classification, we fit two models on a labeled corpus:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
P_\theta(x|y=\text{0}) &amp;&amp; \text{and} &amp;&amp; P_\theta(x|y=\text{1})
\end{align*}
\]</div>
<p>We also define priors <span class="math notranslate nohighlight">\(P_\theta(y=\text{0}), P_\theta(y=\text{1})\)</span>.</p>
<p>Each model <span class="math notranslate nohighlight">\(P_\theta(x | y=k)\)</span> <em>scores</em> <span class="math notranslate nohighlight">\(x\)</span> based on how much it looks like class <span class="math notranslate nohighlight">\(k\)</span>.
The documents <span class="math notranslate nohighlight">\(x\)</span> are in <strong>bag-of-words</strong> representation.</p>
<p>How do we choose <span class="math notranslate nohighlight">\(P_\theta(x|y=k)\)</span>?</p>
<section id="categorical-distributions">
<h4>Categorical Distributions<a class="headerlink" href="#categorical-distributions" title="Permalink to this headline">¶</a></h4>
<p>We will define <span class="math notranslate nohighlight">\(P_\theta(x|y=k)\)</span> using a standard tool from probability—the Categorial distribution. A <a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical</a> distribution with parameters <span class="math notranslate nohighlight">\(\theta\)</span> is a probability
over <span class="math notranslate nohighlight">\(K\)</span> discrete outcomes <span class="math notranslate nohighlight">\(x \in \{1,2,...,K\}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P_\theta(x = j) = \theta_j.
\]</div>
<p>When <span class="math notranslate nohighlight">\(K=2\)</span> this is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a>.</p>
</section>
</section>
</section>
<section id="a-first-attempt-at-a-generative-model">
<h2>6.3.2. A First Attempt at a Generative Model<a class="headerlink" href="#a-first-attempt-at-a-generative-model" title="Permalink to this headline">¶</a></h2>
<p>Note there is a finite number of <span class="math notranslate nohighlight">\(x\)</span>’s: each is a binary vector of size <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>A first solution is to assume that <span class="math notranslate nohighlight">\(P(x|y=k)\)</span> is a categorical distribution that assigns a probability to each possible state of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(x|y=k) = P_k \left( 
\begin{array}{c}
0 \\
1 \\
0 \\
\vdots \\
0 
\end{array}
\right.
\left.
\begin{array}{l}
\;\text{church} \\
\;\text{doctor} \\
\;\text{fervently} \\
\vdots \\
\;\text{purple}
\end{array}
\right) = \theta_{xk} = 0.0012
\end{split}\]</div>
<p>The <span class="math notranslate nohighlight">\(\theta_{xk}\)</span> is the probability of <span class="math notranslate nohighlight">\(x\)</span> under class <span class="math notranslate nohighlight">\(k\)</span>. We want to learn these.</p>
<section id="a-problem-high-dimensionality">
<h3>6.3.2.1. A Problem: High Dimensionality<a class="headerlink" href="#a-problem-high-dimensionality" title="Permalink to this headline">¶</a></h3>
<p>How many parameters does a Categorical model <span class="math notranslate nohighlight">\(P(x|y=k)\)</span> have?</p>
<ul class="simple">
<li><p>If the dimensionality <span class="math notranslate nohighlight">\(d\)</span> of <span class="math notranslate nohighlight">\(x\)</span> is high (e.g., vocabulary has size 10,000), <span class="math notranslate nohighlight">\(x\)</span> can take a huge number of values (<span class="math notranslate nohighlight">\(2^{10000}\)</span> in our example)</p></li>
</ul>
<ul class="simple">
<li><p>We need to specify <span class="math notranslate nohighlight">\(2^{d}-1\)</span> parameters for the Categorical distribution.</p></li>
</ul>
<p>For comparison, there are <span class="math notranslate nohighlight">\(\approx 10^{82}\)</span> atoms in the universe.</p>
</section>
</section>
<section id="the-naive-bayes-model">
<h2>6.3.3. The Naive Bayes Model<a class="headerlink" href="#the-naive-bayes-model" title="Permalink to this headline">¶</a></h2>
<p>To deal with high-dimensional <span class="math notranslate nohighlight">\(x\)</span>, we choose a simpler model for <span class="math notranslate nohighlight">\(P_\theta(y|x)\)</span>:</p>
<ol class="simple">
<li><p>We define a (Bernoulli) model with one parameter <span class="math notranslate nohighlight">\(\psi_{jk} \in [0,1]\)</span> for the occurrence of each word <span class="math notranslate nohighlight">\(j\)</span> in class <span class="math notranslate nohighlight">\(k\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P_\theta(x_j = 1 \mid y=k) = \psi_{jk}
\]</div>
<p><span class="math notranslate nohighlight">\(\psi_{jk}\)</span> is the probability that a document of class <span class="math notranslate nohighlight">\(k\)</span> contains word <span class="math notranslate nohighlight">\(j\)</span>.</p>
<ol class="simple">
<li><p>We define the model <span class="math notranslate nohighlight">\(P_\theta(x|y=k)\)</span> for documents <span class="math notranslate nohighlight">\(x\)</span> as the product of the occurrence probabilities of each of its words <span class="math notranslate nohighlight">\(x_j\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ 
P_\theta(x|y=k) = \prod_{j=1}^d P_\theta(x_j \mid y=k) 
\]</div>
<p>How many parameters does this new model have?</p>
<ul class="simple">
<li><p>We have a distribution <span class="math notranslate nohighlight">\(P_\theta(x_j = 1 \mid y=k)\)</span> for each word <span class="math notranslate nohighlight">\(j\)</span> and each distribution has one parameter <span class="math notranslate nohighlight">\(\psi_{jk}\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>The distribution <span class="math notranslate nohighlight">\(P_\theta(x|y=k) = \prod_{j=1}^d P_\theta(x_j \mid y=k)\)</span> is the product of <span class="math notranslate nohighlight">\(d\)</span> such one-parameter distributions.</p></li>
</ul>
<ul class="simple">
<li><p>We have <span class="math notranslate nohighlight">\(K\)</span> distributions of the form <span class="math notranslate nohighlight">\(P_\theta(x|y=k)\)</span>.</p></li>
</ul>
<p>Thus, we only need <span class="math notranslate nohighlight">\(Kd\)</span> parameters instead of <span class="math notranslate nohighlight">\(K(2^d-1)\)</span>!</p>
<section id="the-naive-bayes-assumption-machine-learning">
<h3>6.3.3.1. The Naive Bayes Assumption Machine Learning<a class="headerlink" href="#the-naive-bayes-assumption-machine-learning" title="Permalink to this headline">¶</a></h3>
<p>The Naive Bayes assumption is a <strong>general technique</strong> that can be used with any <span class="math notranslate nohighlight">\(d\)</span>-dimensional <span class="math notranslate nohighlight">\(x\)</span>.</p>
<ul class="simple">
<li><p>We simplify the model for <span class="math notranslate nohighlight">\(x\)</span> as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
P(x|y) = \prod_{j=1}^d P(x_j \mid y) 
\]</div>
<ul class="simple">
<li><p>We choose a simple distribution family for <span class="math notranslate nohighlight">\(P(x_j \mid y)\)</span>.</p></li>
</ul>
<p>This typcally makes the number of parameters linear instead of exponential in <span class="math notranslate nohighlight">\(d\)</span>.</p>
<section id="is-naive-bayes-a-good-assumption">
<h4>Is Naive Bayes a Good Assumption?<a class="headerlink" href="#is-naive-bayes-a-good-assumption" title="Permalink to this headline">¶</a></h4>
<p>Naive Bayes assumes that words are uncorrelated, but in reality they are.</p>
<ul class="simple">
<li><p>If spam email contains “bank”, it probably contains “account”</p></li>
</ul>
<p>As a result, the probabilities estimated by Naive Bayes can be over- under under-confident.</p>
<p>In practice, however, Naive Bayes is a very useful assumption that gives very good classification accuracy!</p>
</section>
</section>
<section id="prior-distributions-for-naive-bayes">
<h3>6.3.3.2. Prior Distributions for Naive Bayes<a class="headerlink" href="#prior-distributions-for-naive-bayes" title="Permalink to this headline">¶</a></h3>
<p>To obtain a full generative model, we still need to define the distribution <span class="math notranslate nohighlight">\(P_\theta(y=k)\)</span>.</p>
<ul class="simple">
<li><p>This encodes our prior belief about <span class="math notranslate nohighlight">\(y\)</span> before we see <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>It can also be learned from data.</p></li>
</ul>
<p>Since we have a small number of classes <span class="math notranslate nohighlight">\(K\)</span>, we may use a Categorical distribution with parameters <span class="math notranslate nohighlight">\(\vec\phi = (\phi_1,...,\phi_K)\)</span> and learn <span class="math notranslate nohighlight">\(\vec\phi\)</span> from data:</p>
<div class="math notranslate nohighlight">
\[ 
P_\theta(y=k) = \phi_k.
\]</div>
</section>
<section id="bernoulli-naive-bayes-model">
<h3>6.3.3.3. Bernoulli Naive Bayes Model<a class="headerlink" href="#bernoulli-naive-bayes-model" title="Permalink to this headline">¶</a></h3>
<p>The <em>Bernoulli Naive Bayes</em> model <span class="math notranslate nohighlight">\(P_\theta(x,y)\)</span> is defined for <em>binary data</em> <span class="math notranslate nohighlight">\(x \in \{0,1\}^d\)</span> (e.g., bag-of-words documents).</p>
<p>The <span class="math notranslate nohighlight">\(\theta\)</span> contains prior parameters <span class="math notranslate nohighlight">\(\vec\phi = (\phi_1,...,\phi_K)\)</span> and <span class="math notranslate nohighlight">\(K\)</span> sets of per-class parameters <span class="math notranslate nohighlight">\(\psi_k = (\psi_{1k},...,\psi_{dk})\)</span>.</p>
<p>The probability of the data <span class="math notranslate nohighlight">\(x\)</span> for each class equals</p>
<div class="math notranslate nohighlight">
\[
P_\theta(x|y=k) = \prod_{j=1}^d P(x_j \mid y=k),
\]</div>
<p>where each <span class="math notranslate nohighlight">\(P_\theta(x_j \mid y=k)\)</span> is a <span class="math notranslate nohighlight">\(\text{Bernoullli}(\psi_{jk})\)</span>.</p>
<p>The probability over <span class="math notranslate nohighlight">\(y\)</span> is Categorical:
<span class="math notranslate nohighlight">\(P_\theta(y=k) = \phi_k\)</span>.</p>
<p>Formally, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P_\theta(y) &amp; = \text{Categorical}(\phi_1,\phi_2,\ldots,\phi_K) \\
P_\theta(x_j=1|y=k) &amp; = \text{Bernoullli}(\psi_{jk}) \\
P_\theta(x|y=k) &amp; = \prod_{j=1}^d P_\theta(x_j|y=k)
\end{align*}
\end{split}\]</div>
<p>The parameters of the model are <span class="math notranslate nohighlight">\(\theta = (\phi_1,...,\phi_K, \psi_{11}, ...,\psi_{dK})\)</span>.
There are exactly <span class="math notranslate nohighlight">\(K(d+1)\)</span> parameters.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="learning-a-naive-bayes-model">
<h1>6.4. Learning a Naive Bayes Model<a class="headerlink" href="#learning-a-naive-bayes-model" title="Permalink to this headline">¶</a></h1>
<p>We will now turn our attention to learnig the parameters of the Naive Bayes model and using them to make predictions.</p>
<section id="review-maximum-likelihood-learning">
<h2>Review: Maximum Likelihood Learning<a class="headerlink" href="#review-maximum-likelihood-learning" title="Permalink to this headline">¶</a></h2>
<p>Recall that we can learn a generative model <span class="math notranslate nohighlight">\(P_\theta(x, y)\)</span> by maximizing the <em>maximum likelihood</em>:</p>
<div class="math notranslate nohighlight">
\[ 
\max_\theta \frac{1}{n}\sum_{i=1}^n \log P_\theta({x}^{(i)}, y^{(i)}). 
\]</div>
<p>This seeks to find parameters <span class="math notranslate nohighlight">\(\theta\)</span> such that the model assigns high probability to the training data.</p>
<p>We will use maximum likelihood to fit the Bernoulli Naive Bayes model. Note that model parameterss <span class="math notranslate nohighlight">\(\theta\)</span> are the union of the parameters of each sub-model:</p>
<div class="math notranslate nohighlight">
\[
\theta = (\phi_1, \phi_2,\ldots, \phi_K, \psi_{11}, \psi_{21}, \ldots, \psi_{dK}).
\]</div>
</section>
<section id="learning-a-bernoulli-naive-bayes-model">
<h2>6.4.1. Learning a Bernoulli Naive Bayes Model<a class="headerlink" href="#learning-a-bernoulli-naive-bayes-model" title="Permalink to this headline">¶</a></h2>
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x^{(i)}, y^{(i)})\mid i=1,2,\ldots,n\}\)</span>, we want to optimize the log-likelihood <span class="math notranslate nohighlight">\(\ell(\theta) = \log L(\theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\ell &amp; = \sum_{i=1}^n \log P_\theta(x^{(i)}, y^{(i)}) = \sum_{i=1}^n \sum_{j=1}^d \log P_\theta(x^{(i)}_j | y^{(i)}) + \sum_{i=1}^n \log P_\theta(y^{(i)}) \\
&amp; = \sum_{k=1}^K \sum_{j=1}^d \underbrace{\sum_{i :y^{(i)} =k} \log P(x^{(i)}_j | y^{(i)} ; \psi_{jk})}_\text{all the terms that involve $\psi_{jk}$} + \underbrace{\sum_{i=1}^n \log P(y^{(i)} ; \vec \phi)}_\text{all the terms that involve $\vec \phi$}.
\end{align*}
\end{split}\]</div>
<p>In equality #2, we use Naive Bayes: <span class="math notranslate nohighlight">\(P_\theta(x,y)=P_\theta(y) \prod_{i=1}^d P(x_j|y)\)</span>; in the third one, we change the order of summation.</p>
<p>Each <span class="math notranslate nohighlight">\(\psi_{jk}\)</span> for <span class="math notranslate nohighlight">\(k=1,2,\ldots,K\)</span> is found in only the following terms:</p>
<div class="math notranslate nohighlight">
\[
 \max_{\psi_{jk}} \ell(\theta) = \max_{\psi_{jk}} \sum_{i :y^{(i)} =k} \log P(x^{(i)}_j | y^{(i)} ; \psi_{jk}). 
\]</div>
<p>Thus, optimization over <span class="math notranslate nohighlight">\(\psi_{jk}\)</span> can be carried out independently of all the other parameters by just looking at these terms.</p>
<p>Similarly, optimizing for <span class="math notranslate nohighlight">\(\vec \phi = (\phi_1, \phi_2, \ldots, \phi_K)\)</span> only involves a few terms:</p>
<div class="math notranslate nohighlight">
\[ 
\max_{\vec \phi} \sum_{i=1}^n \log P_\theta(x^{(i)}, y^{(i)} ; \theta) = \max_{\vec\phi} \sum_{i=1}^n  \log P_\theta(y^{(i)} ; \vec \phi). 
\]</div>
<section id="learning-the-parameters-phi">
<h3>6.4.1.1. Learning the Parameters <span class="math notranslate nohighlight">\(\phi\)</span><a class="headerlink" href="#learning-the-parameters-phi" title="Permalink to this headline">¶</a></h3>
<p>Let’s first consider the optimization over <span class="math notranslate nohighlight">\(\vec \phi = (\phi_1, \phi_2, \ldots, \phi_K)\)</span>.</p>
<div class="math notranslate nohighlight">
\[ 
\max_{\vec \phi} \sum_{i=1}^n  \log P_\theta(y=y^{(i)} ; \vec \phi). 
\]</div>
<ul class="simple">
<li><p>We have <span class="math notranslate nohighlight">\(n\)</span> datapoints, each having one of <span class="math notranslate nohighlight">\(K\)</span> classes</p></li>
<li><p>We want to learn the most likely class probabilities <span class="math notranslate nohighlight">\(\phi_k\)</span> that generated this data</p></li>
</ul>
<p>What is the maximum likelihood <span class="math notranslate nohighlight">\(\phi\)</span> in this case?</p>
<p>Inuitively, the maximum likelihood class probabilities <span class="math notranslate nohighlight">\(\phi\)</span> should just be the class proportions that we see in the data.</p>
<p>Let’s calculate this formally. Our objective <span class="math notranslate nohighlight">\(J(\vec \phi)\)</span> equals</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
J(\vec\phi) &amp; = \sum_{i=1}^n  \log P_\theta(y^{(i)} ; \vec \phi) = \sum_{i=1}^n  \log \left( \frac{\phi_{y^{(i)}}}{\sum_{k=1}^K \phi_k}\right) \\
&amp; = \sum_{i=1}^n \log \phi_{y^{(i)}} - n \cdot \log \sum_{k=1}^K \phi_k \\ 
&amp; = \sum_{k=1}^K \sum_{i : y^{(i)} = k} \log \phi_k - n \cdot \log \sum_{k=1}^K \phi_k
\end{align*}
\end{split}\]</div>
<p>Taking the derivative and setting it to zero, we obtain</p>
<div class="math notranslate nohighlight">
\[
\frac{\phi_k}{\sum_l \phi_l} = \frac{n_k}{n}
\]</div>
<p>for each <span class="math notranslate nohighlight">\(k\)</span>, where <span class="math notranslate nohighlight">\(n_k = |\{i : y^{(i)} = k\}|\)</span> is the number of training targets with class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Thus, the optimal <span class="math notranslate nohighlight">\(\phi_k\)</span> is just the proportion of data points with class <span class="math notranslate nohighlight">\(k\)</span> in the training set!</p>
</section>
<section id="learning-the-parameters-psi-jk">
<h3>6.4.1.2.  Learning the Parameters <span class="math notranslate nohighlight">\(\psi_{jk}\)</span><a class="headerlink" href="#learning-the-parameters-psi-jk" title="Permalink to this headline">¶</a></h3>
<p>Next, let’s look at the maximum likelihood term</p>
<div class="math notranslate nohighlight">
\[ 
\arg\max_{\psi_{jk}} \sum_{i :y^{(i)} =k} \log P(x^{(i)}_j | y^{(i)} ; \psi_{jk}). 
\]</div>
<p>over the word parameters <span class="math notranslate nohighlight">\(\psi_{jk}\)</span>.</p>
<ul class="simple">
<li><p>Our dataset are all the inputs <span class="math notranslate nohighlight">\(x\)</span> for which <span class="math notranslate nohighlight">\(y=k\)</span>.</p></li>
<li><p>We seek  the probability <span class="math notranslate nohighlight">\(\psi_{jk}\)</span> of a word <span class="math notranslate nohighlight">\(j\)</span> being present in a <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<p>What is the maximum likelihood <span class="math notranslate nohighlight">\(\psi_{jk}\)</span> in this case?</p>
<p>Each <span class="math notranslate nohighlight">\(\psi_{jk}\)</span> is simply the proportion of documents in class <span class="math notranslate nohighlight">\(k\)</span> that contain the word <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>We can maximize the likelihood exactly like we did for <span class="math notranslate nohighlight">\(\phi\)</span> to obtain closed form solutions:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\psi_{jk} = \frac{n_{jk}}{n_k}.
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(|\{i : x^{(i)}_j = 1 \text{ and } y^{(i)} = k\}|\)</span> is the number of <span class="math notranslate nohighlight">\(x^{(i)}\)</span> with label <span class="math notranslate nohighlight">\(k\)</span> and a positive occurrence of word <span class="math notranslate nohighlight">\(j\)</span>.</p>
</section>
</section>
<section id="making-predictions-using-the-model">
<h2>6.4.2. Making Predictions Using the Model<a class="headerlink" href="#making-predictions-using-the-model" title="Permalink to this headline">¶</a></h2>
<p>How do we ask the model for predictions? As discussed earler, we can apply Bayes’ rule:</p>
<div class="math notranslate nohighlight">
\[
\arg\max_y P_\theta(y|x) = \arg\max_y P_\theta(x|y)P(y).
\]</div>
<p>Thus, we can estimate the probability of <span class="math notranslate nohighlight">\(x\)</span> and under each <span class="math notranslate nohighlight">\(P_\theta(x|y=k)P(y=k)\)</span> and choose the class that explains the data best.</p>
<section id="classification-dataset-twenty-newsgroups">
<h3>Classification Dataset: Twenty Newsgroups<a class="headerlink" href="#classification-dataset-twenty-newsgroups" title="Permalink to this headline">¶</a></h3>
<p>To illustrate the text classification problem, we will use a popular dataset called <code class="docutils literal notranslate"><span class="pre">20-newsgroups</span></code>.</p>
<ul class="simple">
<li><p>It contains ~20,000 documents collected approximately evenly from 20 different online newsgroups.</p></li>
<li><p>Each newgroup covers a different topic such as medicine, computer graphics, or religion.</p></li>
<li><p>This dataset is widely used to benchmark text classification and other types of algorithms.</p></li>
</ul>
<p>Let’s load this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</span>
    
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="c1"># for this lecture, we will restrict our attention to just 4 different newsgroups:</span>
<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alt.atheism&#39;</span><span class="p">,</span> <span class="s1">&#39;soc.religion.christian&#39;</span><span class="p">,</span> <span class="s1">&#39;comp.graphics&#39;</span><span class="p">,</span> <span class="s1">&#39;sci.med&#39;</span><span class="p">]</span>

<span class="c1"># load the dataset</span>
<span class="n">twenty_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># print some information on it</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">DESCR</span><span class="p">[:</span><span class="mi">1100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _20newsgroups_dataset:

The 20 newsgroups text dataset
------------------------------

The 20 newsgroups dataset comprises around 18000 newsgroups posts on
20 topics split in two subsets: one for training (or development)
and the other one for testing (or for performance evaluation). The split
between the train and test set is based upon a messages posted before
and after a specific date.

This module contains two loaders. The first one,
:func:`sklearn.datasets.fetch_20newsgroups`,
returns a list of the raw texts that can be fed to text feature
extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`
with custom parameters so as to extract feature vectors.
The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,
returns ready-to-use features, i.e., it is not necessary to use a feature
extractor.

**Data Set Characteristics:**

    =================   ==========
    Classes                     20
    Samples total            18846
    Dimensionality               1
    Features                  text
    =================   ==========

Usage
~~~~~
</pre></div>
</div>
</div>
</div>
</section>
<section id="text-classification-with-naive-bayes">
<h3>6.4.2.1. Text Classification with Naive Bayes<a class="headerlink" href="#text-classification-with-naive-bayes" title="Permalink to this headline">¶</a></h3>
<p>Let’s see how this approach can be used in practice on the text classification dataset.</p>
<ul class="simple">
<li><p>We will learn a good set of parameters for a Bernoulli Naive Bayes model</p></li>
<li><p>We will compare the outputs to the true predictions.</p></li>
</ul>
<p>Let’s see an example of Naive Bayes on <code class="docutils literal notranslate"><span class="pre">20-newsgroups</span></code>.</p>
<p>We start by computing these features using the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># vectorize the training set</span>
<span class="n">count_vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2257, 1000)
</pre></div>
</div>
</div>
</div>
<p>Let’s compute the maximum likelihood model parameters on our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement these formulas over the Iris dataset</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># size of the dataset</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of features in our dataset</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># number of clases</span>

<span class="c1"># these are the shapes of the parameters</span>
<span class="n">psis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
<span class="n">phis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># we now compute the parameters</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">X_k</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span>
    <span class="n">psis</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">phis</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># print out the class proportions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">phis</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.21267169 0.25875055 0.26318121 0.26539654]
</pre></div>
</div>
</div>
</div>
<p>We can compute predictions using Bayes’ rule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can implement this in numpy</span>
<span class="k">def</span> <span class="nf">nb_predictions</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">psis</span><span class="p">,</span> <span class="n">phis</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This returns class assignments and scores under the NB model.</span>
<span class="sd">    </span>
<span class="sd">    We compute \arg\max_y p(y|x) as \arg\max_y p(x|y)p(y)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># adjust shapes</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">psis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">psis</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    
    <span class="c1"># clip probabilities to avoid log(0)</span>
    <span class="n">psis</span> <span class="o">=</span> <span class="n">psis</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mf">1e-14</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-14</span><span class="p">)</span>
    
    <span class="c1"># compute log-probabilities</span>
    <span class="n">logpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">phis</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">logpxy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">psis</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">psis</span><span class="p">)</span>
    <span class="n">logpyx</span> <span class="o">=</span> <span class="n">logpxy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">logpy</span>

    <span class="k">return</span> <span class="n">logpyx</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">logpyx</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>

<span class="n">idx</span><span class="p">,</span> <span class="n">logpyx</span> <span class="o">=</span> <span class="n">nb_predictions</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">psis</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1 1 3 0 3 3 3 2 2 2]
</pre></div>
</div>
</div>
</div>
<p>We can measure the accuracy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">idx</span><span class="o">==</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8692955250332299
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_new</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;OpenGL on the GPU is fast&#39;</span><span class="p">]</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs_new</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">predicted</span><span class="p">,</span> <span class="n">logpyx_new</span> <span class="o">=</span> <span class="n">nb_predictions</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">psis</span><span class="p">,</span> <span class="n">phis</span><span class="p">)</span>

<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs_new</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%r</span><span class="s1"> =&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">category</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;OpenGL on the GPU is fast&#39; =&gt; comp.graphics
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="algorithm-bernoulli-naive-bayes">
<h2>6.4.3. Algorithm: Bernoulli Naive Bayes<a class="headerlink" href="#algorithm-bernoulli-naive-bayes" title="Permalink to this headline">¶</a></h2>
<p>Finally, we summarize everything we defined in this lecture as part of the definition of our newest algorithm—Bernoulli Naive Bayes.</p>
<ul class="simple">
<li><p><strong>Type</strong>: Supervised learning (multi-class classification)</p></li>
<li><p><strong>Model family</strong>: Products of Bernoulli distributions, categorical priors</p></li>
<li><p><strong>Objective function</strong>: Log-likelihood.</p></li>
<li><p><strong>Optimizer</strong>: Closed form solution.</p></li>
</ul>
<p>There exist alternative version of Naive Bayes (Gaussian NB, Bernoulli NB). These differ in the form they use for <span class="math notranslate nohighlight">\(P(x|y)\)</span>. Overall, Naive Bayes is a very important machine learining algorithm. Despite its simplicity, it is still used in practice because it is easy to implement, very fast to train, and produces suprisingly good performance.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="lecture5-regularization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 5: Regularization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture7-gaussian-discriminant-analysis.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 7: Gaussian Discriminant Analysis</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Cornell University<br/>
    
        &copy; Copyright 2023.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>